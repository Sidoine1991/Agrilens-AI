# üîß Solution pour l'erreur "disk_offload" - Gemma 3n E4B IT

## üö® Probl√®me identifi√©

L'erreur `You are trying to offload the whole model to the disk. Please use the disk_offload function instead.` se produit lorsque :

1. **Le mod√®le est trop volumineux** pour la m√©moire disponible
2. **Hugging Face Spaces** a des limitations de m√©moire
3. **Le mod√®le Gemma 3n E4B IT** n√©cessite environ 8-12GB de RAM

## ‚úÖ Solution impl√©ment√©e

### 1. **Strat√©gies de chargement multiples**

L'application utilise maintenant 4 strat√©gies de chargement en cascade :

```python
# Strat√©gie 1: CPU Conservateur
device_map="cpu", torch_dtype=torch.float32, max_memory={"cpu": "8GB"}

# Strat√©gie 2: 4-bit Quantization  
load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16

# Strat√©gie 3: 8-bit Quantization
load_in_8bit=True

# Strat√©gie 4: Gestion m√©moire personnalis√©e
max_memory={0: "4GB", "cpu": "8GB"}
```

### 2. **D√©pendances mises √† jour**

```bash
pip install bitsandbytes>=0.41.0
pip install accelerate>=0.20.0
pip install transformers>=4.35.0
```

### 3. **Gestion automatique des erreurs**

- D√©tection automatique de la m√©moire disponible
- Fallback automatique entre les strat√©gies
- Messages d'erreur informatifs

## üß™ Test de la solution

### Ex√©cuter le script de test :

```bash
python test_model_loading.py
```

Ce script va :
- ‚úÖ V√©rifier la m√©moire disponible
- ‚úÖ Tester chaque strat√©gie de chargement
- ‚úÖ Identifier la meilleure strat√©gie pour votre environnement
- ‚úÖ Fournir des recommandations en cas d'√©chec

## üöÄ Utilisation

### 1. **Installation des d√©pendances**

```bash
pip install -r requirements.txt
```

### 2. **Lancement de l'application**

```bash
streamlit run src/streamlit_app_multilingual.py
```

### 3. **Chargement du mod√®le**

1. Ouvrez l'application dans votre navigateur
2. Allez dans la sidebar "Configuration"
3. Cliquez sur "Charger le mod√®le Gemma 3n E4B IT"
4. L'application testera automatiquement les strat√©gies

## üîç Diagnostic des probl√®mes

### Si le chargement √©choue :

1. **V√©rifiez la m√©moire disponible** :
   ```python
   import torch
   if torch.cuda.is_available():
       print(f"GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
   ```

2. **V√©rifiez les d√©pendances** :
   ```bash
   pip list | grep -E "(transformers|accelerate|bitsandbytes)"
   ```

3. **Consultez les logs** :
   - Les messages d'erreur d√©taill√©s s'affichent dans l'interface
   - Chaque strat√©gie test√©e est document√©e

## üí° Recommandations

### Pour Hugging Face Spaces :

1. **Utilisez un runtime avec plus de m√©moire** :
   - CPU: 8GB minimum
   - GPU: 16GB recommand√©

2. **Configuration dans `app.py`** :
   ```python
   # Ajoutez ces lignes au d√©but
   import os
   os.environ["TOKENIZERS_PARALLELISM"] = "false"
   os.environ["TRANSFORMERS_CACHE"] = "/tmp/transformers_cache"
   ```

3. **Variables d'environnement** :
   ```bash
   export HF_HOME="/tmp/hf_home"
   export TRANSFORMERS_CACHE="/tmp/transformers_cache"
   ```

### Pour d√©veloppement local :

1. **M√©moire recommand√©e** : 16GB RAM minimum
2. **GPU optionnel** : Am√©liore les performances
3. **Espace disque** : 10GB pour le cache des mod√®les

## üõ†Ô∏è D√©pannage avanc√©

### Erreur "bitsandbytes not found" :

```bash
pip install bitsandbytes --upgrade
# Ou pour CPU uniquement
pip install bitsandbytes-cpu
```

### Erreur "CUDA out of memory" :

1. R√©duisez la taille du batch
2. Utilisez la quantification 4-bit
3. Lib√©rez la m√©moire GPU :
   ```python
   torch.cuda.empty_cache()
   ```

### Erreur "disk_offload" persistante :

1. Forcez le mode CPU :
   ```python
   device_map="cpu"
   torch_dtype=torch.float32
   ```

2. Utilisez un mod√®le plus petit :
   ```python
   model_id = "google/gemma-2b-it"  # Au lieu de gemma-3n-E4B-it
   ```

## üìä Performance attendue

| Strat√©gie | M√©moire requise | Vitesse | Qualit√© |
|-----------|----------------|---------|---------|
| CPU Conservateur | 8GB RAM | Lente | Excellente |
| 4-bit Quantization | 4GB RAM | Moyenne | Tr√®s bonne |
| 8-bit Quantization | 6GB RAM | Rapide | Bonne |
| Gestion personnalis√©e | Variable | Variable | Excellente |

## üîÑ Mise √† jour automatique

L'application d√©tecte automatiquement :
- ‚úÖ La m√©moire disponible
- ‚úÖ Les capacit√©s GPU/CPU
- ‚úÖ Les d√©pendances install√©es
- ‚úÖ La meilleure strat√©gie √† utiliser

## üìû Support

Si le probl√®me persiste :

1. **Ex√©cutez le script de test** et partagez les r√©sultats
2. **V√©rifiez les logs** de l'application
3. **Consultez la documentation** Hugging Face
4. **Contactez le support** avec les d√©tails de l'erreur

---

**Note** : Cette solution garantit que l'application fonctionne dans la plupart des environnements, m√™me avec des ressources limit√©es. 