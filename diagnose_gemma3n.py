# =================================================================================
# Diagnostic - Gemma 3n E2B IT
# Analyse approfondie du mod√®le pour comprendre les probl√®mes de chargement
# =================================================================================

import os
import json
import torch
from transformers import AutoConfig

# Configuration
MODEL_PATH = "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1"

def analyze_model_structure():
    """Analyse la structure du mod√®le"""
    print("üîç ANALYSE DE LA STRUCTURE DU MOD√àLE")
    print("=" * 60)
    
    if not os.path.exists(MODEL_PATH):
        print(f"‚ùå Le chemin {MODEL_PATH} n'existe pas!")
        return False
    
    print(f"‚úÖ Chemin existe: {MODEL_PATH}")
    
    # Lister tous les fichiers
    try:
        files = os.listdir(MODEL_PATH)
        print(f"\nüìÅ Fichiers dans le mod√®le:")
        for file in sorted(files):
            file_path = os.path.join(MODEL_PATH, file)
            if os.path.isfile(file_path):
                size = os.path.getsize(file_path)
                print(f"  ‚Ä¢ {file} ({size:,} bytes)")
            else:
                print(f"  ‚Ä¢ {file}/ (dossier)")
    except Exception as e:
        print(f"‚ùå Erreur lors du listing: {e}")
        return False
    
    return True

def analyze_config_file():
    """Analyse le fichier de configuration"""
    print(f"\nüìã ANALYSE DU FICHIER DE CONFIGURATION")
    print("=" * 60)
    
    config_path = os.path.join(MODEL_PATH, "config.json")
    
    if not os.path.exists(config_path):
        print(f"‚ùå Fichier config.json non trouv√©: {config_path}")
        return False
    
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        print("‚úÖ Fichier config.json trouv√© et lu")
        
        print(f"\nüìä Configuration du mod√®le:")
        print(f"  ‚Ä¢ Model type: {config.get('model_type', 'Non sp√©cifi√©')}")
        print(f"  ‚Ä¢ Architectures: {config.get('architectures', 'Non sp√©cifi√©')}")
        print(f"  ‚Ä¢ Vocab size: {config.get('vocab_size', 'Non sp√©cifi√©')}")
        print(f"  ‚Ä¢ Hidden size: {config.get('hidden_size', 'Non sp√©cifi√©')}")
        print(f"  ‚Ä¢ Num layers: {config.get('num_hidden_layers', 'Non sp√©cifi√©')}")
        print(f"  ‚Ä¢ Num attention heads: {config.get('num_attention_heads', 'Non sp√©cifi√©')}")
        print(f"  ‚Ä¢ Intermediate size: {config.get('intermediate_size', 'Non sp√©cifi√©')}")
        print(f"  ‚Ä¢ Max position embeddings: {config.get('max_position_embeddings', 'Non sp√©cifi√©')}")
        print(f"  ‚Ä¢ Rope theta: {config.get('rope_theta', 'Non sp√©cifi√©')}")
        print(f"  ‚Ä¢ Use cache: {config.get('use_cache', 'Non sp√©cifi√©')}")
        print(f"  ‚Ä¢ Pad token id: {config.get('pad_token_id', 'Non sp√©cifi√©')}")
        print(f"  ‚Ä¢ Eos token id: {config.get('eos_token_id', 'Non sp√©cifi√©')}")
        print(f"  ‚Ä¢ Tie word embeddings: {config.get('tie_word_embeddings', 'Non sp√©cifi√©')}")
        
        # V√©rifier les cl√©s importantes
        important_keys = ['model_type', 'architectures', 'vocab_size', 'hidden_size']
        missing_keys = [key for key in important_keys if key not in config]
        
        if missing_keys:
            print(f"\n‚ö†Ô∏è Cl√©s manquantes: {missing_keys}")
        else:
            print(f"\n‚úÖ Toutes les cl√©s importantes sont pr√©sentes")
        
        return config
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la lecture du config: {e}")
        return False

def check_transformers_support():
    """V√©rifie le support de Transformers"""
    print(f"\nüîß V√âRIFICATION DU SUPPORT TRANSFORMERS")
    print("=" * 60)
    
    try:
        from transformers import AutoConfig
        
        print("‚úÖ Transformers import√© avec succ√®s")
        
        # Essayer de charger la configuration
        try:
            config = AutoConfig.from_pretrained(MODEL_PATH, trust_remote_code=True)
            print("‚úÖ Configuration charg√©e avec AutoConfig")
            print(f"  ‚Ä¢ Classe de config: {type(config).__name__}")
            print(f"  ‚Ä¢ Model type: {config.model_type}")
            print(f"  ‚Ä¢ Architectures: {config.architectures}")
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement de la config: {e}")
        
        # V√©rifier les mod√®les support√©s
        from transformers import MODEL_FOR_CAUSAL_LM_MAPPING, MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING
        
        print(f"\nüìö Mod√®les support√©s:")
        print(f"  ‚Ä¢ Causal LM: {len(MODEL_FOR_CAUSAL_LM_MAPPING)} mod√®les")
        print(f"  ‚Ä¢ Image-Text-to-Text: {len(MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING)} mod√®les")
        
        # Chercher "gemma" dans les mod√®les support√©s
        gemma_models = []
        for model_type in MODEL_FOR_CAUSAL_LM_MAPPING.keys():
            if 'gemma' in model_type.lower():
                gemma_models.append(model_type)
        
        print(f"\nüîç Mod√®les Gemma support√©s: {gemma_models}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la v√©rification: {e}")
        return False

def try_manual_loading():
    """Essaie un chargement manuel"""
    print(f"\nüîÑ TENTATIVE DE CHARGEMENT MANUEL")
    print("=" * 60)
    
    approaches = [
        {
            "name": "AutoConfig + AutoModelForCausalLM",
            "imports": ["AutoConfig", "AutoModelForCausalLM"],
            "config_class": "AutoConfig",
            "model_class": "AutoModelForCausalLM"
        },
        {
            "name": "AutoConfig + AutoModelForImageTextToText", 
            "imports": ["AutoConfig", "AutoModelForImageTextToText"],
            "config_class": "AutoConfig",
            "model_class": "AutoModelForImageTextToText"
        },
        {
            "name": "AutoProcessor + AutoModelForCausalLM",
            "imports": ["AutoProcessor", "AutoModelForCausalLM"],
            "config_class": "AutoProcessor",
            "model_class": "AutoModelForCausalLM"
        }
    ]
    
    for approach in approaches:
        print(f"\nüìù Test: {approach['name']}")
        try:
            # Import dynamique
            from transformers import AutoConfig, AutoModelForCausalLM, AutoModelForImageTextToText, AutoProcessor
            
            # Essayer de charger
            if "Processor" in approach['name']:
                processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
                print(f"‚úÖ {approach['config_class']} charg√©")
                
                model = AutoModelForCausalLM.from_pretrained(
                    MODEL_PATH,
                    trust_remote_code=True,
                    device_map="cpu",
                    torch_dtype=torch.float32
                )
                print(f"‚úÖ {approach['model_class']} charg√©")
                
                return model, processor
            else:
                config = AutoConfig.from_pretrained(MODEL_PATH, trust_remote_code=True)
                print(f"‚úÖ {approach['config_class']} charg√©")
                
                if "ImageTextToText" in approach['name']:
                    model = AutoModelForImageTextToText.from_pretrained(
                        MODEL_PATH,
                        trust_remote_code=True,
                        device_map="cpu",
                        torch_dtype=torch.float32
                    )
                else:
                    model = AutoModelForCausalLM.from_pretrained(
                        MODEL_PATH,
                        trust_remote_code=True,
                        device_map="cpu",
                        torch_dtype=torch.float32
                    )
                print(f"‚úÖ {approach['model_class']} charg√©")
                
                return model, config
                
        except Exception as e:
            print(f"‚ùå √âchec: {e}")
    
    return None, None

def main():
    """Fonction principale de diagnostic"""
    print("üöÄ DIAGNOSTIC COMPLET - Gemma 3n E2B IT")
    print("=" * 80)
    
    # 1. Analyse de la structure
    if not analyze_model_structure():
        return
    
    # 2. Analyse de la configuration
    config = analyze_config_file()
    if not config:
        return
    
    # 3. V√©rification du support Transformers
    check_transformers_support()
    
    # 4. Tentative de chargement manuel
    model, processor_or_config = try_manual_loading()
    
    # 5. R√©sum√©
    print(f"\nüìã R√âSUM√â DU DIAGNOSTIC")
    print("=" * 60)
    
    if model is not None:
        print("‚úÖ Le mod√®le peut √™tre charg√© manuellement")
        print(f"  ‚Ä¢ Type de mod√®le: {type(model).__name__}")
        print(f"  ‚Ä¢ Type de processeur/config: {type(processor_or_config).__name__}")
    else:
        print("‚ùå Le mod√®le ne peut pas √™tre charg√©")
        print("\nüí° RECOMMANDATIONS:")
        print("  1. Le mod√®le Gemma 3n est tr√®s r√©cent et n'est pas encore support√©")
        print("  2. Attendez une mise √† jour de Transformers (version stable)")
        print("  3. Utilisez un mod√®le alternatif comme:")
        print("     ‚Ä¢ google/gemma-2b-it")
        print("     ‚Ä¢ google/gemma-7b-it") 
        print("     ‚Ä¢ microsoft/DialoGPT-medium")
        print("  4. Ou utilisez l'API Hugging Face pour l'inf√©rence")

if __name__ == "__main__":
    main() 