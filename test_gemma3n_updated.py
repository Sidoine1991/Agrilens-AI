# =================================================================================
# Test de G√©n√©ration - Gemma 3n E2B IT (Version avec Transformers √† jour)
# 
# Auteur : Sidoine Kolaol√© YEBADOKPO
# Objectif : Tester les capacit√©s de g√©n√©ration avec Transformers mis √† jour
# =================================================================================

import os
import torch
import time
import subprocess
import sys

# Installation des d√©pendances avec Transformers √† jour
print("üîß Installation des d√©pendances avec Transformers √† jour...")

def install_package(package):
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
        print(f"‚úÖ {package} install√© avec succ√®s")
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur installation {package}: {e}")

# Installation de Transformers depuis la source (n√©cessaire pour Gemma 3n)
print("üì¶ Installation de Transformers depuis la source...")
try:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "git+https://github.com/huggingface/transformers.git"])
    print("‚úÖ Transformers install√© depuis la source")
except Exception as e:
    print(f"‚ö†Ô∏è Erreur installation Transformers: {e}")

# Installation des autres d√©pendances
packages = [
    "bitsandbytes",
    "accelerate",
    "torch",
    "torchvision",
    "pillow",
    "requests"
]

for package in packages:
    install_package(package)

print("\nüìö Import des biblioth√®ques...")

# Import apr√®s installation
from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig
from PIL import Image
import requests
from io import BytesIO

# Configuration de l'environnement
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Chemin local du mod√®le fourni par Kaggle
MODEL_PATH = "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1"

def load_model():
    """Charge le mod√®le Gemma 3n avec la configuration optimis√©e"""
    try:
        print("üîÑ Chargement du mod√®le Gemma 3n E2B IT...")
        print(f"üìÅ Chemin du mod√®le: {MODEL_PATH}")
        
        # V√©rifier si le chemin existe
        if not os.path.exists(MODEL_PATH):
            print(f"‚ùå Le chemin {MODEL_PATH} n'existe pas!")
            return None, None
        
        print("‚úÖ Chemin du mod√®le v√©rifi√©")
        
        # Configuration 4-bit pour √©conomiser la m√©moire
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_block_size=16,
        )
        
        # Chargement du processeur
        print("üìù Chargement du processeur...")
        processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
        
        # Chargement du mod√®le
        print("ü§ñ Chargement du mod√®le...")
        model = AutoModelForImageTextToText.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            device_map="auto",
            quantization_config=bnb_config,
        )
        
        print("‚úÖ Mod√®le charg√© avec succ√®s!")
        return model, processor
        
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement: {e}")
        print(f"Type d'erreur: {type(e).__name__}")
        return None, None

def test_text_generation(model, processor, prompt, max_new_tokens=100):
    """Teste la g√©n√©ration de texte avec un prompt simple"""
    try:
        print(f"\nüî§ Test de g√©n√©ration de texte:")
        print(f"Prompt: {prompt}")
        print("-" * 50)
        
        # Pr√©paration des inputs
        inputs = processor(text=prompt, return_tensors="pt")
        
        # Mesure du temps de g√©n√©ration
        start_time = time.time()
        
        # G√©n√©ration
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        generation_time = time.time() - start_time
        
        # D√©codage de la r√©ponse
        response = processor.decode(outputs[0], skip_special_tokens=True)
        
        # Nettoyage de la r√©ponse (enlever le prompt original)
        if prompt in response:
            response = response.replace(prompt, "").strip()
        
        print(f"‚è±Ô∏è Temps de g√©n√©ration: {generation_time:.2f} secondes")
        print(f"üìù R√©ponse g√©n√©r√©e:")
        print(response)
        print("-" * 50)
        
        return response, generation_time
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la g√©n√©ration: {e}")
        return None, 0

def test_multimodal_generation(model, processor, image, prompt, max_new_tokens=100):
    """Teste la g√©n√©ration multimodale (image + texte)"""
    try:
        print(f"\nüñºÔ∏è Test de g√©n√©ration multimodale:")
        print(f"Prompt: {prompt}")
        print(f"Image: {image.size}")
        print("-" * 50)
        
        # Pr√©paration des inputs avec image
        inputs = processor(text=prompt, images=image, return_tensors="pt")
        
        # Mesure du temps de g√©n√©ration
        start_time = time.time()
        
        # G√©n√©ration
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        generation_time = time.time() - start_time
        
        # D√©codage de la r√©ponse
        response = processor.decode(outputs[0], skip_special_tokens=True)
        
        # Nettoyage de la r√©ponse
        if prompt in response:
            response = response.replace(prompt, "").strip()
        
        print(f"‚è±Ô∏è Temps de g√©n√©ration: {generation_time:.2f} secondes")
        print(f"üìù R√©ponse g√©n√©r√©e:")
        print(response)
        print("-" * 50)
        
        return response, generation_time
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la g√©n√©ration multimodale: {e}")
        return None, 0

def load_test_image():
    """Charge une image de test pour la d√©monstration"""
    try:
        # URL d'une image de plante pour test
        image_url = "https://images.unsplash.com/photo-1546094096-0df4bcaaa337?w=400"
        response = requests.get(image_url)
        image = Image.open(BytesIO(response.content))
        print(f"‚úÖ Image de test charg√©e: {image.size}")
        return image
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement de l'image: {e}")
        return None

def main():
    """Fonction principale de test"""
    print("üöÄ D√©marrage des tests de g√©n√©ration Gemma 3n")
    print("=" * 60)
    
    # V√©rification de l'environnement
    print(f"üîß PyTorch version: {torch.__version__}")
    print(f"üíª CUDA disponible: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"üéÆ GPU: {torch.cuda.get_device_name(0)}")
        print(f"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # Chargement du mod√®le
    model, processor = load_model()
    
    if model is None or processor is None:
        print("‚ùå Impossible de continuer sans mod√®le charg√©")
        return
    
    # Chargement d'une image de test
    test_image = load_test_image()
    
    # Test 1: G√©n√©ration de texte simple
    print("\n" + "="*60)
    print("TEST 1: G√âN√âRATION DE TEXTE SIMPLE")
    print("="*60)
    
    simple_prompts = [
        "Qu'est-ce que l'intelligence artificielle?",
        "Explique-moi l'agriculture durable en 3 points.",
        "Comment les plantes communiquent-elles entre elles?"
    ]
    
    for prompt in simple_prompts:
        test_text_generation(model, processor, prompt)
    
    # Test 2: G√©n√©ration multimodale (si image disponible)
    if test_image:
        print("\n" + "="*60)
        print("TEST 2: G√âN√âRATION MULTIMODALE")
        print("="*60)
        
        multimodal_prompts = [
            "D√©cris ce que tu vois dans cette image.",
            "Cette plante semble-t-elle en bonne sant√©?",
            "Quels conseils donnerais-tu pour cette plante?"
        ]
        
        for prompt in multimodal_prompts:
            test_multimodal_generation(model, processor, test_image, prompt)
    
    # Test 3: Diagnostic de plantes
    print("\n" + "="*60)
    print("TEST 3: DIAGNOSTIC DE PLANTES")
    print("="*60)
    
    if test_image:
        plant_prompts = [
            "Analyse cette image de plante et identifie les probl√®mes visibles.",
            "D√©cris les sympt√¥mes visibles sur cette plante et leurs causes possibles.",
            "Bas√© sur cette image, quels traitements recommandes-tu pour cette plante?"
        ]
        
        for prompt in plant_prompts:
            test_multimodal_generation(model, processor, test_image, prompt)
    else:
        # Test avec texte seulement
        text_prompts = [
            "Comment identifier les maladies des plantes?",
            "Quels sont les sympt√¥mes courants des carences en nutriments?",
            "Comment pr√©venir les maladies fongiques chez les plantes?"
        ]
        
        for prompt in text_prompts:
            test_text_generation(model, processor, prompt)
    
    print("\n" + "="*60)
    print("üéâ Tests termin√©s avec succ√®s!")
    print("="*60)

if __name__ == "__main__":
    main() 