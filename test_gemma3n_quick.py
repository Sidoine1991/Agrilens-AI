# =================================================================================
# Test Rapide de G√©n√©ration de Texte - Gemma 3n E2B IT
# Version simplifi√©e pour test rapide
# =================================================================================

import os
import torch
import time
from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig

# Configuration
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
MODEL_PATH = "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1"

def quick_test():
    """Test rapide de g√©n√©ration de texte"""
    
    print("üöÄ Test rapide de g√©n√©ration Gemma 3n")
    print("=" * 50)
    
    try:
        # 1. Chargement du mod√®le
        print("üì• Chargement du mod√®le...")
        
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_block_size=16,
        )
        
        processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
        model = AutoModelForImageTextToText.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            device_map="auto",
            quantization_config=bnb_config,
        )
        
        print("‚úÖ Mod√®le charg√©!")
        
        # 2. Test de g√©n√©ration simple
        print("\nüî§ Test de g√©n√©ration de texte...")
        
        prompt = "Explique-moi l'agriculture durable en 3 points cl√©s."
        print(f"Prompt: {prompt}")
        
        # Pr√©paration
        inputs = processor(text=prompt, return_tensors="pt")
        
        # G√©n√©ration
        start_time = time.time()
        
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=150,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        gen_time = time.time() - start_time
        
        # D√©codage
        response = processor.decode(outputs[0], skip_special_tokens=True)
        
        # Nettoyage
        if prompt in response:
            response = response.replace(prompt, "").strip()
        
        print(f"\n‚è±Ô∏è Temps: {gen_time:.2f}s")
        print(f"üìù R√©ponse:")
        print(response)
        
        print("\nüéâ Test r√©ussi!")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        return False

if __name__ == "__main__":
    quick_test() 