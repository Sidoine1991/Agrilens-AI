# =================================================================================
# AgriLens AI - Version Am√©lior√©e pour Kaggle T4 GPU
# Auteur : Sidoine Kolaol√© YEBADOKPO
# Optimis√© pour la g√©n√©ration de texte et l'analyse d'images
# =================================================================================

import os
import torch
import time
import gc
from typing import Optional, Tuple, Dict, Any
import warnings
warnings.filterwarnings("ignore")

# =================================================================================
# CONFIGURATION ET SETUP
# =================================================================================

def setup_environment():
    """Configure l'environnement pour l'ex√©cution optimale"""
    print("üîß Configuration de l'environnement...")
    
    # Forcer l'utilisation d'un seul GPU
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    print(f"‚úÖ CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}")
    print(f"‚úÖ PyTorch version: {torch.__version__}")
    print(f"‚úÖ CUDA disponible: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")
        print(f"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

def install_dependencies():
    """Installe les d√©pendances n√©cessaires"""
    print("\nüì¶ Installation des d√©pendances...")
    
    try:
        import subprocess
        import sys
        
        packages = [
            "timm",
            "accelerate", 
            "git+https://github.com/huggingface/transformers.git"
        ]
        
        for package in packages:
            print(f"üì• Installation de {package}...")
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", package, "--upgrade", "-q"
            ])
        
        print("‚úÖ Toutes les d√©pendances install√©es")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de l'installation: {e}")

def memory_monitor(stage: str = ""):
    """Surveille l'utilisation de la m√©moire GPU"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / (1024 ** 3)
        reserved = torch.cuda.memory_reserved(0) / (1024 ** 3)
        print(f"üíæ M√©moire ({stage}): Allou√©e: {allocated:.2f} GB, R√©serv√©e: {reserved:.2f} GB")

def clear_memory():
    """Nettoie la m√©moire GPU et CPU"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
    print("üßπ M√©moire nettoy√©e")

# =================================================================================
# CHARGEMENT DU MOD√àLE
# =================================================================================

class AgriLensModel:
    """Classe pour g√©rer le mod√®le AgriLens AI"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.tokenizer = None
        self.model = None
        self.processor = None
        self.multimodal_model = None
        
    def load_text_model(self) -> bool:
        """Charge le mod√®le de g√©n√©ration de texte"""
        try:
            print("\nüìù Chargement du mod√®le de texte...")
            
            from transformers import AutoTokenizer, AutoModelForCausalLM
            
            # Chargement du tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path, 
                trust_remote_code=True
            )
            
            # Configuration du tokenizer
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # Chargement du mod√®le avec optimisations
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True
            )
            
            memory_monitor("Apr√®s chargement texte")
            print("‚úÖ Mod√®le de texte charg√© avec succ√®s")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement du mod√®le de texte: {e}")
            return False
    
    def load_multimodal_model(self) -> bool:
        """Charge le mod√®le multimodal pour l'analyse d'images"""
        try:
            print("\nüñºÔ∏è Chargement du mod√®le multimodal...")
            
            from transformers import AutoProcessor, AutoModelForImageTextToText
            
            # Chargement du processeur
            self.processor = AutoProcessor.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            
            # Chargement du mod√®le multimodal (CPU pour √©conomiser la m√©moire)
            self.multimodal_model = AutoModelForImageTextToText.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                device_map="cpu",
                torch_dtype=torch.float32
            )
            
            print("‚úÖ Mod√®le multimodal charg√© avec succ√®s")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement du mod√®le multimodal: {e}")
            return False

# =================================================================================
# G√âN√âRATION DE TEXTE
# =================================================================================

def generate_text_response(
    model: AgriLensModel,
    prompt: str,
    max_new_tokens: int = 256,
    temperature: float = 0.7
) -> Optional[str]:
    """G√©n√®re une r√©ponse textuelle"""
    
    try:
        print(f"\nüî§ G√©n√©ration de texte...")
        print(f"üìù Prompt: {prompt[:100]}...")
        
        if model.tokenizer is None or model.model is None:
            print("‚ùå Mod√®le de texte non charg√©")
            return None
        
        # Tokenisation
        inputs = model.tokenizer(
            prompt, 
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        
        # D√©placer vers le bon device
        device = next(model.model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        memory_monitor("Avant g√©n√©ration")
        
        # G√©n√©ration
        start_time = time.time()
        
        with torch.no_grad():
            outputs = model.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=temperature,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=model.tokenizer.eos_token_id
            )
        
        generation_time = time.time() - start_time
        
        # D√©codage
        response = model.tokenizer.decode(
            outputs[0], 
            skip_special_tokens=True
        )
        
        # Nettoyage de la r√©ponse
        if prompt in response:
            response = response.replace(prompt, "").strip()
        
        memory_monitor("Apr√®s g√©n√©ration")
        
        print(f"‚è±Ô∏è Temps de g√©n√©ration: {generation_time:.2f}s")
        print(f"üìù R√©ponse g√©n√©r√©e ({len(response)} caract√®res)")
        
        return response
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la g√©n√©ration de texte: {e}")
        return None

# =================================================================================
# ANALYSE D'IMAGES
# =================================================================================

def analyze_plant_image(
    model: AgriLensModel,
    image_path: str,
    prompt: str = None
) -> Optional[str]:
    """Analyse une image de plante"""
    
    try:
        print(f"\nüñºÔ∏è Analyse d'image: {image_path}")
        
        if model.processor is None or model.multimodal_model is None:
            print("‚ùå Mod√®le multimodal non charg√©")
            return None
        
        # Chargement et pr√©paration de l'image
        from PIL import Image
        
        image = Image.open(image_path).convert("RGB")
        image = image.resize((224, 224))  # Redimensionnement pour √©conomiser la m√©moire
        
        print(f"‚úÖ Image charg√©e: {image.size}")
        
        # Prompt par d√©faut si aucun fourni
        if prompt is None:
            prompt = (
                "Analyse cette image de plante. D√©cris les sympt√¥mes visibles et "
                "fournis un diagnostic structur√© incluant:\n"
                "1. Nom de la maladie probable\n"
                "2. Sympt√¥mes observ√©s\n"
                "3. Causes possibles\n"
                "4. Traitements recommand√©s\n"
                "5. Mesures pr√©ventives"
            )
        
        # Pr√©paration des inputs
        inputs = model.processor(
            text=prompt,
            images=image,
            return_tensors="pt"
        )
        
        # G√©n√©ration
        start_time = time.time()
        
        with torch.no_grad():
            outputs = model.multimodal_model.generate(
                **inputs,
                max_new_tokens=300,
                do_sample=True,
                temperature=0.7,
                top_p=0.9
            )
        
        generation_time = time.time() - start_time
        
        # D√©codage
        response = model.processor.decode(
            outputs[0], 
            skip_special_tokens=True
        )
        
        # Nettoyage
        if prompt in response:
            response = response.replace(prompt, "").strip()
        
        print(f"‚è±Ô∏è Temps d'analyse: {generation_time:.2f}s")
        print(f"üìã Diagnostic g√©n√©r√© ({len(response)} caract√®res)")
        
        return response
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'analyse d'image: {e}")
        return None

# =================================================================================
# FONCTION PRINCIPALE
# =================================================================================

def main():
    """Fonction principale d'ex√©cution"""
    
    print("üöÄ AgriLens AI - Version Am√©lior√©e")
    print("=" * 60)
    
    # 1. Configuration de l'environnement
    setup_environment()
    
    # 2. Installation des d√©pendances
    install_dependencies()
    
    # 3. Import des biblioth√®ques
    print("\nüìö Import des biblioth√®ques...")
    try:
        import kagglehub
        from transformers import GenerationConfig
        print("‚úÖ Biblioth√®ques import√©es")
    except Exception as e:
        print(f"‚ùå Erreur d'import: {e}")
        return
    
    # 4. Chemin du mod√®le
    try:
        GEMMA_PATH = kagglehub.model_download("google/gemma-3n/transformers/gemma-3n-e2b-it")
        print(f"‚úÖ Mod√®le localis√©: {GEMMA_PATH}")
    except:
        # Fallback vers le chemin local
        GEMMA_PATH = "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1"
        print(f"‚úÖ Utilisation du chemin local: {GEMMA_PATH}")
    
    # 5. Initialisation du mod√®le
    agrilens = AgriLensModel(GEMMA_PATH)
    
    # 6. Chargement des mod√®les
    text_success = agrilens.load_text_model()
    multimodal_success = agrilens.load_multimodal_model()
    
    if not text_success and not multimodal_success:
        print("‚ùå Aucun mod√®le n'a pu √™tre charg√©")
        return
    
    # 7. Tests de g√©n√©ration
    print("\n" + "="*60)
    print("üß™ TESTS DE G√âN√âRATION")
    print("="*60)
    
    # Test de g√©n√©ration de texte
    if text_success:
        print("\nüìù Test de g√©n√©ration de texte...")
        
        text_prompt = """
        Fournis une description d√©taill√©e des sympt√¥mes de la maladie du mildiou 
        chez les plants de tomate. Inclus des informations sur l'apparence, 
        la progression et les parties sp√©cifiques de la plante affect√©es.
        """
        
        text_response = generate_text_response(
            agrilens, 
            text_prompt, 
            max_new_tokens=200
        )
        
        if text_response:
            print("\nüìã R√©ponse du mod√®le:")
            print("-" * 40)
            print(text_response)
            print("-" * 40)
    
    # Test d'analyse d'image
    if multimodal_success:
        print("\nüñºÔ∏è Test d'analyse d'image...")
        
        # Chercher une image de test
        test_images = [
            "/kaggle/input/tomato/tomato_early_blight.jpg",
            "/kaggle/input/tomato/tomato_late_blight.jpg",
            "/kaggle/input/tomato/tomato_healthy.jpg"
        ]
        
        image_found = False
        for image_path in test_images:
            if os.path.exists(image_path):
                image_response = analyze_plant_image(agrilens, image_path)
                if image_response:
                    print("\nüìã Diagnostic d'image:")
                    print("-" * 40)
                    print(image_response)
                    print("-" * 40)
                    image_found = True
                    break
        
        if not image_found:
            print("‚ö†Ô∏è Aucune image de test trouv√©e")
    
    # 8. Nettoyage final
    clear_memory()
    
    print("\n" + "="*60)
    print("üéâ AgriLens AI - Ex√©cution termin√©e avec succ√®s!")
    print("="*60)

if __name__ == "__main__":
    main() 