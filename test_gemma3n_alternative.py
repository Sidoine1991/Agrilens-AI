# =================================================================================
# Test Alternative - Gemma 3n E2B IT
# Approches alternatives pour charger le mod√®le Gemma 3n
# =================================================================================

import os
import torch
import time
import json

# Configuration
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
MODEL_PATH = "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1"

def check_model_files():
    """V√©rifie les fichiers disponibles dans le mod√®le"""
    print("üîç V√©rification des fichiers du mod√®le...")
    
    if not os.path.exists(MODEL_PATH):
        print(f"‚ùå Le chemin {MODEL_PATH} n'existe pas!")
        return False
    
    print(f"‚úÖ Chemin existe: {MODEL_PATH}")
    
    # Lister les fichiers
    try:
        files = os.listdir(MODEL_PATH)
        print(f"üìÅ Fichiers trouv√©s: {files}")
        
        # V√©rifier le fichier config.json
        config_path = os.path.join(MODEL_PATH, "config.json")
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                config = json.load(f)
            print(f"üìã Configuration du mod√®le:")
            print(f"  ‚Ä¢ Model type: {config.get('model_type', 'Non sp√©cifi√©')}")
            print(f"  ‚Ä¢ Architecture: {config.get('architectures', 'Non sp√©cifi√©')}")
            print(f"  ‚Ä¢ Vocab size: {config.get('vocab_size', 'Non sp√©cifi√©')}")
            print(f"  ‚Ä¢ Hidden size: {config.get('hidden_size', 'Non sp√©cifi√©')}")
            print(f"  ‚Ä¢ Num layers: {config.get('num_hidden_layers', 'Non sp√©cifi√©')}")
        else:
            print("‚ùå Fichier config.json non trouv√©")
            
    except Exception as e:
        print(f"‚ùå Erreur lors de la v√©rification: {e}")
        return False
    
    return True

def try_different_loading_approaches():
    """Essaie diff√©rentes approches pour charger le mod√®le"""
    
    print("\nüîÑ Tentative de chargement avec diff√©rentes approches...")
    
    # Approche 1: AutoProcessor + AutoModelForCausalLM
    print("\nüìù Approche 1: AutoProcessor + AutoModelForCausalLM")
    try:
        from transformers import AutoProcessor, AutoModelForCausalLM, BitsAndBytesConfig
        
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        
        processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            device_map="auto",
            quantization_config=bnb_config,
        )
        
        print("‚úÖ Approche 1 r√©ussie!")
        return model, processor
        
    except Exception as e:
        print(f"‚ùå Approche 1 √©chou√©e: {e}")
    
    # Approche 2: AutoTokenizer + AutoModelForCausalLM
    print("\nüìù Approche 2: AutoTokenizer + AutoModelForCausalLM")
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
        
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            device_map="auto",
            quantization_config=bnb_config,
        )
        
        print("‚úÖ Approche 2 r√©ussie!")
        return model, tokenizer
        
    except Exception as e:
        print(f"‚ùå Approche 2 √©chou√©e: {e}")
    
    # Approche 3: Chargement sans quantification
    print("\nüìù Approche 3: Chargement sans quantification")
    try:
        from transformers import AutoProcessor, AutoModelForCausalLM
        
        processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            device_map="auto",
            torch_dtype=torch.float16,
        )
        
        print("‚úÖ Approche 3 r√©ussie!")
        return model, processor
        
    except Exception as e:
        print(f"‚ùå Approche 3 √©chou√©e: {e}")
    
    # Approche 4: Chargement CPU seulement
    print("\nüìù Approche 4: Chargement CPU seulement")
    try:
        from transformers import AutoProcessor, AutoModelForCausalLM
        
        processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            device_map="cpu",
            torch_dtype=torch.float32,
        )
        
        print("‚úÖ Approche 4 r√©ussie!")
        return model, processor
        
    except Exception as e:
        print(f"‚ùå Approche 4 √©chou√©e: {e}")
    
    return None, None

def test_generation_with_processor(model, processor, prompt="Explique-moi l'agriculture durable."):
    """Test de g√©n√©ration avec un processeur"""
    try:
        print(f"\nüî§ Test de g√©n√©ration avec processeur:")
        print(f"Prompt: {prompt}")
        
        inputs = processor(text=prompt, return_tensors="pt")
        
        start_time = time.time()
        
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=True,
                temperature=0.7,
            )
        
        gen_time = time.time() - start_time
        
        response = processor.decode(outputs[0], skip_special_tokens=True)
        
        if prompt in response:
            response = response.replace(prompt, "").strip()
        
        print(f"‚è±Ô∏è Temps: {gen_time:.2f}s")
        print(f"üìù R√©ponse: {response}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration: {e}")
        return False

def test_generation_with_tokenizer(model, tokenizer, prompt="Explique-moi l'agriculture durable."):
    """Test de g√©n√©ration avec un tokenizer"""
    try:
        print(f"\nüî§ Test de g√©n√©ration avec tokenizer:")
        print(f"Prompt: {prompt}")
        
        inputs = tokenizer(prompt, return_tensors="pt")
        
        start_time = time.time()
        
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=True,
                temperature=0.7,
            )
        
        gen_time = time.time() - start_time
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        if prompt in response:
            response = response.replace(prompt, "").strip()
        
        print(f"‚è±Ô∏è Temps: {gen_time:.2f}s")
        print(f"üìù R√©ponse: {response}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration: {e}")
        return False

def main():
    """Fonction principale"""
    print("üöÄ Test Alternative - Gemma 3n E2B IT")
    print("=" * 60)
    
    # V√©rification des fichiers
    if not check_model_files():
        print("‚ùå Impossible de continuer")
        return
    
    # Tentative de chargement
    model, processor_or_tokenizer = try_different_loading_approaches()
    
    if model is None:
        print("\n‚ùå Aucune approche n'a fonctionn√©")
        print("üí° Suggestions:")
        print("  ‚Ä¢ Le mod√®le Gemma 3n est tr√®s r√©cent et n'est pas encore support√©")
        print("  ‚Ä¢ Attendez une mise √† jour de Transformers")
        print("  ‚Ä¢ Utilisez un mod√®le alternatif (Gemma 2B, Llama, etc.)")
        return
    
    # Test de g√©n√©ration
    print("\nüéØ Test de g√©n√©ration...")
    
    if hasattr(processor_or_tokenizer, 'decode'):
        # C'est un processeur
        success = test_generation_with_processor(model, processor_or_tokenizer)
    else:
        # C'est un tokenizer
        success = test_generation_with_tokenizer(model, processor_or_tokenizer)
    
    if success:
        print("\nüéâ Test r√©ussi!")
    else:
        print("\n‚ùå Test √©chou√©")

if __name__ == "__main__":
    main() 