# üîß Guide de D√©pannage Final - Probl√®mes CUDA/BitsAndBytes

## üö® Probl√®me Principal Identifi√©

Votre environnement a **CUDA 12.5** mais bitsandbytes ne trouve pas la biblioth√®que `libbitsandbytes_cuda125_nocublaslt.so`. C'est un probl√®me courant avec les Tesla P100 qui ont une capacit√© de calcul 6.0.

## ‚úÖ Solution Recommand√©e : Sans BitsAndBytes

Utilisez le fichier **`notebook_no_bitsandbytes.py`** qui :
- ‚úÖ √âvite compl√®tement bitsandbytes
- ‚úÖ Utilise `torch.float16` pour √©conomiser la m√©moire
- ‚úÖ Fonctionne avec votre Tesla P100
- ‚úÖ Inclut des fallbacks multiples

## üîç Pourquoi BitsAndBytes Ne Fonctionne Pas

### Probl√®me 1 : Version CUDA
- **D√©tect√©** : CUDA 12.5
- **BitsAndBytes** : Cherche `libbitsandbytes_cuda125_nocublaslt.so`
- **R√©sultat** : Biblioth√®que non trouv√©e

### Probl√®me 2 : Capacit√© de Calcul
- **Tesla P100** : Capacit√© 6.0
- **BitsAndBytes** : Optimis√© pour ‚â•7.5
- **R√©sultat** : Performance d√©grad√©e

### Probl√®me 3 : Conflit de Versions
- **PyTorch** : 2.1.2 avec CUDA 11.8
- **BitsAndBytes** : D√©tecte CUDA 12.5
- **R√©sultat** : Incompatibilit√©

## üöÄ Solutions par Ordre de Priorit√©

### Solution 1 : Sans Quantification (Recommand√©e)
```python
# Utiliser torch.float16 au lieu de bitsandbytes
model = Gemma3nForConditionalGeneration.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=torch.float16  # √âconomise ~50% de m√©moire
)
```

### Solution 2 : Compilation depuis Source
```bash
# Compiler bitsandbytes pour CUDA 12.5
git clone https://github.com/TimDettmers/bitsandbytes.git
cd bitsandbytes
CUDA_VERSION=125_nomatmul python setup.py install
```

### Solution 3 : Version CPU de BitsAndBytes
```python
# Forcer l'utilisation CPU (plus lent mais fonctionne)
import os
os.environ["BITSANDBYTES_FUNCTIONAL"] = "cpu"
```

## üìä Comparaison des Approches

| Approche | M√©moire | Performance | Compatibilit√© |
|----------|---------|-------------|---------------|
| **torch.float16** | ~50% √©conomie | ‚úÖ Bonne | ‚úÖ Excellente |
| **BitsAndBytes 4-bit** | ~75% √©conomie | ‚ö†Ô∏è Lente (P100) | ‚ùå Probl√©matique |
| **BitsAndBytes 8-bit** | ~50% √©conomie | ‚ùå Tr√®s lente (P100) | ‚ùå Probl√©matique |
| **Sans optimisation** | 100% | ‚úÖ Rapide | ‚úÖ Parfaite |

## üéØ Configuration Optimale pour Tesla P100

```python
# Configuration recommand√©e
import torch
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3n-2b-it",  # Mod√®le 2B (plus petit)
    device_map="auto",
    torch_dtype=torch.float16,
    trust_remote_code=True
)
```

## üîß Commandes de Diagnostic

### V√©rifier CUDA
```bash
nvidia-smi
nvcc --version
python -c "import torch; print(torch.version.cuda)"
```

### V√©rifier BitsAndBytes
```bash
python -m bitsandbytes
```

### V√©rifier la M√©moire GPU
```python
import torch
print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"M√©moire: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
print(f"Capacit√©: {torch.cuda.get_device_capability(0)}")
```

## üìã Checklist de R√©solution

- [ ] **Essayer `notebook_no_bitsandbytes.py`**
- [ ] V√©rifier que le mod√®le local existe
- [ ] Tester avec un mod√®le HF Hub
- [ ] V√©rifier l'espace disque disponible
- [ ] Red√©marrer le kernel si n√©cessaire

## üÜò Si Rien Ne Fonctionne

### Option 1 : Mod√®le Plus Petit
```python
# Utiliser un mod√®le 2B au lieu de 9B/27B
model_id = "google/gemma-3n-2b-it"
```

### Option 2 : Chargement Partiel
```python
# Charger seulement certaines couches
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True
)
```

### Option 3 : CPU Fallback
```python
# Utiliser CPU si GPU pose probl√®me
device = torch.device("cpu")
model = model.to(device)
```

## üí° Conseils pour Tesla P100

1. **Privil√©giez torch.float16** sur bitsandbytes
2. **Utilisez des mod√®les 2B** plut√¥t que 9B/27B
3. **√âvitez la quantification** sur ce GPU
4. **Surveillez la m√©moire** avec `nvidia-smi`
5. **Red√©marrez le kernel** si n√©cessaire

---

**Note** : Le Tesla P100 est un excellent GPU, mais il a des limitations avec les techniques de quantification modernes. `torch.float16` offre un bon compromis performance/m√©moire. 