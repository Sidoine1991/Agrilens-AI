#!/usr/bin/env python3
"""
Script de test pour le chargement du mod√®le Gemma 3n E4B IT
Teste diff√©rentes strat√©gies de chargement pour √©viter l'erreur de disk_offload
"""

import torch
import sys
import os

def test_memory_availability():
    """Teste la disponibilit√© de la m√©moire"""
    print("üîç V√©rification de la m√©moire disponible...")
    
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"‚úÖ GPU disponible : {gpu_memory:.1f} GB")
        return gpu_memory
    else:
        print("‚ö†Ô∏è GPU non disponible, utilisation du CPU")
        return 0

def test_model_loading():
    """Teste le chargement du mod√®le avec diff√©rentes strat√©gies"""
    print("\nüöÄ Test de chargement du mod√®le Gemma 3n E4B IT...")
    
    try:
        from transformers import AutoProcessor, Gemma3nForConditionalGeneration
        
        model_id = "google/gemma-3n-E4B-it"
        
        # Charger le processeur
        print("üì• Chargement du processeur...")
        processor = AutoProcessor.from_pretrained(
            model_id,
            trust_remote_code=True
        )
        print("‚úÖ Processeur charg√© avec succ√®s")
        
        # Strat√©gies de chargement
        strategies = [
            ("CPU Conservateur", lambda: Gemma3nForConditionalGeneration.from_pretrained(
                model_id,
                device_map="cpu",
                torch_dtype=torch.float32,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                max_memory={"cpu": "8GB"}
            )),
            ("4-bit Quantization", lambda: Gemma3nForConditionalGeneration.from_pretrained(
                model_id,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16
            )),
            ("8-bit Quantization", lambda: Gemma3nForConditionalGeneration.from_pretrained(
                model_id,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                load_in_8bit=True
            )),
            ("Gestion m√©moire personnalis√©e", lambda: Gemma3nForConditionalGeneration.from_pretrained(
                model_id,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                max_memory={0: "4GB", "cpu": "8GB"}
            ))
        ]
        
        # Tester chaque strat√©gie
        for name, strategy in strategies:
            print(f"\nüîÑ Test de la strat√©gie : {name}")
            try:
                model = strategy()
                print(f"‚úÖ {name} : SUCC√àS")
                
                # Test rapide de g√©n√©ration
                print("üß™ Test de g√©n√©ration...")
                test_input = processor.apply_chat_template(
                    [{"role": "user", "content": [{"type": "text", "text": "Hello"}]}],
                    add_generation_prompt=True,
                    tokenize=True,
                    return_dict=True,
                    return_tensors="pt"
                ).to(model.device)
                
                with torch.inference_mode():
                    output = model.generate(
                        **test_input,
                        max_new_tokens=10,
                        do_sample=False
                    )
                
                print("‚úÖ G√©n√©ration r√©ussie")
                return model, processor, name
                
            except Exception as e:
                error_msg = str(e)
                print(f"‚ùå {name} : √âCHEC")
                print(f"   Erreur : {error_msg}")
                
                if "disk_offload" in error_msg:
                    print("   ‚Üí Erreur de disk_offload d√©tect√©e")
                elif "out of memory" in error_msg.lower():
                    print("   ‚Üí Erreur de m√©moire insuffisante")
                elif "bitsandbytes" in error_msg.lower():
                    print("   ‚Üí Erreur de bitsandbytes (quantization)")
                
                continue
        
        print("\n‚ùå Toutes les strat√©gies ont √©chou√©")
        return None, None, None
        
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©rale : {e}")
        return None, None, None

def main():
    """Fonction principale"""
    print("üå± Test de chargement du mod√®le AgriLens AI")
    print("=" * 50)
    
    # V√©rifier les d√©pendances
    print("üì¶ V√©rification des d√©pendances...")
    try:
        import transformers
        import accelerate
        print(f"‚úÖ Transformers : {transformers.__version__}")
        print(f"‚úÖ Accelerate : {accelerate.__version__}")
    except ImportError as e:
        print(f"‚ùå D√©pendance manquante : {e}")
        return
    
    # Tester la m√©moire
    gpu_memory = test_memory_availability()
    
    # Tester le chargement du mod√®le
    model, processor, strategy_name = test_model_loading()
    
    if model and processor:
        print(f"\nüéâ SUCC√àS ! Mod√®le charg√© avec la strat√©gie : {strategy_name}")
        print("‚úÖ L'application devrait fonctionner correctement")
        
        # Nettoyer la m√©moire
        del model
        del processor
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
    else:
        print("\n‚ùå √âCHEC ! Aucune strat√©gie n'a fonctionn√©")
        print("\nüí° Recommandations :")
        print("1. V√©rifiez que vous avez suffisamment de m√©moire RAM (8GB minimum)")
        print("2. Si vous utilisez Hugging Face Spaces, essayez un runtime avec plus de m√©moire")
        print("3. Installez les d√©pendances : pip install bitsandbytes")
        print("4. Red√©marrez l'application")

if __name__ == "__main__":
    main() 