import streamlit as st
import os
import io
from PIL import Image
# import requests # Plus n√©cessaire si on n'utilise pas de requ√™tes externes pour d'autres IA
import torch
# import google.generativeai as genai # Supprim√© car non utilis√© par Gemma
import gc
import time
import sys
import psutil
from transformers import AutoProcessor, Gemma3nForConditionalGeneration
from huggingface_hub import HfFolder

# --- Configuration de la Page ---
st.set_page_config(
    page_title="AgriLens AI - Diagnostic des Plantes",
    page_icon="üå±",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# --- Traduction (Simplifi√©e pour cet exemple) ---
# Dans une vraie application, vous utiliseriez un syst√®me de traduction plus robuste.
# Pour cet exemple, nous allons utiliser un dictionnaire simple.
TRANSLATIONS = {
    "title": {"fr": "AgriLens AI", "en": "AgriLens AI"},
    "subtitle": {"fr": "Votre assistant IA pour le diagnostic des plantes", "en": "Your AI Assistant for Plant Diagnosis"},
    "config_title": {"fr": "Configuration", "en": "Configuration"},
    "load_model": {"fr": "Charger le mod√®le", "en": "Load Model"},
    "image_analysis_title": {"fr": "üì∏ Analyse d'Image", "en": "üì∏ Image Analysis"},
    "image_analysis_desc": {"fr": "T√©l√©chargez une image de votre plante ou utilisez votre webcam pour obtenir un diagnostic.", "en": "Upload an image of your plant or use your webcam to get a diagnosis."},
    "choose_image": {"fr": "Choisissez une image", "en": "Choose an image"},
    "file_too_large_error": {"fr": "Erreur : Le fichier est trop volumineux. Maximum 200MB.", "en": "Error: File too large. Maximum 200MB."},
    "empty_file_error": {"fr": "Erreur : Le fichier est vide.", "en": "Error: File is empty."},
    "file_size_warning": {"fr": "Attention : Le fichier est tr√®s volumineux, le chargement peut prendre du temps.", "en": "Warning: File is very large, loading may take time."},
    "analyze_button": {"fr": "Analyser l'image", "en": "Analyze Image"},
    "analysis_results": {"fr": "R√©sultats de l'analyse :", "en": "Analysis Results:"},
    "text_analysis_title": {"fr": "üí¨ Analyse de Texte", "en": "üí¨ Text Analysis"},
    "text_analysis_desc": {"fr": "D√©crivez les sympt√¥mes de votre plante pour obtenir des conseils.", "en": "Describe your plant's symptoms to get advice."},
    "symptoms_desc": {"fr": "D√©crivez les sympt√¥mes ici...", "en": "Describe the symptoms here..."},
    "manual_title": {"fr": "üìö Manuel d'utilisation", "en": "üìö User Manual"},
    "about_title": {"fr": "‚ÑπÔ∏è √Ä propos", "en": "‚ÑπÔ∏è About"},
    "footer": {"fr": "¬© 2024 AgriLens AI. Tous droits r√©serv√©s.", "en": "¬© 2024 AgriLens AI. All rights reserved."}
}

def t(key):
    """Fonction de traduction simple."""
    lang = st.session_state.get('language', 'fr')
    return TRANSLATIONS.get(key, {}).get(lang, key) # Retourne la cl√© si la traduction n'existe pas

# --- Initialisation de la langue ---
if 'language' not in st.session_state:
    st.session_state.language = 'fr'

# --- Cache global pour le mod√®le ---
if 'global_model_cache' not in st.session_state:
    st.session_state.global_model_cache = {}
if 'model_load_time' not in st.session_state:
    st.session_state.model_load_time = None
if 'model_persistence_check' not in st.session_state:
    st.session_state.model_persistence_check = False

def check_model_persistence():
    """V√©rifie si le mod√®le est toujours persistant en m√©moire et fonctionnel."""
    try:
        if hasattr(st.session_state, 'model') and st.session_state.model is not None:
            if hasattr(st.session_state.model, 'device'):
                device = st.session_state.model.device
                return True
        return False
    except Exception:
        return False

def force_model_persistence():
    """Stocke le mod√®le et le processeur dans le cache global pour assurer la persistance."""
    try:
        if hasattr(st.session_state, 'model') and st.session_state.model is not None:
            st.session_state.global_model_cache['model'] = st.session_state.model
            st.session_state.global_model_cache['processor'] = st.session_state.processor
            st.session_state.global_model_cache['load_time'] = time.time()
            st.session_state.global_model_cache['model_type'] = type(st.session_state.model).__name__
            st.session_state.global_model_cache['processor_type'] = type(st.session_state.processor).__name__
            if hasattr(st.session_state.model, 'device'):
                st.session_state.global_model_cache['device'] = st.session_state.model.device

            if st.session_state.global_model_cache.get('model') is not None:
                st.session_state.model_persistence_check = True
                return True
        return False
    except Exception:
        return False

def restore_model_from_cache():
    """Restaure le mod√®le et le processeur depuis le cache global."""
    try:
        if 'model' in st.session_state.global_model_cache and st.session_state.global_model_cache['model'] is not None:
            cached_model = st.session_state.global_model_cache['model']
            if hasattr(cached_model, 'device'): # V√©rifie si le mod√®le est toujours valide
                st.session_state.model = cached_model
                st.session_state.processor = st.session_state.global_model_cache.get('processor')
                st.session_state.model_loaded = True
                st.session_state.model_status = "Charg√© (cache)"
                if 'load_time' in st.session_state.global_model_cache:
                    st.session_state.model_load_time = st.session_state.global_model_cache['load_time']
                return True
        return False
    except Exception:
        return False

def diagnose_loading_issues():
    """Diagnostique les probl√®mes potentiels de chargement (d√©pendances, ressources, etc.)."""
    issues = []
    
    # V√©rifier la pr√©sence du token Hugging Face
    if not HfFolder.get_token() and not os.environ.get("HF_TOKEN"):
        issues.append("‚ö†Ô∏è **Jeton Hugging Face (HF_TOKEN) non configur√©.** Le t√©l√©chargement du mod√®le pourrait √©chouer ou √™tre ralenti. Voir la section Configuration pour plus de d√©tails.")

    # V√©rifier les d√©pendances et les ressources syst√®me
    try:
        import transformers; issues.append(f"‚úÖ Transformers v{transformers.__version__}")
        import torch; issues.append(f"‚úÖ PyTorch v{torch.__version__}")
        if torch.cuda.is_available(): issues.append(f"‚úÖ CUDA disponible : {torch.cuda.get_device_name(0)}")
        else: issues.append("‚ö†Ô∏è CUDA non disponible - utilisation CPU (plus lent)")
    except ImportError as e: issues.append(f"‚ùå D√©pendance manquante : ")

    try:
        mem = psutil.virtual_memory()
        issues.append(f"üíæ RAM disponible : {mem.available // (1024**3)} GB")
        if mem.available < 4 * 1024**3: # Moins de 4GB RAM est critique
            issues.append("‚ö†Ô∏è RAM insuffisante (< 4GB) - Le chargement risque d'√©chouer.")
    except ImportError: issues.append("‚ö†Ô∏è Impossible de v√©rifier la m√©moire syst√®me")

    return issues

def resize_image_if_needed(image, max_size=(800, 800)):
    """Redimensionne une image PIL si elle d√©passe `max_size` tout en conservant les proportions."""
    width, height = image.size
    if width > max_size[0] or height > max_size[1]:
        ratio = min(max_size[0] / width, max_size[1] / height)
        new_width = int(width * ratio)
        new_height = int(height * ratio)
        resized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        return resized_image, True # True: redimensionn√©e
    return image, False # False: non redimensionn√©e

def afficher_ram_disponible(context=""):
    """Affiche l'utilisation de la RAM."""
    try:
        mem = psutil.virtual_memory()
        st.info(f"üíæ RAM : {mem.available // (1024**3)} GB disponible")
        if mem.available < 4 * 1024**3:
            st.warning("‚ö†Ô∏è Moins de 4GB de RAM disponible, le chargement du mod√®le risque d'√©chouer !")
    except ImportError:
        st.warning("‚ö†Ô∏è Impossible de v√©rifier la RAM syst√®me.")

# --- Fonctions d'Analyse avec Gemma 3n E4B IT ---
MODEL_ID_HF = "google/gemma-3n-E4B-it"
LOCAL_MODEL_PATH = "D:/Dev/model_gemma" # Chemin vers votre mod√®le local (ajustez si n√©cessaire)

def load_model_strategy(model_identifier, device_map=None, torch_dtype=None, quantization=None, force_persistence=False):
    """
    Charge un mod√®le et son processeur en utilisant des param√®tres sp√©cifiques.
    Retourne le mod√®le et le processeur, ou (None, None) en cas d'√©chec.
    """
    try:
        st.info(f"Chargement de  avec device_map='{device_map}', dtype=torch.{torch_dtype.__name__ if torch_dtype else 'None'}, quant='{quantization}'")
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # Configuration des arguments pour from_pretrained
        common_args = {
            "trust_remote_code": True,
            "low_cpu_mem_usage": True,
            "device_map": device_map,
            "torch_dtype": torch_dtype,
            "token": os.environ.get("HF_TOKEN") # Utilise le token si d√©fini
        }
        
        # Ajout des arguments de quantification si sp√©cifi√©s
        if quantization == "4bit":
            common_args.update({
                "load_in_4bit": True,
                "bnb_4bit_compute_dtype": torch.float16 # Souvent utilis√© avec 4-bit
            })
        elif quantization == "8bit":
            common_args.update({"load_in_8bit": True})
        
        # Charger le processeur
        processor = AutoProcessor.from_pretrained(model_identifier, trust_remote_code=True, token=os.environ.get("HF_TOKEN"))
        
        # Charger le mod√®le
        model = Gemma3nForConditionalGeneration.from_pretrained(model_identifier, **common_args)
        
        afficher_ram_disponible("apr√®s chargement")
        
        # Stocker dans session_state et forcer la persistance
        st.session_state.model = model
        st.session_state.processor = processor
        st.session_state.model_loaded = True
        st.session_state.model_status = f"Charg√© ({device_map or 'auto'})"
        st.session_state.model_load_time = time.time()
        
        if force_persistence:
            if force_model_persistence():
                st.success("Mod√®le charg√© et persistant.")
            else:
                st.warning("Mod√®le charg√© mais probl√®me de persistance.")
        
        return model, processor

    except ImportError as e:
        raise ImportError(f"Biblioth√®que manquante : . Installez-la avec `pip install transformers torch accelerate bitsandbytes`")
    except Exception as e:
        # Capturez l'exception sp√©cifique ici pour un meilleur d√©bogage
        raise Exception(f"√âchec du chargement avec la strat√©gie  : ")

def load_model():
    """Charge le mod√®le Gemma 3n E4B IT (local ou Hugging Face) avec des strat√©gies robustes."""
    try:
        # Diagnostic initial
        issues = diagnose_loading_issues()
        with st.expander("üìä Diagnostic syst√®me", expanded=False):
            for issue in issues:
                st.markdown(issue)

        # Nettoyer la m√©moire avant le chargement
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # D√©tecter si le mod√®le local existe
        is_local = os.path.exists(LOCAL_MODEL_PATH)

        # --- Strat√©gies de chargement ---
        strategies_to_try = []

        if is_local:
            # Strat√©gies pour le mod√®le local (priorit√© haute RAM)
            strategies_to_try.append(("Local (ultra-conservateur CPU)", lambda: load_model_strategy(LOCAL_MODEL_PATH, device_map="cpu", torch_dtype=torch.bfloat16, quantization=None, force_persistence=True)))
            strategies_to_try.append(("Local (conservateur CPU)", lambda: load_model_strategy(LOCAL_MODEL_PATH, device_map="cpu", torch_dtype=torch.bfloat16, quantization=None, force_persistence=True)))
        else:
            # Strat√©gies pour le mod√®le Hugging Face
            st.info("Mod√®le local non trouv√©. Tentative de chargement depuis Hugging Face...")
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3 # en GB
                st.info(f"M√©moire GPU disponible : {gpu_memory:.1f} GB")
                
                # Strat√©gies GPU bas√©es sur la m√©moire
                # Note: Gemma 3n E4B IT est assez grand, ajuster les seuils si n√©cessaire
                if gpu_memory >= 10: # 10 GB pour une exp√©rience plus fluide avec float16
                    strategies_to_try.append(("Hugging Face (float16)", lambda: load_model_strategy(MODEL_ID_HF, device_map="auto", torch_dtype=torch.float16, quantization=None)))
                if gpu_memory >= 8: # 8 GB pour une version quantifi√©e 8-bit
                    strategies_to_try.append(("Hugging Face (8-bit quantization)", lambda: load_model_strategy(MODEL_ID_HF, device_map="auto", torch_dtype=torch.float16, quantization="8bit")))
                if gpu_memory >= 6: # 6 GB pour une version quantifi√©e 4-bit
                    strategies_to_try.append(("Hugging Face (4-bit quantization)", lambda: load_model_strategy(MODEL_ID_HF, device_map="auto", torch_dtype=torch.float16, quantization="4bit")))
                
                # Si la m√©moire est moindre, une strat√©gie CPU est plus s√ªre
                if gpu_memory < 6:
                     st.warning("M√©moire GPU limit√©e. L'utilisation du CPU sera probablement plus stable.")

            # Strat√©gie CPU (par d√©faut ou si GPU insuffisant/absent)
            # Utiliser float32 pour plus de stabilit√© sur CPU
            strategies_to_try.append(("Hugging Face (conservative CPU)", lambda: load_model_strategy(MODEL_ID_HF, device_map="cpu", torch_dtype=torch.float32, quantization=None)))
            strategies_to_try.append(("Hugging Face (ultra-conservative CPU)", lambda: load_model_strategy(MODEL_ID_HF, device_map="cpu", torch_dtype=torch.float32, quantization=None)))

        # Boucle pour essayer chaque strat√©gie
        for i, (name, strategy_func) in enumerate(strategies_to_try):
            st.info(f"Tentative {i+1}/{len(strategies_to_try)} : Chargement via '{name}'...")
            try:
                model, processor = strategy_func()
                if model and processor:
                    st.success(f"‚úÖ Mod√®le charg√© avec succ√®s via la strat√©gie : '{name}'")
                    return model, processor
            except Exception as e:
                error_msg = str(e)
                st.warning(f"La strat√©gie '{name}' a √©chou√© : {error_msg}")
                if "disk_offload" in error_msg.lower() or "out of memory" in error_msg.lower():
                    st.warning("Probl√®me de m√©moire ou de disk_offload. Tentative suivante...")
                elif "403" in error_msg or "Forbidden" in error_msg:
                    st.error(f"‚ùå Erreur d'acc√®s Hugging Face (403) avec la strat√©gie '{name}'. V√©rifiez votre HF_TOKEN.")
                    # Ne pas continuer si c'est une erreur d'authentification critique
                    return None, None 
                else:
                    st.warning("Tentative suivante...")
                
                # Nettoyage m√©moire apr√®s √©chec
                gc.collect()
                if torch.cuda.is_available(): torch.cuda.empty_cache()
                
                continue # Passer √† la strat√©gie suivante
        
        # Si toutes les strat√©gies √©chouent
        st.error("Toutes les strat√©gies de chargement du mod√®le ont √©chou√©.")
        return None, None

    except ImportError as e:
        st.error(f"‚ùå Erreur de d√©pendance : . Installez avec `pip install transformers torch accelerate bitsandbytes`.")
        return None, None
    except Exception as e:
        st.error(f"‚ùå Une erreur g√©n√©rale s'est produite lors du chargement du mod√®le : ")
        return None, None

def analyze_image_multilingual(image, prompt=""):
    """Analyse une image avec Gemma 3n E4B IT pour un diagnostic pr√©cis."""
    if not st.session_state.model_loaded and not restore_model_from_cache():
        return "‚ùå Mod√®le Gemma non charg√©. Veuillez d'abord charger le mod√®le dans les r√©glages."
    
    model, processor = st.session_state.model, st.session_state.processor
    if not model or not processor:
        return "‚ùå Mod√®le Gemma non disponible. Veuillez recharger le mod√®le."

    try:
        # Pr√©parer le prompt textuel pour Gemma 3n
        # Le token <image> est crucial pour que le mod√®le sache o√π ins√©rer les informations visuelles
        if st.session_state.language == "fr":
            gemma_prompt_text = f"<image>\nTu es un expert en pathologie v√©g√©tale. Analyse cette image et r√©ponds √† la question : " if prompt else "<image>\nTu es un expert en pathologie v√©g√©tale. Analyse cette image et fournis un diagnostic pr√©cis."
        else: # English
            gemma_prompt_text = f"<image>\nYou are an expert in plant pathology. Analyze this image and answer the question: " if prompt else "<image>\nYou are an expert in plant pathology. Analyze this image and provide a precise diagnosis."
        
        # Pr√©traiter l'image et le texte avec le processeur pour obtenir les inputs du mod√®le
        # C'est la m√©thode correcte pour les mod√®les multimodaux qui attendent des inputs combin√©s.
        processed_inputs = processor(
            images=[image], # L'image PIL doit √™tre dans une liste
            text=gemma_prompt_text, # Le prompt textuel contenant <image>
            return_tensors="pt",
            padding=True
        )

        # D√©placer les inputs au bon device
        device = getattr(model, 'device', 'cpu')
        if hasattr(processed_inputs, 'to'):
            processed_inputs = processed_inputs.to(device)
        
        # G√©n√©rer la r√©ponse
        with torch.inference_mode():
            generation = model.generate(
                **processed_inputs, # Passer les inputs pr√©par√©s par le processor
                max_new_tokens=500,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )
            # D√©code la r√©ponse compl√®te g√©n√©r√©e
            response_text = processor.decode(generation[0], skip_special_tokens=True)

        # Nettoyer la r√©ponse pour enlever le prompt r√©p√©t√© et les tokens sp√©ciaux
        final_response = response_text.strip()
        
        # Supprimer le prompt texte initial si le mod√®le l'a r√©p√©t√©
        if gemma_prompt_text.strip() in final_response:
            # Trouver la premi√®re occurrence du prompt textuel et prendre ce qui suit
            try:
                prompt_start_index = final_response.index(gemma_prompt_text.strip())
                final_response = final_response[prompt_start_index + len(gemma_prompt_text.strip()):].strip()
            except ValueError:
                # Si le prompt n'est pas trouv√© exactement (ce qui est peu probable ici), on le laisse tel quel
                pass
            
        # Retirer les tokens sp√©ciaux inutiles de la r√©ponse
        final_response = final_response.replace("<start_of_turn>", "").replace("<end_of_turn>", "").strip()

        # Formater la sortie
        if st.session_state.language == "fr":
            return f"""
## üß† **Analyse par Gemma 3n E4B IT**

{final_response}
"""
        else:
            return f"""
## üß† **Analysis by Gemma 3n E4B IT**

{final_response}
"""
            
    except Exception as e:
        error_message = str(e)
        if "403" in error_message or "Forbidden" in error_message:
            return "‚ùå Erreur 403 - Acc√®s refus√©. Veuillez v√©rifier votre jeton Hugging Face (HF_TOKEN) et les quotas."
        elif "Number of images does not match number of special image tokens" in error_message:
            return "‚ùå Erreur : Le mod√®le n'a pas pu lier l'image au texte. Assurez-vous que le prompt contient bien le token `<image>` et que l'image est dans un format standard."
        else:
            return f"‚ùå Erreur lors de l'analyse d'image : {e}"

def analyze_text_multilingual(text):
    """Analyse un texte avec le mod√®le Gemma 3n E4B IT."""
    if not st.session_state.model_loaded and not restore_model_from_cache():
        return "‚ùå Mod√®le non charg√©. Veuillez le charger dans les r√©glages."
    
    model, processor = st.session_state.model, st.session_state.processor
    if not model or not processor:
        return "‚ùå Mod√®le Gemma non disponible. Veuillez recharger le mod√®le."
    
    try:
        # D√©finir le prompt bas√© sur la langue
        if st.session_state.language == "fr":
            prompt_template = f"Tu es un assistant agricole expert. Analyse ce probl√®me de plante : \n\n**Instructions :**\n1. **Diagnostic** : Quel est le probl√®me principal ?\n2. **Causes** : Quelles sont les causes possibles ?\n3. **Traitement** : Quelles sont les actions √† entreprendre ?\n4. **Pr√©vention** : Comment √©viter le probl√®me √† l'avenir ?"
        else:
            prompt_template = f"You are an expert agricultural assistant. Analyze this plant problem: \n\n**Instructions:**\n1. **Diagnosis**: What is the main problem?\n2. **Causes**: What are the possible causes?\n3. **Treatment**: What actions should be taken?\n4. **Prevention**: How to avoid the problem in the future?"
        
        full_prompt = f"{prompt_template}\n\n**Description du probl√®me :**\n{text}"

        # Pr√©parer les messages pour le mod√®le Gemma (format conversationnel)
        messages = [{"role": "user", "content": [{"type": "text", "text": full_prompt}]}]
        
        # Utiliser le processeur pour convertir le format conversationnel en tenseurs
        inputs = processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
        )
        
        # D√©placer les inputs au bon device
        device = getattr(model, 'device', 'cpu')
        if hasattr(inputs, 'to'):
            inputs = inputs.to(device)
        
        # G√©n√©rer la r√©ponse
        with torch.inference_mode():
            generation = model.generate(
                **inputs,
                max_new_tokens=500, # Augment√© pour des r√©ponses plus compl√®tes
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )
            # D√©coder uniquement la partie g√©n√©r√©e (apr√®s le prompt)
            # Il est important de trouver le bon d√©calage pour ne d√©coder que la r√©ponse.
            # `apply_chat_template` peut ajouter des tokens de d√©but/fin de tour.
            # Une approche plus s√ªre est de d√©coder toute la s√©quence et de voir ce qu'on obtient.
            
            # Si apply_chat_template ajoute des tokens, on pourrait les retirer
            # Mais pour simplifier, d√©codons toute la s√©quence g√©n√©r√©e.
            response = processor.decode(generation[0], skip_special_tokens=True)

            # On essaie de retirer le prompt initial si le mod√®le l'a r√©p√©t√©
            # Le format de chat de Gemma peut inclure <start_of_turn>, <end_of_turn>, etc.
            # Il faut √™tre prudent lors du nettoyage.
            
            # La m√©thode la plus simple est de d√©coder et de nettoyer les tokens sp√©ciaux
            # Si le prompt_template est r√©p√©t√©, il faudra le retirer manuellement.
            cleaned_response = response.strip()
            # On peut tenter de retirer le prompt de base, mais attention aux variations
            if prompt_template in cleaned_response:
                cleaned_response = cleaned_response.split(prompt_template)[-1].strip()
            
            # Retirer les tokens de chat sp√©cifiques si pr√©sents
            cleaned_response = cleaned_response.replace("<start_of_turn>", "").replace("<end_of_turn>", "").strip()

        return cleaned_response
        
    except Exception as e:
        return f"‚ùå Erreur lors de l'analyse de texte : {e}"

# --- Interface Principale ---
st.title(t("title"))
st.markdown(t("subtitle"))

# --- Initialisation et V√©rifications au d√©marrage ---
if 'model_loaded' not in st.session_state:
    st.session_state.model_loaded = False
if 'model_status' not in st.session_state:
    st.session_state.model_status = "Non charg√©"

# Tenter de restaurer le mod√®le au chargement de l'application si non d√©j√† charg√©
if not st.session_state.model_loaded:
    if restore_model_from_cache():
        st.success("üîÑ Mod√®le restaur√© automatiquement depuis le cache au d√©marrage.")
    else:
        st.info("üí° Cliquez sur 'Charger le mod√®le' dans les r√©glages pour commencer.")

# --- Sidebar pour la Configuration ---
with st.sidebar:
    st.header(t("config_title"))
    
    # S√©lecteur de langue
    st.subheader("üåê Langue / Language")
    language_options = ["Fran√ßais", "English"]
    # Assurer que st.session_state.language existe
    if 'language' not in st.session_state:
        st.session_state.language = 'fr'
    current_lang_index = 0 if st.session_state.language == "fr" else 1
    
    language = st.selectbox(
        "S√©lectionnez votre langue :",
        language_options,
        index=current_lang_index,
        help="Change la langue de l'interface et des r√©ponses de l'IA."
    )
    # Mettre √† jour la langue dans st.session_state et recharger si changement
    if st.session_state.language != ("fr" if language == "Fran√ßais" else "en"):
        st.session_state.language = "fr" if language == "Fran√ßais" else "en"
        st.rerun() # Recharge pour appliquer le changement de langue

    st.divider()

    # Statut du Jeton Hugging Face
    st.subheader("üîë Jeton Hugging Face")
    hf_token_found = HfFolder.get_token() or os.environ.get("HF_TOKEN")
    if hf_token_found:
        st.success("‚úÖ Jeton HF trouv√© et configur√©.")
    else:
        st.warning("‚ö†Ô∏è Jeton HF non trouv√©.")
    st.info("Il est recommand√© de d√©finir la variable d'environnement `HF_TOKEN` avec votre jeton personnel Hugging Face pour √©viter les erreurs d'acc√®s (403).")
    st.markdown("[Obtenir un jeton HF](https://huggingface.co/settings/tokens)")

    st.divider()

    # Gestion du Mod√®le IA
    st.header("ü§ñ Mod√®le IA Gemma 3n")
    if st.session_state.model_loaded and check_model_persistence():
        st.success(f"‚úÖ Mod√®le charg√© ({st.session_state.model_status})")
        if st.session_state.model_load_time:
            load_time_str = time.strftime('%H:%M:%S', time.localtime(st.session_state.model_load_time))
            st.write(f"Heure de chargement : ")
        if hasattr(st.session_state.model, 'device'):
            st.write(f"Device utilis√© : `{st.session_state.model.device}`")
        
        col1_btn, col2_btn = st.columns(2)
        with col1_btn:
            if st.button("üîÑ Recharger le mod√®le", type="secondary"):
                st.session_state.model_loaded = False
                st.session_state.model = None
                st.session_state.processor = None
                st.session_state.global_model_cache.clear() # Vider le cache pour recharger
                st.session_state.model_persistence_check = False
                st.rerun()
        with col2_btn:
            if st.button("üíæ Forcer Persistance", type="secondary"):
                if force_model_persistence():
                    st.success("Persistance forc√©e avec succ√®s.")
                else:
                    st.error("√âchec de la persistance.")
                st.rerun()
    else:
        st.warning("‚ùå Mod√®le non charg√©.")
        if st.button(t("load_model"), type="primary"):
            with st.spinner("Chargement du mod√®le en cours..."):
                model, processor = load_model()
                if model and processor:
                    st.success("‚úÖ Mod√®le charg√© avec succ√®s !")
                else:
                    st.error("‚ùå √âchec du chargement du mod√®le.")
            st.rerun() # Recharger pour mettre √† jour le statut

    # Affichage du statut de persistance
    st.divider()
    st.subheader("üíæ Persistance du Mod√®le")
    if st.session_state.model_loaded and st.session_state.model_persistence_check:
        st.success("‚úÖ Mod√®le charg√© et persistant en cache.")
    elif st.session_state.model_loaded:
        st.warning("‚ö†Ô∏è Mod√®le charg√© mais non persistant. Cliquez sur 'Forcer Persistance'.")
    else:
        st.warning("‚ö†Ô∏è Mod√®le non charg√©.")


# --- Onglets Principaux ---
tab1, tab2, tab3, tab4 = st.tabs([t("image_analysis_title"), t("text_analysis_title"), t("manual_title"), t("about_title")])

with tab1:
    st.header(t("image_analysis_title"))
    st.markdown(t("image_analysis_desc"))
    
    capture_option = st.radio(
        "Choisissez votre m√©thode :",
        ["üìÅ Upload d'image", "üì∑ Capture par webcam"],
        horizontal=True,
        key="image_capture_method"
    )
    
    uploaded_file = None
    captured_image = None
    
    if capture_option == "üìÅ Upload d'image":
        uploaded_file = st.file_uploader(
            t("choose_image"),
            type=['png', 'jpg', 'jpeg'],
            help="Formats accept√©s : PNG, JPG, JPEG (max 200MB). Privil√©giez des images claires.",
            accept_multiple_files=False,
            key="image_uploader"
        )
        if uploaded_file is not None:
            MAX_FILE_SIZE_BYTES = 200 * 1024 * 1024 # 200 MB
            if uploaded_file.size > MAX_FILE_SIZE_BYTES:
                st.error(t("file_too_large_error"))
                uploaded_file = None
            elif uploaded_file.size == 0:
                st.error(t("empty_file_error"))
                uploaded_file = None
            elif uploaded_file.size > (MAX_FILE_SIZE_BYTES * 0.8): # Avertissement si tr√®s volumineux
                st.warning(t("file_size_warning"))
    else: # Webcam capture
        st.markdown("**üì∑ Capture d'image par webcam**")
        st.info("üí° Positionnez votre plante malade devant la webcam et cliquez sur 'Prendre une photo'. Assurez-vous d'un bon √©clairage.")
        captured_image = st.camera_input("Prendre une photo de la plante", key="webcam_capture")
    
    image = None
    image_source = None
    
    if uploaded_file is not None:
        try:
            image = Image.open(uploaded_file)
            image_source = "upload"
        except Exception as e:
            st.error(f"‚ùå Erreur lors du traitement de l'image upload√©e : ")
            st.info("üí° Essayez avec une image diff√©rente ou un format diff√©rent (PNG, JPG, JPEG).")
    elif captured_image is not None:
        try:
            image = Image.open(captured_image)
            image_source = "webcam"
        except Exception as e:
            st.error(f"‚ùå Erreur lors du traitement de l'image captur√©e : ")
            st.info("üí° Essayez de reprendre la photo.")
    
    if image is not None:
        try:
            original_size = image.size
            image, was_resized = resize_image_if_needed(image, max_size=(800, 800))
            
            col1, col2 = st.columns([1, 1])
            with col1:
                st.image(image, caption=f"Image ()" if image_source else "Image", use_container_width=True)
                if was_resized:
                    st.warning(f"‚ö†Ô∏è L'image a √©t√© redimensionn√©e de  √† {image.size} pour optimiser le traitement.")
            
            with col2:
                st.markdown("**Informations de l'image :**")
                st.write(f"‚Ä¢ Format : {image.format}")
                st.write(f"‚Ä¢ Taille originale : {original_size[0]}x{original_size[1]} pixels")
                st.write(f"‚Ä¢ Taille actuelle : {image.size[0]}x{image.size[1]} pixels")
                st.write(f"‚Ä¢ Mode : {image.mode}")
            
            question = st.text_area(
                "Question sp√©cifique (optionnel) :",
                placeholder="Ex: Les feuilles ont des taches jaunes, que faire ?",
                height=100
            )
            
            if st.button(t("analyze_button"), disabled=not st.session_state.model_loaded, type="primary"):
                if not st.session_state.model_loaded:
                    st.error("‚ùå Mod√®le non charg√©. Veuillez le charger dans les r√©glages.")
                else:
                    with st.spinner("üîç Analyse d'image en cours..."):
                        result = analyze_image_multilingual(image, question)
                    
                    st.markdown(t("analysis_results"))
                    st.markdown("---")
                    st.markdown(result)
        except Exception as e:
            st.error(f"Erreur lors du traitement de l'image : ")

with tab2:
    st.header(t("text_analysis_title"))
    st.markdown(t("text_analysis_desc"))
    
    text_input = st.text_area(
        t("symptoms_desc"),
        placeholder="Ex: Mes tomates ont des taches brunes sur les feuilles et les fruits, une poudre blanche sur les tiges...",
        height=150
    )
    
    if st.button("üß† Analyser avec l'IA", disabled=not st.session_state.model_loaded, type="primary"):
        if not st.session_state.model_loaded:
            st.error("‚ùå Mod√®le non charg√©. Veuillez le charger dans les r√©glages.")
        elif not text_input.strip():
            st.error("‚ùå Veuillez saisir une description des sympt√¥mes.")
        else:
            with st.spinner("üîç Analyse de texte en cours..."):
                result = analyze_text_multilingual(text_input)
            
            st.markdown(t("analysis_results"))
            st.markdown("---")
            st.markdown(result)

with tab3:
    st.header(t("manual_title"))
    
    manual_content = {
        "fr": """
        ### üöÄ **D√©marrage Rapide**
        1.  **Charger le mod√®le** : Cliquez sur 'Charger le mod√®le' dans les r√©glages (sidebar).
        2.  **Choisir le mode** : Allez √† l'onglet 'üì∏ Analyse d'Image' ou 'üí¨ Analyse de Texte'.
        3.  **Soumettre votre demande** : Upload d'image, capture webcam, ou description textuelle.
        4.  **Obtenir le diagnostic** : Lisez les r√©sultats avec recommandations.
        
        ### üì∏ **Analyse d'Image**
        *   **Formats accept√©s** : PNG, JPG, JPEG.
        *   **Qualit√©** : Privil√©giez des images claires, bien √©clair√©es, avec le probl√®me bien visible.
        *   **Redimensionnement** : Les images trop grandes sont automatiquement redimensionn√©es pour optimiser le traitement.
        
        ### üí¨ **Analyse de Texte**
        *   **Soyez pr√©cis** : D√©crivez les sympt√¥mes, le type de plante, les conditions de culture, et les actions d√©j√† tent√©es. Plus la description est d√©taill√©e, plus le diagnostic sera pertinent.
        
        ### üîç **Interpr√©tation des R√©sultats**
        *   Les r√©sultats incluent un diagnostic potentiel, les causes probables, des recommandations de traitement et des conseils de pr√©vention.
        *   Ces informations sont bas√©es sur l'IA et doivent √™tre consid√©r√©es comme un guide. Consultez un expert pour des cas critiques.
        
        ### üí° **Bonnes Pratiques**
        *   **Images multiples** : Si possible, prenez des photos sous diff√©rents angles.
        *   **√âclairage** : La lumi√®re naturelle est id√©ale.
        *   **Focus** : Assurez-vous que la zone affect√©e est nette et bien visible.
        
        ### üíæ **Persistance du Mod√®le**
        *   Une fois charg√©, le mod√®le est sauvegard√© dans le cache de l'application pour un chargement plus rapide lors des prochaines utilisations sur le m√™me environnement.
        *   Vous pouvez le recharger manuellement si n√©cessaire.
        
        ### üîí **Jeton Hugging Face (HF_TOKEN)**
        *   Pour garantir la stabilit√© et la performance lors du t√©l√©chargement de mod√®les depuis Hugging Face, il est fortement recommand√© de d√©finir la variable d'environnement `HF_TOKEN`.
        *   Cr√©ez un jeton de lecture sur [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) et d√©finissez-le dans votre environnement avant de lancer l'application.
        """,
        "en": """
        ### üöÄ **Quick Start**
        1.  **Load the model** : Click 'Load Model' in the settings (sidebar).
        2.  **Choose mode** : Navigate to 'üì∏ Image Analysis' or 'üí¨ Text Analysis' tab.
        3.  **Submit your request** : Upload an image, capture via webcam, or provide a text description.
        4.  **Get diagnosis** : Read the results with recommendations.
        
        ### üì∏ **Image Analysis**
        *   **Accepted formats** : PNG, JPG, JPEG.
        *   **Quality** : Prefer clear, well-lit images with the problem clearly visible.
        *   **Resizing** : Oversized images are automatically resized for processing optimization.
        
        ### üí¨ **Text Analysis**
        *   **Be specific** : Describe symptoms, plant type, growing conditions, and actions already taken. More detail leads to better accuracy.
        
        ### üîç **Result Interpretation**
        *   Results include a potential diagnosis, likely causes, treatment recommendations, and preventive advice.
        *   This AI-driven information is for guidance only. Consult a qualified expert for critical cases.
        
        ### üí° **Best Practices**
        *   **Multiple images** : If possible, take photos from different angles.
        *   **Lighting** : Natural light is ideal.
        *   **Focus** : Ensure the affected area is sharp and clearly visible.
        
        ### üíæ **Model Persistence**
        *   Once loaded, the model is cached for faster loading in future sessions on the same environment.
        *   You can manually reload it if needed.
        
        ### üîí **Hugging Face Token (HF_TOKEN)**
        *   To ensure stability and performance when downloading models from Hugging Face, it's highly recommended to set the environment variable `HF_TOKEN`.
        *   Create a read token on [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) and set it in your environment before launching the app.
        """
    }
    st.markdown(manual_content[st.session_state.language])

with tab4:
    st.header(t("about_title"))
    
    st.markdown("### üå± Notre Mission / Our Mission")
    st.markdown("AgriLens AI est une application de diagnostic des maladies de plantes utilisant l'intelligence artificielle pour aider les agriculteurs √† identifier et traiter les probl√®mes de leurs cultures.")
    
    st.markdown("### üöÄ Fonctionnalit√©s / Features")
    st.markdown("""
    ‚Ä¢ **Analyse d'images** : Diagnostic visuel des maladies
    ‚Ä¢ **Analyse de texte** : Conseils bas√©s sur les descriptions
    ‚Ä¢ **Recommandations pratiques** : Actions concr√®tes √† entreprendre
    ‚Ä¢ **Interface optimis√©e** : Pour une utilisation sur divers appareils
    ‚Ä¢ **Support multilingue** : Fran√ßais et Anglais
    """)
    
    st.markdown("### üîß Technologie / Technology")
    
    # D√©tecter l'environnement pour l'affichage
    is_local = os.path.exists(LOCAL_MODEL_PATH)
    
    if is_local:
        st.markdown(f"""
        ‚Ä¢ **Mod√®le** : Gemma 3n E4B IT (Local - {LOCAL_MODEL_PATH})
        ‚Ä¢ **Framework** : Streamlit
        ‚Ä¢ **D√©ploiement** : Local
        """)
    else:
        st.markdown("""
        ‚Ä¢ **Mod√®le** : Gemma 3n E4B IT (Hugging Face - en ligne)
        ‚Ä¢ **Framework** : Streamlit
        ‚Ä¢ **D√©ploiement** : Hugging Face Spaces / en ligne
        """)
    
    # Informations du cr√©ateur
    st.markdown(f"### üë®‚Äçüíª Cr√©ateur de l'Application / Application Creator")
    st.markdown(f"**Sidoine Kolaol√© YEBADOKPO**")
    st.markdown(f"üìç Bohicon, R√©publique du B√©nin")
    st.markdown(f"üìß syebadokpo@gmail.com")
    st.markdown(f"üîó [linkedin.com/in/sidoineko](https://linkedin.com/in/sidoineko)")
    st.markdown(f"üìÅ [Hugging Face Portfolio](https://huggingface.co/Sidoineko)")
    
    # Informations de comp√©tition
    st.markdown(f"### üèÜ Version Comp√©tition Kaggle / Kaggle Competition Version")
    st.markdown("Cette premi√®re version d'AgriLens AI a √©t√© d√©velopp√©e sp√©cifiquement pour participer √† une comp√©tition Kaggle.")
    
    st.markdown("### ‚ö†Ô∏è Avertissement / Warning")
    st.markdown("Les r√©sultats fournis par l'IA sont √† titre indicatif uniquement et ne remplacent pas l'avis d'un expert agricole qualifi√©.")
    
    st.markdown("### üìû Support")
    st.markdown("Pour toute question ou probl√®me, consultez la documentation ou contactez le cr√©ateur.")

# --- Pied de page ---
st.markdown("---")
st.markdown("¬© 2024 AgriLens AI. Tous droits r√©serv√©s.")