import streamlit as st
import os
import io
from PIL import Image
import requests
import torch
import google.generativeai as genai
import gc
import time
import sys

# Cache global pour le mod√®le (persiste entre les reruns)
if 'global_model_cache' not in st.session_state:
    st.session_state.global_model_cache = {}
if 'model_load_time' not in st.session_state:
    st.session_state.model_load_time = None
if 'model_persistence_check' not in st.session_state:
    st.session_state.model_persistence_check = False

def check_model_persistence():
    """V√©rifie si le mod√®le est toujours persistant en m√©moire"""
    try:
        if hasattr(st.session_state, 'model') and st.session_state.model is not None:
            # Test simple pour v√©rifier que le mod√®le fonctionne
            if hasattr(st.session_state.model, 'device'):
                device = st.session_state.model.device
                return True
        return False
    except Exception:
        return False

def force_model_persistence():
    """Force la persistance du mod√®le en m√©moire"""
    try:
        if hasattr(st.session_state, 'model') and st.session_state.model is not None:
            # Cr√©er une r√©f√©rence forte au mod√®le
            st.session_state.global_model_cache['model'] = st.session_state.model
            st.session_state.global_model_cache['processor'] = st.session_state.processor
            st.session_state.global_model_cache['load_time'] = time.time()
            
            # V√©rification imm√©diate
            if st.session_state.global_model_cache.get('model') is not None:
                st.session_state.model_persistence_check = True
                return True
        return False
    except Exception as e:
        st.error(f"Erreur lors de la persistance forc√©e : {e}")
        return False

def restore_model_from_cache():
    """Restaure le mod√®le depuis le cache global"""
    try:
        if 'model' in st.session_state.global_model_cache:
            st.session_state.model = st.session_state.global_model_cache['model']
            st.session_state.processor = st.session_state.global_model_cache['processor']
            st.session_state.model_loaded = True
            st.session_state.model_status = "Charg√© (cache)"
            return True
        return False
    except Exception:
        return False

def diagnose_loading_issues():
    """Diagnostique les probl√®mes potentiels de chargement"""
    issues = []
    
    # V√©rifier l'environnement
    if os.path.exists("D:/Dev/model_gemma"):
        issues.append("‚úÖ Mod√®le local d√©tect√©")
    else:
        issues.append("üåê Mode Hugging Face d√©tect√©")
    
    # V√©rifier les d√©pendances
    try:
        import transformers
        issues.append(f"‚úÖ Transformers version: {transformers.__version__}")
    except ImportError:
        issues.append("‚ùå Transformers non install√©")
    
    try:
        import torch
        issues.append(f"‚úÖ PyTorch version: {torch.__version__}")
        if torch.cuda.is_available():
            issues.append(f"‚úÖ CUDA disponible: {torch.cuda.get_device_name(0)}")
        else:
            issues.append("‚ö†Ô∏è CUDA non disponible - utilisation CPU")
    except ImportError:
        issues.append("‚ùå PyTorch non install√©")
    
    # V√©rifier la m√©moire disponible
    try:
        import psutil
        memory = psutil.virtual_memory()
        issues.append(f"üíæ M√©moire disponible: {memory.available // (1024**3)} GB")
    except ImportError:
        issues.append("‚ö†Ô∏è Impossible de v√©rifier la m√©moire")
    
    return issues

def resize_image_if_needed(image, max_size=(800, 800)):
    """
    Redimensionne l'image si elle d√©passe la taille maximale sp√©cifi√©e
    """
    width, height = image.size
    
    if width > max_size[0] or height > max_size[1]:
        # Calculer le ratio pour maintenir les proportions
        ratio = min(max_size[0] / width, max_size[1] / height)
        new_width = int(width * ratio)
        new_height = int(height * ratio)
        
        # Redimensionner l'image
        resized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        return resized_image, True  # True indique que l'image a √©t√© redimensionn√©e
    else:
        return image, False  # False indique que l'image n'a pas √©t√© redimensionn√©e

# Configuration de la page
st.set_page_config(
    page_title="AgriLens AI - Plant Disease Diagnosis",
    page_icon="üå±",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# Configuration pour √©viter les erreurs 403
st.markdown("""
<script>
// D√©sactiver les v√©rifications CORS pour les uploads
window.addEventListener('load', function() {
    if (typeof window.parent !== 'undefined' && window.parent !== window) {
        // Si l'app est dans un iframe (comme sur Hugging Face Spaces)
        console.log('Application d√©tect√©e dans un iframe - configuration sp√©ciale activ√©e');
    }
});
</script>
""", unsafe_allow_html=True)

# CSS pour mobile
st.markdown("""
<style>
@media (max-width: 600px) {
    .main {
        max-width: 100vw !important;
        padding: 0.5rem !important;
    }
    .stButton button, .stTextInput input, .stTextArea textarea {
        width: 100% !important;
        font-size: 1.1rem !important;
    }
    .stSidebar {
        width: 100vw !important;
        min-width: 100vw !important;
    }
    .result-box {
        font-size: 1.05rem !important;
    }
    .stMarkdown, .stHeader, .stSubheader {
        font-size: 1.1rem !important;
    }
    .stFileUploader, .stImage {
        width: 100% !important;
    }
}
</style>
""", unsafe_allow_html=True)

# Initialisation des variables de session
if 'model_loaded' not in st.session_state:
    st.session_state.model_loaded = False
if 'model_status' not in st.session_state:
    st.session_state.model_status = "Non charg√©"
if 'language' not in st.session_state:
    st.session_state.language = "fr"

# Configuration Gemini API
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)
    gemini_model = genai.GenerativeModel('gemini-1.5-flash')
else:
    gemini_model = None

# Dictionnaires de traductions
translations = {
    "fr": {
        "title": "üå± AgriLens AI - Diagnostic des Plantes",
        "subtitle": "**Application de diagnostic des maladies de plantes avec IA**",
        "config_title": "‚öôÔ∏è Configuration",
        "load_model": "Charger le mod√®le Gemma 3n E4B IT",
        "model_status": "**Statut du mod√®le :**",
        "not_loaded": "Non charg√©",
        "loaded": "‚úÖ Charg√©",
        "error": "‚ùå Erreur",
        "tabs": ["üì∏ Analyse d'Image", "üí¨ Analyse de Texte", "üìñ Manuel", "‚ÑπÔ∏è √Ä propos"],
        "image_analysis_title": "üîç Diagnostic par Image",
        "image_analysis_desc": "T√©l√©chargez une photo de plante malade pour obtenir un diagnostic",
        "choose_image": "Choisissez une image...",
        "analyze_button": "üî¨ Analyser avec l'IA",
        "text_analysis_title": "üí¨ Diagnostic par Texte",
        "text_analysis_desc": "D√©crivez les sympt√¥mes de votre plante pour obtenir des conseils",
        "symptoms_desc": "Description des sympt√¥mes :",
        "analysis_results": "## üìä R√©sultats de l'Analyse",
        "manual_title": "üìñ Manuel Utilisateur",
        "about_title": "‚ÑπÔ∏è √Ä propos d'AgriLens AI",
        "creator_title": "üë®‚Äçüíª Cr√©ateur de l'Application",
        "creator_name": "**Sidoine Kolaol√© YEBADOKPO**",
        "creator_location": "Bohicon, R√©publique du B√©nin",
        "creator_phone": "+229 01 96 91 13 46",
        "creator_email": "syebadokpo@gmail.com",
        "creator_linkedin": "linkedin.com/in/sidoineko",
        "creator_portfolio": "Hugging Face Portfolio: Sidoineko/portfolio",
        "competition_title": "üèÜ Version Comp√©tition Kaggle",
        "competition_text": "Cette premi√®re version d'AgriLens AI a √©t√© d√©velopp√©e sp√©cifiquement pour participer √† la comp√©tition Kaggle. Elle repr√©sente notre premi√®re production publique et d√©montre notre expertise en IA appliqu√©e √† l'agriculture.",
        "footer": "*AgriLens AI - Diagnostic intelligent des plantes avec IA*"
    },
    "en": {
        "title": "üå± AgriLens AI - Plant Disease Diagnosis",
        "subtitle": "**AI-powered plant disease diagnosis application**",
        "config_title": "‚öôÔ∏è Configuration",
        "load_model": "Load Gemma 3n E4B IT Model",
        "model_status": "**Model Status:**",
        "not_loaded": "Not loaded",
        "loaded": "‚úÖ Loaded",
        "error": "‚ùå Error",
        "tabs": ["üì∏ Image Analysis", "üí¨ Text Analysis", "üìñ Manual", "‚ÑπÔ∏è About"],
        "image_analysis_title": "üîç Image Diagnosis",
        "image_analysis_desc": "Upload a photo of a diseased plant to get a diagnosis",
        "choose_image": "Choose an image...",
        "analyze_button": "üî¨ Analyze with AI",
        "text_analysis_title": "üí¨ Text Diagnosis",
        "text_analysis_desc": "Describe your plant symptoms to get advice",
        "symptoms_desc": "Symptom description:",
        "analysis_results": "## üìä Analysis Results",
        "manual_title": "üìñ User Manual",
        "about_title": "‚ÑπÔ∏è About AgriLens AI",
        "creator_title": "üë®‚Äçüíª Application Creator",
        "creator_name": "**Sidoine Kolaol√© YEBADOKPO**",
        "creator_location": "Bohicon, Benin Republic",
        "creator_phone": "+229 01 96 91 13 46",
        "creator_email": "syebadokpo@gmail.com",
        "creator_linkedin": "linkedin.com/in/sidoineko",
        "creator_portfolio": "Hugging Face Portfolio: Sidoineko/portfolio",
        "competition_title": "üèÜ Kaggle Competition Version",
        "competition_text": "This first version of AgriLens AI was specifically developed to participate in the Kaggle competition. It represents our first public production and demonstrates our expertise in AI applied to agriculture.",
        "footer": "*AgriLens AI - Intelligent plant diagnosis with AI*"
    }
}

def t(key):
    return translations[st.session_state.language][key]

def load_model():
    """Charge le mod√®le Gemma 3n E4B IT selon l'environnement (local ou Hugging Face)"""
    try:
        from transformers import AutoProcessor, Gemma3nForConditionalGeneration
        
        # Diagnostic initial
        st.info("üîç Diagnostic de l'environnement...")
        issues = diagnose_loading_issues()
        with st.expander("üìä Diagnostic syst√®me", expanded=False):
            for issue in issues:
                st.write(issue)
        
        # Nettoyer la m√©moire avant le chargement
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # D√©tecter l'environnement
        is_local = os.path.exists("D:/Dev/model_gemma")
        
        if is_local:
            # Mode LOCAL - Utiliser le mod√®le t√©l√©charg√©
            st.info("Chargement du mod√®le Gemma 3n E4B IT depuis D:/Dev/model_gemma (mode local)...")
            model_path = "D:/Dev/model_gemma"
            
            # Charger le processeur
            processor = AutoProcessor.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            
            # Strat√©gies de chargement pour le mode local
            def load_local_ultra_conservative():
                st.info("Chargement local ultra-conservateur (CPU uniquement, sans device_map)...")
                return Gemma3nForConditionalGeneration.from_pretrained(
                    model_path,
                    torch_dtype=torch.float32,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            def load_local_conservative():
                st.info("Chargement local conservateur (device_map CPU)...")
                return Gemma3nForConditionalGeneration.from_pretrained(
                    model_path,
                    device_map="cpu",
                    torch_dtype=torch.float32,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            # Essayer les strat√©gies de chargement local
            strategies = [load_local_ultra_conservative, load_local_conservative]
            
            for strategy in strategies:
                try:
                    model = strategy()
                    st.success("Mod√®le Gemma 3n E4B IT charg√© avec succ√®s depuis le dossier local !")
                    return model, processor
                except Exception as e:
                    error_msg = str(e)
                    if "disk_offload" in error_msg.lower():
                        st.warning(f"Strat√©gie {strategy.__name__} √©chou√©e (disk_offload). Tentative suivante...")
                        gc.collect()
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        continue
                    elif "out of memory" in error_msg.lower():
                        st.warning(f"Strat√©gie {strategy.__name__} √©chou√©e (m√©moire insuffisante). Tentative suivante...")
                        gc.collect()
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        continue
                    else:
                        st.warning(f"Strat√©gie {strategy.__name__} √©chou√©e : {error_msg}. Tentative suivante...")
                        gc.collect()
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        continue
            
            # Si toutes les strat√©gies √©chouent
            st.error("Toutes les strat√©gies de chargement local ont √©chou√©.")
            return None, None
            
        else:
            # Mode HUGGING FACE - Utiliser le mod√®le en ligne
            st.info("Chargement du mod√®le Gemma 3n E4B IT depuis Hugging Face (mode en ligne)...")
            model_id = "google/gemma-3n-E4B-it"
            
            # Charger le processeur
            try:
                st.info("T√©l√©chargement du processeur depuis Hugging Face...")
                processor = AutoProcessor.from_pretrained(
                    model_id,
                    trust_remote_code=True
                )
                st.success("Processeur t√©l√©charg√© avec succ√®s !")
            except Exception as e:
                st.error(f"Erreur lors du t√©l√©chargement du processeur : {e}")
                st.info("Tentative de t√©l√©chargement avec cache...")
                try:
                    processor = AutoProcessor.from_pretrained(
                        model_id,
                        trust_remote_code=True,
                        cache_dir="./cache"
                    )
                    st.success("Processeur t√©l√©charg√© avec cache !")
                except Exception as e2:
                    st.error(f"Erreur fatale lors du t√©l√©chargement du processeur : {e2}")
                    return None, None
            
            # Strat√©gie 1: Chargement ultra-conservateur (CPU uniquement, sans device_map)
            def load_ultra_conservative():
                st.info("Chargement ultra-conservateur (CPU uniquement, sans device_map)...")
                return Gemma3nForConditionalGeneration.from_pretrained(
                    model_id,
                    torch_dtype=torch.float32,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            # Strat√©gie 2: Chargement conservateur avec device_map CPU
            def load_conservative():
                st.info("Chargement conservateur (device_map CPU)...")
                return Gemma3nForConditionalGeneration.from_pretrained(
                    model_id,
                    device_map="cpu",
                    torch_dtype=torch.float32,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            # Strat√©gie 3: Chargement avec 8-bit quantization
            def load_8bit():
                st.info("Chargement avec quantification 8-bit...")
                return Gemma3nForConditionalGeneration.from_pretrained(
                    model_id,
                    device_map="auto",
                    torch_dtype=torch.float16,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                    load_in_8bit=True
                )
            
            # Strat√©gie 4: Chargement avec 4-bit quantization
            def load_4bit():
                st.info("Chargement avec quantification 4-bit...")
                return Gemma3nForConditionalGeneration.from_pretrained(
                    model_id,
                    device_map="auto",
                    torch_dtype=torch.float16,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True,
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16
                )
            
            # Strat√©gie 5: Chargement avec gestion m√©moire personnalis√©e (sans max_memory)
            def load_custom_memory():
                st.info("Chargement avec gestion m√©moire personnalis√©e...")
                return Gemma3nForConditionalGeneration.from_pretrained(
                    model_id,
                    device_map="auto",
                    torch_dtype=torch.float16,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            # V√©rifier la m√©moire disponible
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
                st.info(f"M√©moire GPU disponible : {gpu_memory:.1f} GB")
                
                # Essayer diff√©rentes strat√©gies selon la m√©moire disponible
                strategies = []
                
                if gpu_memory >= 8:
                    strategies = [load_custom_memory, load_4bit, load_8bit, load_conservative, load_ultra_conservative]
                elif gpu_memory >= 4:
                    strategies = [load_4bit, load_8bit, load_conservative, load_ultra_conservative]
                else:
                    strategies = [load_8bit, load_conservative, load_ultra_conservative]
                
                # Essayer chaque strat√©gie jusqu'√† ce qu'une fonctionne
                for i, strategy in enumerate(strategies):
                    try:
                        st.info(f"Tentative {i+1}/{len(strategies)} : {strategy.__name__}")
                        model = strategy()
                        st.success(f"Mod√®le charg√© avec succ√®s via {strategy.__name__} !")
                        return model, processor
                    except Exception as e:
                        error_msg = str(e)
                        if "disk_offload" in error_msg.lower():
                            st.warning(f"Strat√©gie {strategy.__name__} √©chou√©e (disk_offload). Tentative suivante...")
                            # Nettoyer la m√©moire avant la prochaine tentative
                            gc.collect()
                            if torch.cuda.is_available():
                                torch.cuda.empty_cache()
                            continue
                        elif "out of memory" in error_msg.lower():
                            st.warning(f"Strat√©gie {strategy.__name__} √©chou√©e (m√©moire insuffisante). Tentative suivante...")
                            # Nettoyer la m√©moire avant la prochaine tentative
                            gc.collect()
                            if torch.cuda.is_available():
                                torch.cuda.empty_cache()
                            continue
                        else:
                            st.warning(f"Strat√©gie {strategy.__name__} √©chou√©e : {error_msg}. Tentative suivante...")
                            # Nettoyer la m√©moire avant la prochaine tentative
                            gc.collect()
                            if torch.cuda.is_available():
                                torch.cuda.empty_cache()
                            continue
                
                # Si toutes les strat√©gies ont √©chou√©
                st.error("Toutes les strat√©gies de chargement ont √©chou√©.")
                return None, None
                
            else:
                # Mode CPU uniquement - essayer plusieurs strat√©gies
                st.warning("GPU non disponible, utilisation du CPU (plus lent)")
                cpu_strategies = [load_ultra_conservative, load_conservative]
                
                for i, strategy in enumerate(cpu_strategies):
                    try:
                        st.info(f"Tentative CPU {i+1}/{len(cpu_strategies)} : {strategy.__name__}")
                        model = strategy()
                        st.success(f"Mod√®le charg√© avec succ√®s en mode CPU via {strategy.__name__} !")
                        return model, processor
                    except Exception as e:
                        error_msg = str(e)
                        if "disk_offload" in error_msg.lower():
                            st.warning(f"Strat√©gie CPU {strategy.__name__} √©chou√©e (disk_offload). Tentative suivante...")
                            gc.collect()
                            continue
                        else:
                            st.warning(f"Strat√©gie CPU {strategy.__name__} √©chou√©e : {error_msg}. Tentative suivante...")
                            gc.collect()
                            continue
                
                st.error("Toutes les strat√©gies de chargement CPU ont √©chou√©.")
                return None, None
        
    except Exception as e:
        st.error(f"Erreur lors du chargement du mod√®le : {e}")
        return None, None

def analyze_image_multilingual(image, prompt=""):
    """Analyse une image avec Gemma 3n E4B IT pour diagnostic pr√©cis"""
    try:
        # V√©rification compl√®te du mod√®le avec cache
        if not st.session_state.model_loaded and not check_model_persistence():
            # Essayer de restaurer depuis le cache
            if restore_model_from_cache():
                st.info("üîÑ Mod√®le restaur√© depuis le cache pour l'analyse")
            else:
                return "‚ùå Mod√®le Gemma non charg√©. Veuillez d'abord charger le mod√®le dans les r√©glages."
        
        # V√©rifier que le mod√®le et le processeur sont disponibles
        if not hasattr(st.session_state, 'model') or st.session_state.model is None:
            # Essayer de restaurer depuis le cache
            if restore_model_from_cache():
                st.info("üîÑ Mod√®le restaur√© depuis le cache")
            else:
                st.session_state.model_loaded = False
                return "‚ùå Mod√®le perdu en m√©moire. Veuillez recharger le mod√®le."
        
        if not hasattr(st.session_state, 'processor') or st.session_state.processor is None:
            st.session_state.model_loaded = False
            return "‚ùå Processeur perdu en m√©moire. Veuillez recharger le mod√®le."
        
        # R√©cup√©rer le mod√®le et le processeur
        model, processor = st.session_state.model, st.session_state.processor
        
        # V√©rification finale
        if not model or not processor:
            st.session_state.model_loaded = False
            return "‚ùå Mod√®le Gemma non disponible. Veuillez recharger le mod√®le."
        
        # Pr√©parer le prompt pour Gemma 3n
        if st.session_state.language == "fr":
            if prompt:
                gemma_prompt = f"""
Tu es un expert en pathologie v√©g√©tale. Analyse cette image de plante et fournis un diagnostic pr√©cis.

**Question sp√©cifique :** {prompt}

**Instructions :**
1. **Diagnostic pr√©cis** : Identifie la maladie sp√©cifique avec son nom scientifique
2. **Causes** : Explique les causes probables (champignons, bact√©ries, virus, carences, etc.)
3. **Sympt√¥mes d√©taill√©s** : Liste tous les sympt√¥mes observables dans l'image
4. **Traitement sp√©cifique** : Donne des recommandations de traitement pr√©cises
5. **Actions pr√©ventives** : Conseils pour √©viter la propagation
6. **Urgence** : Indique si c'est urgent ou non

**Format de r√©ponse :**
## üîç **Diagnostic Pr√©cis**
[Nom de la maladie et causes]

## üìã **Sympt√¥mes D√©taill√©s**
[Liste des sympt√¥mes observ√©s]

## üíä **Traitement Recommand√©**
[Actions sp√©cifiques √† entreprendre]

## üõ°Ô∏è **Actions Pr√©ventives**
[Mesures pour √©viter la propagation]

## ‚ö†Ô∏è **Niveau d'Urgence**
[Urgent/Mod√©r√©/Faible]

R√©ponds de mani√®re structur√©e et pr√©cise.
"""
            else:
                gemma_prompt = """
Tu es un expert en pathologie v√©g√©tale. Analyse cette image de plante et fournis un diagnostic pr√©cis.

**Instructions :**
1. **Diagnostic pr√©cis** : Identifie la maladie sp√©cifique avec son nom scientifique
2. **Causes** : Explique les causes probables (champignons, bact√©ries, virus, carences, etc.)
3. **Sympt√¥mes d√©taill√©s** : Liste tous les sympt√¥mes observables dans l'image
4. **Traitement sp√©cifique** : Donne des recommandations de traitement pr√©cises
5. **Actions pr√©ventives** : Conseils pour √©viter la propagation
6. **Urgence** : Indique si c'est urgent ou non

**Format de r√©ponse :**
## üîç **Diagnostic Pr√©cis**
[Nom de la maladie et causes]

## üìã **Sympt√¥mes D√©taill√©s**
[Liste des sympt√¥mes observ√©s]

## üíä **Traitement Recommand√©**
[Actions sp√©cifiques √† entreprendre]

## üõ°Ô∏è **Actions Pr√©ventives**
[Mesures pour √©viter la propagation]

## ‚ö†Ô∏è **Niveau d'Urgence**
[Urgent/Mod√©r√©/Faible]

R√©ponds de mani√®re structur√©e et pr√©cise.
"""
        else:
            if prompt:
                gemma_prompt = f"""
You are an expert in plant pathology. Analyze this plant image and provide a precise diagnosis.

**Specific Question:** {prompt}

**Instructions:**
1. **Precise Diagnosis**: Identify the specific disease with its scientific name
2. **Causes**: Explain probable causes (fungi, bacteria, viruses, deficiencies, etc.)
3. **Detailed Symptoms**: List all observable symptoms in the image
4. **Specific Treatment**: Give precise treatment recommendations
5. **Preventive Actions**: Advice to prevent spread
6. **Urgency**: Indicate if urgent or not

**Response Format:**
## üîç **Precise Diagnosis**
[Disease name and causes]

## üìã **Detailed Symptoms**
[List of observed symptoms]

## üíä **Recommended Treatment**
[Specific actions to take]

## üõ°Ô∏è **Preventive Actions**
[Measures to prevent spread]

## ‚ö†Ô∏è **Urgency Level**
[Urgent/Moderate/Low]

Respond in a structured and precise manner.
"""
            else:
                gemma_prompt = """
You are an expert in plant pathology. Analyze this plant image and provide a precise diagnosis.

**Instructions:**
1. **Precise Diagnosis**: Identify the specific disease with its scientific name
2. **Causes**: Explain probable causes (fungi, bacteria, viruses, deficiencies, etc.)
3. **Detailed Symptoms**: List all observable symptoms in the image
4. **Specific Treatment**: Give precise treatment recommendations
5. **Preventive Actions**: Advice to prevent spread
6. **Urgency**: Indicate if urgent or not

**Response Format:**
## üîç **Precise Diagnosis**
[Disease name and causes]

## üìã **Detailed Symptoms**
[List of observed symptoms]

## üíä **Recommended Treatment**
[Specific actions to take]

## üõ°Ô∏è **Preventive Actions**
[Measures to prevent spread]

## ‚ö†Ô∏è **Urgency Level**
[Urgent/Moderate/Low]

Respond in a structured and precise manner.
"""
        
        # Pr√©parer les messages pour Gemma 3n
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": "You are an expert in plant pathology."}]
            },
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": gemma_prompt}
                ]
            }
        ]
        
        # Traiter les entr√©es avec le processeur
        inputs = processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
        ).to(model.device)
        
        input_len = inputs["input_ids"].shape[-1]
        
        # G√©n√©rer la r√©ponse
        with torch.inference_mode():
            generation = model.generate(
                **inputs, 
                max_new_tokens=500, 
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )
            generation = generation[0][input_len:]
        
        # D√©coder la r√©ponse
        response_text = processor.decode(generation, skip_special_tokens=True)
        
        if st.session_state.language == "fr":
            return f"""
## üß† **Analyse par Gemma 3n E4B IT**
{response_text}
"""
        else:
            return f"""
## üß† **Analysis by Gemma 3n E4B IT**
{response_text}
"""
        
    except Exception as e:
        return f"‚ùå Erreur lors de l'analyse d'image : {e}"

def analyze_text_multilingual(text):
    """Analyse un texte avec le mod√®le Gemma 3n E4B IT"""
    # V√©rification compl√®te du mod√®le avec cache
    if not st.session_state.model_loaded and not check_model_persistence():
        # Essayer de restaurer depuis le cache
        if restore_model_from_cache():
            st.info("üîÑ Mod√®le restaur√© depuis le cache pour l'analyse")
        else:
            return "‚ùå Mod√®le non charg√©. Veuillez le charger dans les r√©glages."
    
    # V√©rifier que le mod√®le et le processeur sont disponibles
    if not hasattr(st.session_state, 'model') or st.session_state.model is None:
        # Essayer de restaurer depuis le cache
        if restore_model_from_cache():
            st.info("üîÑ Mod√®le restaur√© depuis le cache")
        else:
            st.session_state.model_loaded = False
            return "‚ùå Mod√®le perdu en m√©moire. Veuillez recharger le mod√®le."
    
    if not hasattr(st.session_state, 'processor') or st.session_state.processor is None:
        st.session_state.model_loaded = False
        return "‚ùå Processeur perdu en m√©moire. Veuillez recharger le mod√®le."
    
    try:
        model, processor = st.session_state.model, st.session_state.processor
        
        # V√©rification finale
        if not model or not processor:
            st.session_state.model_loaded = False
            return "‚ùå Mod√®le Gemma non disponible. Veuillez recharger le mod√®le."
        
        if st.session_state.language == "fr":
            prompt = f"Tu es un assistant agricole expert. Analyse ce probl√®me : {text}"
        else:
            prompt = f"You are an expert agricultural assistant. Analyze this problem: {text}"
        
        # Pr√©parer les messages
        messages = [
            {
                "role": "user",
                "content": [{"type": "text", "text": prompt}]
            }
        ]
        
        # Traiter les entr√©es
        inputs = processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
        ).to(model.device)
        
        input_len = inputs["input_ids"].shape[-1]
        
        # G√©n√©rer la r√©ponse
        with torch.inference_mode():
            generation = model.generate(
                **inputs,
                max_new_tokens=300,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )
            generation = generation[0][input_len:]
        
        response = processor.decode(generation, skip_special_tokens=True)
        return response
        
    except Exception as e:
        return f"‚ùå Erreur lors de l'analyse de texte : {e}"

def analyze_with_gemini(gemma_description, image_info=""):
    """
    Analyse approfondie avec Gemini API pour diagnostic pr√©cis
    """
    if not gemini_model:
        return "‚ùå Gemini API non configur√©e. V√©rifiez votre cl√© API Google."
    
    try:
        if st.session_state.language == "fr":
            prompt = f"""
Tu es un expert en pathologie v√©g√©tale et en agriculture. Analyse cette description d'une plante malade et fournis un diagnostic pr√©cis.

**Description de Gemma :**
{gemma_description}

**Informations suppl√©mentaires :**
{image_info}

**Instructions :**
1. **Diagnostic pr√©cis** : Identifie la maladie sp√©cifique avec son nom scientifique
2. **Causes** : Explique les causes probables (champignons, bact√©ries, virus, carences, etc.)
3. **Sympt√¥mes d√©taill√©s** : Liste tous les sympt√¥mes observables
4. **Traitement sp√©cifique** : Donne des recommandations de traitement pr√©cises
5. **Actions pr√©ventives** : Conseils pour √©viter la propagation
6. **Urgence** : Indique si c'est urgent ou non

**Format de r√©ponse :**
## üîç **Diagnostic Pr√©cis**
[Nom de la maladie et causes]

## üìã **Sympt√¥mes D√©taill√©s**
[Liste des sympt√¥mes]

## üíä **Traitement Recommand√©**
[Actions sp√©cifiques √† entreprendre]

## üõ°Ô∏è **Actions Pr√©ventives**
[Mesures pour √©viter la propagation]

## ‚ö†Ô∏è **Niveau d'Urgence**
[Urgent/Mod√©r√©/Faible]

R√©ponds de mani√®re structur√©e et pr√©cise.
"""
        else:
            prompt = f"""
You are an expert in plant pathology and agriculture. Analyze this description of a diseased plant and provide a precise diagnosis.

**Gemma's Description:**
{gemma_description}

**Additional Information:**
{image_info}

**Instructions:**
1. **Precise Diagnosis**: Identify the specific disease with its scientific name
2. **Causes**: Explain probable causes (fungi, bacteria, viruses, deficiencies, etc.)
3. **Detailed Symptoms**: List all observable symptoms
4. **Specific Treatment**: Give precise treatment recommendations
5. **Preventive Actions**: Advice to prevent spread
6. **Urgency**: Indicate if urgent or not

**Response Format:**
## üîç **Precise Diagnosis**
[Disease name and causes]

## üìã **Detailed Symptoms**
[List of symptoms]

## üíä **Recommended Treatment**
[Specific actions to take]

## üõ°Ô∏è **Preventive Actions**
[Measures to prevent spread]

## ‚ö†Ô∏è **Urgency Level**
[Urgent/Moderate/Low]

Respond in a structured and precise manner.
"""
        
        response = gemini_model.generate_content(prompt)
        return response.text
        
    except Exception as e:
        return f"‚ùå Erreur lors de l'analyse Gemini : {e}"

# Interface principale
st.title(t("title"))
st.markdown(t("subtitle"))

# Sidebar pour la configuration
with st.sidebar:
    st.header(t("config_title"))
    
    # S√©lecteur de langue
    st.subheader("üåê S√©lection de langue / Language Selection")
    language = st.selectbox(
        "Language / Langue",
        ["Fran√ßais", "English"],
        index=0 if st.session_state.language == "fr" else 1
    )
    
    if language == "Fran√ßais":
        st.session_state.language = "fr"
    else:
        st.session_state.language = "en"
    
    # V√©rification automatique de la persistance du mod√®le
    if not st.session_state.model_loaded and not check_model_persistence():
        # Essayer de restaurer depuis le cache
        if restore_model_from_cache():
            st.success("‚úÖ Mod√®le restaur√© depuis le cache")
        else:
            st.info("üîÑ Mod√®le non trouv√© en cache - chargement n√©cessaire")
    
    # Chargement du mod√®le
    if st.button(t("load_model"), type="primary"):
        with st.spinner("Chargement du mod√®le..." if st.session_state.language == "fr" else "Loading model..."):
            try:
                model, processor = load_model()
                if model and processor:
                    # Stocker le mod√®le dans la session avec v√©rification
                    st.session_state.model = model
                    st.session_state.processor = processor
                    st.session_state.model_loaded = True
                    st.session_state.model_status = t("loaded")
                    st.session_state.model_load_time = time.time()
                    
                    # Forcer la persistance en cache global
                    if force_model_persistence():
                        if is_local:
                            st.success("‚úÖ Mod√®le Gemma 3n E4B IT charg√© et persist√© avec succ√®s (mode local) !")
                            st.info("üîÑ Le mod√®le local est maintenant sauvegard√© en cache pour la persistance")
                        else:
                            st.success("‚úÖ Mod√®le Gemma 3n E4B IT charg√© et persist√© avec succ√®s (mode Hugging Face) !")
                            st.info("üîÑ Le mod√®le en ligne est maintenant sauvegard√© en cache pour la persistance")
                    else:
                        st.warning("‚ö†Ô∏è Mod√®le charg√© mais probl√®me de persistance d√©tect√©")
                        
                else:
                    st.session_state.model_loaded = False
                    st.session_state.model_status = t("error")
                    st.error("√âchec du chargement du mod√®le" if st.session_state.language == "fr" else "Model loading failed")
            except Exception as e:
                st.session_state.model_loaded = False
                st.session_state.model_status = t("error")
                st.error(f"Erreur lors du chargement : {e}")
    
    st.info(f"{t('model_status')} {st.session_state.model_status}")
    
    # V√©rification de la persistance du mod√®le
    if st.session_state.model_loaded or check_model_persistence():
        # V√©rifier que le mod√®le est toujours disponible
        if hasattr(st.session_state, 'model') and st.session_state.model is not None:
            if hasattr(st.session_state, 'processor') and st.session_state.processor is not None:
                # D√©tecter l'environnement pour l'affichage
                is_local = os.path.exists("D:/Dev/model_gemma")
                
                # Afficher le statut de persistance
                if st.session_state.model_persistence_check:
                    if is_local:
                        st.success("‚úÖ Mod√®le Gemma 3n E4B IT charg√© et PERSISTANT (mode local - cache activ√©)")
                        st.info("üîÑ Le mod√®le local est sauvegard√© en cache et persiste entre les sessions")
                    else:
                        st.success("‚úÖ Mod√®le Gemma 3n E4B IT charg√© et PERSISTANT (mode Hugging Face - cache activ√©)")
                        st.info("üîÑ Le mod√®le en ligne est sauvegard√© en cache et persiste entre les sessions")
                else:
                    if is_local:
                        st.success("‚úÖ Mod√®le Gemma 3n E4B IT charg√© (mode local)")
                        st.info("Le mod√®le local est pr√™t pour l'analyse d'images et de texte")
                    else:
                        st.success("‚úÖ Mod√®le Gemma 3n E4B IT charg√© (mode Hugging Face)")
                        st.info("Le mod√®le en ligne est pr√™t pour l'analyse d'images et de texte")
                
                # Diagnostic du mod√®le
                with st.expander("üîç Diagnostic du mod√®le"):
                    st.write(f"**Mode d'ex√©cution :** {'üè† Local (D:/Dev/model_gemma)' if is_local else 'üåê Hugging Face (en ligne)'}")
                    st.write(f"**Mod√®le charg√© :** {type(st.session_state.model).__name__}")
                    st.write(f"**Processeur charg√© :** {type(st.session_state.processor).__name__}")
                    st.write(f"**Device du mod√®le :** {st.session_state.model.device}")
                    st.write(f"**M√©moire utilis√©e :** {torch.cuda.memory_allocated() / 1024**3:.2f} GB" if torch.cuda.is_available() else "CPU uniquement")
                    if st.session_state.model_load_time:
                        load_time_str = time.strftime('%H:%M:%S', time.localtime(st.session_state.model_load_time))
                        st.write(f"**Heure de chargement :** {load_time_str}")
                    st.write(f"**Cache global :** {'‚úÖ Actif' if st.session_state.global_model_cache else '‚ùå Inactif'}")
                
                # Boutons de gestion
                col1, col2 = st.columns(2)
                with col1:
                    if st.button("üîÑ Recharger le mod√®le", type="secondary"):
                        st.session_state.model_loaded = False
                        st.session_state.model = None
                        st.session_state.processor = None
                        st.session_state.global_model_cache.clear()
                        st.rerun()
                with col2:
                    if st.button("üíæ Forcer la persistance", type="secondary"):
                        if force_model_persistence():
                            st.success("‚úÖ Persistance forc√©e avec succ√®s")
                        else:
                            st.error("‚ùå √âchec de la persistance forc√©e")
                        st.rerun()
            else:
                st.warning("‚ö†Ô∏è Processeur manquant - rechargement n√©cessaire")
                st.session_state.model_loaded = False
                st.session_state.model = None
        else:
            st.warning("‚ö†Ô∏è Mod√®le perdu en m√©moire - rechargement n√©cessaire")
            st.session_state.model_loaded = False
            st.session_state.processor = None
    else:
        # D√©tecter l'environnement pour l'affichage
        is_local = os.path.exists("D:/Dev/model_gemma")
        
        st.warning("‚ö†Ô∏è Mod√®le Gemma 3n E4B IT non charg√©")
        if is_local:
            st.info("Cliquez sur 'Charger le mod√®le' pour activer l'analyse (mode local)")
        else:
            st.info("Cliquez sur 'Charger le mod√®le' pour activer l'analyse (mode Hugging Face)")

# Onglets principaux
tab1, tab2, tab3, tab4 = st.tabs(t("tabs"))

with tab1:
    st.header(t("image_analysis_title"))
    st.markdown(t("image_analysis_desc"))
    
    # Options de capture d'image
    capture_option = st.radio(
        "Choisissez votre m√©thode :" if st.session_state.language == "fr" else "Choose your method:",
        ["üìÅ Upload d'image" if st.session_state.language == "fr" else "üìÅ Upload Image", 
         "üì∑ Capture par webcam" if st.session_state.language == "fr" else "üì∑ Webcam Capture"],
        horizontal=True
    )
    
    uploaded_file = None
    captured_image = None
    
    if capture_option == "üìÅ Upload d'image" or capture_option == "üìÅ Upload Image":
        try:
            uploaded_file = st.file_uploader(
                t("choose_image"), 
                type=['png', 'jpg', 'jpeg'],
                help="Formats accept√©s : PNG, JPG, JPEG (max 200MB)" if st.session_state.language == "fr" else "Accepted formats: PNG, JPG, JPEG (max 200MB)",
                accept_multiple_files=False,
                key="image_uploader"
            )
        except Exception as e:
            st.error(f"‚ùå Erreur lors de l'upload : {e}")
    
    else:  # Webcam capture
        st.markdown("**üì∑ Capture d'image par webcam**" if st.session_state.language == "fr" else "**üì∑ Webcam Image Capture**")
        st.info("üí° Positionnez votre plante malade devant la webcam et cliquez sur 'Prendre une photo'" if st.session_state.language == "fr" else "üí° Position your diseased plant in front of the webcam and click 'Take Photo'")
        
        captured_image = st.camera_input(
            "Prendre une photo de la plante" if st.session_state.language == "fr" else "Take a photo of the plant",
            key="webcam_capture"
        )
    
    # Traitement de l'image (upload ou webcam)
    image = None
    image_source = None
    
    if uploaded_file is not None:
        try:
            image = Image.open(uploaded_file)
            image_source = "upload"
        except Exception as e:
            st.error(f"‚ùå Erreur lors du traitement de l'image upload√©e : {e}")
            st.info("üí° Essayez avec une image diff√©rente ou un format diff√©rent (PNG, JPG, JPEG)")
    elif captured_image is not None:
        try:
            image = Image.open(captured_image)
            image_source = "webcam"
        except Exception as e:
            st.error(f"‚ùå Erreur lors du traitement de l'image captur√©e : {e}")
            st.info("üí° Essayez de reprendre la photo")
    
    if image is not None:
        try:
            # Redimensionner l'image si n√©cessaire
            original_size = image.size
            image, was_resized = resize_image_if_needed(image, max_size=(800, 800))
            
            col1, col2 = st.columns([1, 1])
            with col1:
                if image_source == "upload":
                    st.image(image, caption="Image upload√©e" if st.session_state.language == "fr" else "Uploaded Image", use_container_width=True)
                else:
                    st.image(image, caption="Image captur√©e par webcam" if st.session_state.language == "fr" else "Webcam Captured Image", use_container_width=True)
            
            with col2:
                st.markdown("**Informations de l'image :**")
                st.write(f"‚Ä¢ Format : {image.format}")
                st.write(f"‚Ä¢ Taille originale : {original_size[0]}x{original_size[1]} pixels")
                st.write(f"‚Ä¢ Taille actuelle : {image.size[0]}x{image.size[1]} pixels")
                st.write(f"‚Ä¢ Mode : {image.mode}")
                
                if was_resized:
                    st.warning("‚ö†Ô∏è L'image a √©t√© automatiquement redimensionn√©e pour optimiser le traitement")
            
            question = st.text_area(
                "Question sp√©cifique (optionnel) :",
                placeholder="Ex: Quelle est cette maladie ? Que faire pour la traiter ?",
                height=100
            )
            
            if st.button(t("analyze_button"), disabled=not st.session_state.model_loaded, type="primary"):
                if not st.session_state.model_loaded:
                    st.error("‚ùå Mod√®le Gemma non charg√©. Veuillez d'abord charger le mod√®le dans les r√©glages.")
                    st.info("üí° L'analyse d'image n√©cessite le mod√®le Gemma 3n E4B IT. Chargez-le dans les r√©glages.")
                else:
                    with st.spinner("üîç Analyse en cours..."):
                        result = analyze_image_multilingual(image, question)
                    
                    st.markdown(t("analysis_results"))
                    st.markdown("---")
                    st.markdown(result)
        except Exception as e:
            error_msg = str(e)
            if "403" in error_msg or "Forbidden" in error_msg:
                st.error("‚ùå Erreur 403 - Acc√®s refus√© lors du traitement de l'image")
                st.warning("üîí Cette erreur indique un probl√®me d'autorisation c√¥t√© serveur.")
                st.info("üí° Solutions possibles :")
                st.info("‚Ä¢ V√©rifiez les logs de votre espace Hugging Face")
                st.info("‚Ä¢ Essayez avec une image plus petite (< 1MB)")
                st.info("‚Ä¢ Rafra√Æchissez la page et r√©essayez")
                st.info("‚Ä¢ Contactez le support Hugging Face si le probl√®me persiste")
            else:
                st.error(f"‚ùå Erreur lors du traitement de l'image : {e}")
                st.info("üí° Essayez avec une image diff√©rente ou un format diff√©rent (PNG, JPG, JPEG)")

with tab2:
    st.header(t("text_analysis_title"))
    st.markdown(t("text_analysis_desc"))
    
    text_input = st.text_area(
        t("symptoms_desc"),
        placeholder="Ex: Mes tomates ont des taches brunes sur les feuilles et les fruits...",
        height=150
    )
    
    if st.button("üß† Analyser avec l'IA", disabled=not st.session_state.model_loaded, type="primary"):
        if not st.session_state.model_loaded:
            st.error("‚ùå Veuillez d'abord charger le mod√®le dans les r√©glages")
        elif not text_input.strip():
            st.error("‚ùå Veuillez saisir une description")
        else:
            with st.spinner("üîç Analyse en cours..."):
                result = analyze_text_multilingual(text_input)
            
            st.markdown(t("analysis_results"))
            st.markdown("---")
            st.markdown(result)

with tab3:
    st.header(t("manual_title"))
    
    if st.session_state.language == "fr":
        st.markdown("""
        ### üöÄ **D√©marrage Rapide**
        1. **Charger le mod√®le** : Cliquez sur 'Charger le mod√®le' dans les r√©glages
        2. **Choisir le mode** : Analyse d'image ou analyse de texte
        3. **Soumettre votre demande** : Upload d'image ou description
        4. **Obtenir le diagnostic** : R√©sultats avec recommandations
        
        ### üì∏ **Analyse d'Image**
        ‚Ä¢ **Formats accept√©s** : PNG, JPG, JPEG
        ‚Ä¢ **Taille recommand√©e** : 500x500 pixels minimum
        ‚Ä¢ **Qualit√©** : Image claire et bien √©clair√©e
        ‚Ä¢ **Focus** : Centrer sur la zone malade
        ‚Ä¢ **Question optionnelle** : Pr√©cisez votre pr√©occupation
        
        ### üí¨ **Analyse de Texte**
        ‚Ä¢ **Description d√©taill√©e** : Sympt√¥mes observ√©s
        ‚Ä¢ **Contexte** : Type de plante, conditions
        ‚Ä¢ **Historique** : √âvolution du probl√®me
        ‚Ä¢ **Actions d√©j√† tent√©es** : Traitements appliqu√©s
        
        ### üîç **Interpr√©tation des R√©sultats**
        ‚Ä¢ **Diagnostic** : Identification de la maladie
        ‚Ä¢ **Causes possibles** : Facteurs d√©clencheurs
        ‚Ä¢ **Recommandations** : Actions √† entreprendre
        ‚Ä¢ **Pr√©vention** : Mesures pr√©ventives
        
        ### üí° **Bonnes Pratiques**
        ‚Ä¢ **Images multiples** : Diff√©rents angles de la maladie
        ‚Ä¢ **√âclairage naturel** : √âviter les ombres
        ‚Ä¢ **Description pr√©cise** : D√©tails des sympt√¥mes
        ‚Ä¢ **Suivi r√©gulier** : Surveiller l'√©volution
        ‚Ä¢ **Consultation expert** : Pour cas complexes
        """)
    else:
        st.markdown("""
        ### üöÄ **Quick Start**
        1. **Load the model** : Click 'Load Model' in settings
        2. **Choose mode** : Image analysis or text analysis
        3. **Submit your request** : Upload image or description
        4. **Get diagnosis** : Results with recommendations
        
        ### üì∏ **Image Analysis**
        ‚Ä¢ **Accepted formats** : PNG, JPG, JPEG
        ‚Ä¢ **Recommended size** : 500x500 pixels minimum
        ‚Ä¢ **Quality** : Clear and well-lit image
        ‚Ä¢ **Focus** : Center on the diseased area
        ‚Ä¢ **Optional question** : Specify your concern
        
        ### üí¨ **Text Analysis**
        ‚Ä¢ **Detailed description** : Observed symptoms
        ‚Ä¢ **Context** : Plant type, conditions
        ‚Ä¢ **History** : Problem evolution
        ‚Ä¢ **Actions already tried** : Applied treatments
        
        ### üîç **Result Interpretation**
        ‚Ä¢ **Diagnosis** : Disease identification
        ‚Ä¢ **Possible causes** : Triggering factors
        ‚Ä¢ **Recommendations** : Actions to take
        ‚Ä¢ **Prevention** : Preventive measures
        
        ### üí° **Best Practices**
        ‚Ä¢ **Multiple images** : Different angles of the disease
        ‚Ä¢ **Natural lighting** : Avoid shadows
        ‚Ä¢ **Precise description** : Symptom details
        ‚Ä¢ **Regular monitoring** : Track evolution
        ‚Ä¢ **Expert consultation** : For complex cases
        """)

with tab4:
    st.header(t("about_title"))
    
    st.markdown("### üå± Notre Mission / Our Mission")
    st.markdown("AgriLens AI est une application de diagnostic des maladies de plantes utilisant l'intelligence artificielle pour aider les agriculteurs √† identifier et traiter les probl√®mes de leurs cultures.")
    
    st.markdown("### üöÄ Fonctionnalit√©s / Features")
    st.markdown("""
    ‚Ä¢ **Analyse d'images** : Diagnostic visuel des maladies
    ‚Ä¢ **Analyse de texte** : Conseils bas√©s sur les descriptions
    ‚Ä¢ **Recommandations pratiques** : Actions concr√®tes √† entreprendre
    ‚Ä¢ **Interface mobile** : Optimis√©e pour smartphones et tablettes
    ‚Ä¢ **Support multilingue** : Fran√ßais et Anglais
    """)
    
    st.markdown("### üîß Technologie / Technology")
    
    # D√©tecter l'environnement pour l'affichage
    is_local = os.path.exists("D:/Dev/model_gemma")
    
    if is_local:
        st.markdown("""
        ‚Ä¢ **Mod√®le** : Gemma 3n E4B IT (Local - D:/Dev/model_gemma)
        ‚Ä¢ **Framework** : Streamlit
        ‚Ä¢ **D√©ploiement** : Local
        """)
    else:
        st.markdown("""
        ‚Ä¢ **Mod√®le** : Gemma 3n E4B IT (Hugging Face - en ligne)
        ‚Ä¢ **Framework** : Streamlit
        ‚Ä¢ **D√©ploiement** : Hugging Face Spaces
        """)
    
    # Informations du cr√©ateur
    st.markdown(f"### {t('creator_title')}")
    st.markdown(f"{t('creator_name')}")
    st.markdown(f"üìç {t('creator_location')}")
    st.markdown(f"üìû {t('creator_phone')}")
    st.markdown(f"üìß {t('creator_email')}")
    st.markdown(f"üîó {t('creator_linkedin')}")
    st.markdown(f"üìÅ {t('creator_portfolio')}")
    
    # Informations de comp√©tition
    st.markdown(f"### {t('competition_title')}")
    st.markdown(t("competition_text"))
    
    st.markdown("### ‚ö†Ô∏è Avertissement / Warning")
    st.markdown("Les r√©sultats fournis sont √† titre indicatif uniquement. Pour un diagnostic professionnel, consultez un expert qualifi√©.")
    
    st.markdown("### üìû Support")
    st.markdown("Pour toute question ou probl√®me, consultez la documentation ou contactez l'√©quipe de d√©veloppement.")

# Footer
st.markdown("---")
st.markdown(t("footer")) 