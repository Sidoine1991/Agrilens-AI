import streamlit as st
import os
import io
from PIL import Image
import requests
import torch
import google.generativeai as genai
import gc
import time
import sys
import psutil
from transformers import AutoProcessor, Gemma3nForConditionalGeneration # Importations pour Hugging Face
from huggingface_hub import HfFolder # Pour v√©rifier le token HF

# --- Configuration de la Page ---
st.set_page_config(
    page_title="AgriLens AI - Diagnostic des Plantes",
    page_icon="üå±",
    layout="centered", # Centr√© pour une meilleure exp√©rience sur toutes les tailles d'√©cran
    initial_sidebar_state="collapsed" # Sidebar ferm√©e par d√©faut, peut √™tre ouverte via le menu
)

# --- Cache global pour le mod√®le ---
# Ce cache permet au mod√®le de persister en m√©moire entre les re-runs de Streamlit
if 'global_model_cache' not in st.session_state:
    st.session_state.global_model_cache = {}
if 'model_load_time' not in st.session_state:
    st.session_state.model_load_time = None
if 'model_persistence_check' not in st.session_state:
    st.session_state.model_persistence_check = False

def check_model_persistence():
    """V√©rifie si le mod√®le est toujours persistant en m√©moire et fonctionnel."""
    try:
        if hasattr(st.session_state, 'model') and st.session_state.model is not None:
            # Test simple pour v√©rifier que le mod√®le fonctionne
            if hasattr(st.session_state.model, 'device'):
                device = st.session_state.model.device # Acc√©der √† l'attribut device
                return True
        return False
    except Exception:
        return False

def force_model_persistence():
    """Stocke le mod√®le et le processeur dans le cache global pour assurer la persistance."""
    try:
        if hasattr(st.session_state, 'model') and st.session_state.model is not None:
            st.session_state.global_model_cache['model'] = st.session_state.model
            st.session_state.global_model_cache['processor'] = st.session_state.processor
            st.session_state.global_model_cache['load_time'] = time.time()
            st.session_state.global_model_cache['model_type'] = type(st.session_state.model).__name__
            st.session_state.global_model_cache['processor_type'] = type(st.session_state.processor).__name__
            if hasattr(st.session_state.model, 'device'):
                st.session_state.global_model_cache['device'] = st.session_state.model.device

            if st.session_state.global_model_cache.get('model') is not None:
                st.session_state.model_persistence_check = True
                return True
        return False
    except Exception:
        return False

def restore_model_from_cache():
    """Restaure le mod√®le et le processeur depuis le cache global."""
    try:
        if 'model' in st.session_state.global_model_cache and st.session_state.global_model_cache['model'] is not None:
            cached_model = st.session_state.global_model_cache['model']
            if hasattr(cached_model, 'device'): # V√©rifie si le mod√®le est toujours valide
                st.session_state.model = cached_model
                st.session_state.processor = st.session_state.global_model_cache.get('processor')
                st.session_state.model_loaded = True
                st.session_state.model_status = "Charg√© (cache)"
                if 'load_time' in st.session_state.global_model_cache:
                    st.session_state.model_load_time = st.session_state.global_model_cache['load_time']
                return True
        return False
    except Exception:
        return False

def diagnose_loading_issues():
    """Diagnostique les probl√®mes potentiels de chargement (d√©pendances, ressources, etc.)."""
    issues = []
    
    # V√©rifier la pr√©sence du token Hugging Face
    if not HfFolder.get_token() and not os.environ.get("HF_TOKEN"):
        issues.append("‚ö†Ô∏è **Jeton Hugging Face (HF_TOKEN) non configur√©.** Le t√©l√©chargement du mod√®le pourrait √©chouer ou √™tre ralenti. Voir la section Configuration pour plus de d√©tails.")

    # V√©rifier les d√©pendances et les ressources syst√®me
    try:
        import transformers; issues.append(f"‚úÖ Transformers v{transformers.__version__}")
        import torch; issues.append(f"‚úÖ PyTorch v{torch.__version__}")
        if torch.cuda.is_available(): issues.append(f"‚úÖ CUDA disponible : {torch.cuda.get_device_name(0)}")
        else: issues.append("‚ö†Ô∏è CUDA non disponible - utilisation CPU (plus lent)")
    except ImportError as e: issues.append(f"‚ùå D√©pendance manquante : {e}")

    try:
        mem = psutil.virtual_memory()
        issues.append(f"üíæ RAM disponible : {mem.available // (1024**3)} GB")
        if mem.available < 4 * 1024**3: # Moins de 4GB RAM est critique
            issues.append("‚ö†Ô∏è RAM insuffisante (< 4GB) - Le chargement risque d'√©chouer.")
    except ImportError: issues.append("‚ö†Ô∏è Impossible de v√©rifier la m√©moire syst√®me")

    return issues

def resize_image_if_needed(image, max_size=(800, 800)):
    """Redimensionne une image PIL si elle d√©passe `max_size` tout en conservant les proportions."""
    width, height = image.size
    if width > max_size[0] or height > max_size[1]:
        ratio = min(max_size[0] / width, max_size[1] / height)
        new_width = int(width * ratio)
        new_height = int(height * ratio)
        resized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        return resized_image, True # True: redimensionn√©e
    return image, False # False: non redimensionn√©e

def afficher_ram_disponible(context=""):
    """Affiche l'utilisation de la RAM."""
    try:
        mem = psutil.virtual_memory()
        st.info(f"üíæ RAM {context}: {mem.available // (1024**3)} GB disponible")
        if mem.available < 4 * 1024**3:
            st.warning("‚ö†Ô∏è Moins de 4GB de RAM disponible, le chargement du mod√®le risque d'√©chouer !")
    except ImportError:
        st.warning("‚ö†Ô∏è Impossible de v√©rifier la RAM syst√®me.")

# --- Fonctions d'Analyse avec Gemma 3n E4B IT ---
MODEL_ID_HF = "google/gemma-3n-E4B-it"
LOCAL_MODEL_PATH = "D:/Dev/model_gemma" # Chemin vers votre mod√®le local (ajustez si n√©cessaire)

def load_model():
    """Charge le mod√®le Gemma 3n E4B IT (local ou Hugging Face) avec des strat√©gies robustes."""
    try:
        from transformers import AutoProcessor, Gemma3nForConditionalGeneration

        # Diagnostic initial
        issues = diagnose_loading_issues()
        with st.expander("üìä Diagnostic syst√®me", expanded=False):
            for issue in issues:
                st.markdown(issue)

        # Nettoyer la m√©moire avant le chargement
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # D√©tecter si le mod√®le local existe
        is_local = os.path.exists(LOCAL_MODEL_PATH)

        # --- Strat√©gies de chargement ---
        strategies_to_try = []

        # 1. Mod√®le Local (si disponible)
        if is_local:
            strategies_to_try.append(("Local (ultra-conservateur CPU)", lambda: load_model_strategy(LOCAL_MODEL_PATH, device_map="cpu", torch_dtype=torch.bfloat16, quantization=None, force_persistence=True)))
            strategies_to_try.append(("Local (conservateur CPU)", lambda: load_model_strategy(LOCAL_MODEL_PATH, device_map="cpu", torch_dtype=torch.bfloat16, quantization=None, force_persistence=True))) # Ce sont les m√™mes pour le local, mais pour coh√©rence
        else:
            # 2. Mod√®le Hugging Face (strat√©gies diverses)
            st.info("Mod√®le local non trouv√©. Tentative de chargement depuis Hugging Face...")
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3 # en GB
                st.info(f"M√©moire GPU disponible : {gpu_memory:.1f} GB")
                
                # Strat√©gies GPU bas√©es sur la m√©moire
                if gpu_memory >= 8:
                    strategies_to_try.append(("Hugging Face (custom memory)", lambda: load_model_strategy(MODEL_ID_HF, device_map="auto", torch_dtype=torch.float16, quantization=None, force_persistence=True)))
                if gpu_memory >= 4:
                    strategies_to_try.append(("Hugging Face (4-bit quantization)", lambda: load_model_strategy(MODEL_ID_HF, device_map="auto", torch_dtype=torch.float16, quantization="4bit", force_persistence=True)))
                if gpu_memory >= 6: # Les mod√®les Gemma sont plus √† l'aise avec bfloat16 si support√©, sinon float16
                    strategies_to_try.append(("Hugging Face (8-bit quantization)", lambda: load_model_strategy(MODEL_ID_HF, device_map="auto", torch_dtype=torch.float16, quantization="8bit", force_persistence=True)))
                strategies_to_try.append(("Hugging Face (conservative GPU)", lambda: load_model_strategy(MODEL_ID_HF, device_map="auto", torch_dtype=torch.float16, quantization=None, force_persistence=True)))
            
            # Strat√©gie par d√©faut pour CPU si GPU absent ou trop petit
            strategies_to_try.append(("Hugging Face (conservative CPU)", lambda: load_model_strategy(MODEL_ID_HF, device_map="cpu", torch_dtype=torch.float32, quantization=None, force_persistence=True)))
            strategies_to_try.append(("Hugging Face (ultra-conservative CPU)", lambda: load_model_strategy(MODEL_ID_HF, device_map="cpu", torch_dtype=torch.float32, quantization=None, force_persistence=True))) # Double v√©rif CPU

        # Boucle pour essayer chaque strat√©gie
        for i, (name, strategy_func) in enumerate(strategies_to_try):
            st.info(f"Tentative {i+1}/{len(strategies_to_try)} : {name}...")
            try:
                model, processor = strategy_func()
                if model and processor:
                    st.success(f"‚úÖ Mod√®le charg√© avec succ√®s via la strat√©gie : {name}")
                    return model, processor
            except Exception as e:
                error_msg = str(e)
                if "disk_offload" in error_msg.lower() or "out of memory" in error_msg.lower():
                    st.warning(f"La strat√©gie {name} a √©chou√© (m√©moire/disk_offload). Tentative suivante...")
                    gc.collect()
                    if torch.cuda.is_available(): torch.cuda.empty_cache()
                    continue
                elif "403" in error_msg or "Forbidden" in error_msg:
                    st.error(f"‚ùå Erreur d'acc√®s Hugging Face (403) avec la strat√©gie {name}. V√©rifiez votre HF_TOKEN.")
                    return None, None # Arr√™ter si c'est une erreur d'authentification
                else:
                    st.warning(f"La strat√©gie {name} a √©chou√© : {error_msg}. Tentative suivante...")
                    gc.collect()
                    if torch.cuda.is_available(): torch.cuda.empty_cache()
                    continue
        
        # Si toutes les strat√©gies √©chouent
        st.error("Toutes les strat√©gies de chargement du mod√®le ont √©chou√©.")
        return None, None

    except ImportError as e:
        st.error(f"‚ùå Erreur de d√©pendance : {e}. Assurez-vous que transformers et torch sont install√©s.")
        return None, None
    except Exception as e:
        st.error(f"‚ùå Une erreur g√©n√©rale s'est produite lors du chargement du mod√®le : {e}")
        return None, None

def load_model_strategy(model_identifier, device_map=None, torch_dtype=None, quantization=None, force_persistence=False):
    """
    Charge un mod√®le et son processeur en utilisant des param√®tres sp√©cifiques.
    Retourne le mod√®le et le processeur, ou (None, None) en cas d'√©chec.
    """
    try:
        st.info(f"Chargement de {model_identifier} avec device_map='{device_map}', dtype={torch_dtype}, quant={quantization}...")
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        afficher_ram_disponible("avant chargement")

        # Configuration des arguments pour from_pretrained
        common_args = {
            "trust_remote_code": True,
            "low_cpu_mem_usage": True,
            "device_map": device_map,
            "torch_dtype": torch_dtype,
        }
        
        # Ajout des arguments de quantification si sp√©cifi√©s
        if quantization == "4bit":
            common_args.update({
                "load_in_4bit": True,
                "bnb_4bit_compute_dtype": torch.float16 # Souvent utilis√© avec 4-bit
            })
        elif quantization == "8bit":
            common_args.update({"load_in_8bit": True})
        
        # R√©cup√©rer le token HF pour l'authentification
        hf_token = os.environ.get("HF_TOKEN") or HfFolder.get_token()
        if hf_token:
            common_args["token"] = hf_token
            st.info("Utilisation du jeton Hugging Face pour l'authentification.")
        else:
            st.warning("Aucun jeton Hugging Face trouv√©. L'acc√®s aux mod√®les peut √™tre limit√©.")

        # Charger le processeur
        processor = AutoProcessor.from_pretrained(model_identifier, trust_remote_code=True, token=hf_token)
        
        # Charger le mod√®le
        model = Gemma3nForConditionalGeneration.from_pretrained(model_identifier, **common_args)
        
        afficher_ram_disponible("apr√®s chargement")
        
        # Stocker dans session_state et forcer la persistance
        st.session_state.model = model
        st.session_state.processor = processor
        st.session_state.model_loaded = True
        st.session_state.model_status = f"Charg√© ({device_map or 'auto'})"
        st.session_state.model_load_time = time.time()
        
        if force_persistence:
            if force_model_persistence():
                st.success("Mod√®le charg√© et persistant.")
            else:
                st.warning("Mod√®le charg√© mais probl√®me de persistance.")
        
        return model, processor

    except ImportError as e:
        raise ImportError(f"Biblioth√®que manquante : {e}. Installez-la avec `pip install transformers torch accelerate bitsandbytes`")
    except Exception as e:
        raise Exception(f"√âchec du chargement avec la strat√©gie {model_identifier} : {e}")

def analyze_image_multilingual(image, prompt=""):
    """Analyse une image avec Gemma 3n E4B IT pour un diagnostic pr√©cis."""
    if not st.session_state.model_loaded and not restore_model_from_cache():
        return "‚ùå Mod√®le Gemma non charg√©. Veuillez d'abord charger le mod√®le dans les r√©glages."
    
    model, processor = st.session_state.model, st.session_state.processor
    if not model or not processor:
        return "‚ùå Mod√®le Gemma non disponible. Veuillez recharger le mod√®le."

    try:
        # D√©finir le prompt textuel bas√© sur la langue et la question
        if st.session_state.language == "fr":
            gemma_prompt_text = f"Tu es un expert en pathologie v√©g√©tale. Analyse cette image et r√©ponds √† la question : {prompt}" if prompt else "Tu es un expert en pathologie v√©g√©tale. Analyse cette image et fournis un diagnostic pr√©cis."
        else:
            gemma_prompt_text = f"You are an expert in plant pathology. Analyze this image and answer the question: {prompt}" if prompt else "You are an expert in plant pathology. Analyze this image and provide a precise diagnosis."
        
        # Pr√©traiter l'image et le texte avec le processeur pour obtenir les inputs du mod√®le
        # Ceci est la m√©thode cl√© pour les mod√®les multimodaux comme Gemma3n
        processed_inputs = processor(
            images=[image], # L'image PIL doit √™tre dans une liste
            text=gemma_prompt_text,
            return_tensors="pt",
            padding=True
        )

        # D√©placer les inputs au bon device
        device = getattr(model, 'device', 'cpu')
        if hasattr(processed_inputs, 'to'):
            processed_inputs = processed_inputs.to(device)
        
        # G√©n√©rer la r√©ponse en passant les inputs pr√©par√©s
        # Les param√®tres comme max_new_tokens, temperature, etc., sont g√©r√©s ici.
        with torch.inference_mode():
            generation = model.generate(
                **processed_inputs,
                max_new_tokens=500,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )
            # D√©code uniquement la partie g√©n√©r√©e par le mod√®le
            # Il faut trouver le d√©calage correct, souvent l'input_ids g√©n√©r√© contient aussi le prompt.
            # L'erreur "0 image tokens" vient du fait que le mod√®le n'a pas bien int√©gr√© l'image.
            # En utilisant `processor(images=..., text=...)` on esp√®re corriger √ßa.
            # Pour le d√©codage, on prend la r√©ponse compl√®te pour le moment.
            
            # Si la r√©ponse contient le prompt, on pourrait essayer de le retirer.
            # C'est souvent le cas avec les mod√®les de chat.
            # Il faut d√©coder TOUS les tokens g√©n√©r√©s.
            response_text = processor.decode(generation[0], skip_special_tokens=True)

        # Nettoyer la r√©ponse
        final_response = response_text.strip()
        
        # Supprimer le prompt texte si le mod√®le l'a r√©p√©t√© au d√©but de sa r√©ponse
        if gemma_prompt_text.strip() in final_response:
            final_response = final_response.split(gemma_prompt_text.strip())[-1].strip()
            
        # Retirer les tokens sp√©ciaux qui pourraient √™tre visibles
        final_response = final_response.replace("<start_of_turn>", "").replace("<end_of_turn>", "").strip()

        # Formater la sortie
        if st.session_state.language == "fr":
            return f"""
## üß† **Analyse par Gemma 3n E4B IT**
{final_response}
"""
        else:
            return f"""
## üß† **Analysis by Gemma 3n E4B IT**
{final_response}
"""
            
    except ImportError:
        return "‚ùå Erreur : Les biblioth√®ques n√©cessaires (transformers, torch) ne sont pas install√©es."
    except Exception as e:
        error_message = str(e)
        if "403" in error_message or "Forbidden" in error_message:
            return "‚ùå Erreur 403 - Acc√®s refus√©. Veuillez v√©rifier votre jeton Hugging Face (HF_TOKEN) et les quotas."
        elif "Number of images does not match number of special image tokens" in error_message:
            return "‚ùå Erreur : Le mod√®le n'a pas pu traiter l'image. L'image n'est peut-√™tre pas correctement int√©gr√©e au prompt ou le format n'est pas reconnu. Essayez avec une image plus simple ou un autre format."
        else:
            return f"‚ùå Erreur lors de l'analyse d'image : {e}"

def analyze_text_multilingual(text):
    """Analyse un texte avec le mod√®le Gemma 3n E4B IT."""
    if not st.session_state.model_loaded and not restore_model_from_cache():
        return "‚ùå Mod√®le non charg√©. Veuillez le charger dans les r√©glages."
    
    model, processor = st.session_state.model, st.session_state.processor
    if not model or not processor:
        return "‚ùå Mod√®le Gemma non disponible. Veuillez recharger le mod√®le."
    
    try:
        # D√©finir le prompt bas√© sur la langue
        if st.session_state.language == "fr":
            prompt = f"Tu es un assistant agricole expert. Analyse ce probl√®me de plante : {text}\n\n**Instructions :**\n1. **Diagnostic** : Quel est le probl√®me principal ?\n2. **Causes** : Quelles sont les causes possibles ?\n3. **Traitement** : Quelles sont les actions √† entreprendre ?\n4. **Pr√©vention** : Comment √©viter le probl√®me √† l'avenir ?"
        else:
            prompt = f"You are an expert agricultural assistant. Analyze this plant problem: {text}\n\n**Instructions:**\n1. **Diagnosis**: What is the main problem?\n2. **Causes**: What are the possible causes?\n3. **Treatment**: What actions should be taken?\n4. **Prevention**: How to avoid the problem in the future?"
        
        # Pr√©parer les messages pour le mod√®le Gemma (format conversationnel)
        messages = [
            {"role": "user", "content": [{"type": "text", "text": prompt}]}
        ]
        
        # Utiliser le processeur pour convertir le format conversationnel en tenseurs
        inputs = processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
        )
        
        # D√©placer les inputs au bon device
        device = getattr(model, 'device', 'cpu')
        if hasattr(inputs, 'to'):
            inputs = inputs.to(device)
        
        # G√©n√©rer la r√©ponse
        with torch.inference_mode():
            generation = model.generate(
                **inputs,
                max_new_tokens=300,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )
            # D√©coder uniquement la partie g√©n√©r√©e (apr√®s le prompt)
            input_len = inputs["input_ids"].shape[-1]
            generation = generation[0][input_len:]
            response = processor.decode(generation, skip_special_tokens=True)
        
        return response.strip()
        
    except Exception as e:
        return f"‚ùå Erreur lors de l'analyse de texte : {e}"

# --- Interface Principale ---
st.title(t("title"))
st.markdown(t("subtitle"))

# --- Initialisation et V√©rifications ---
if 'model_loaded' not in st.session_state:
    st.session_state.model_loaded = False
if 'model_status' not in st.session_state:
    st.session_state.model_status = "Non charg√©"

# Tenter de restaurer le mod√®le au chargement de l'application si non d√©j√† charg√©
if not st.session_state.model_loaded:
    if restore_model_from_cache():
        st.success("üîÑ Mod√®le restaur√© automatiquement depuis le cache au d√©marrage.")
    else:
        st.info("üí° Cliquez sur 'Charger le mod√®le' dans les r√©glages pour commencer.")

# --- Sidebar pour la Configuration ---
with st.sidebar:
    st.header(t("config_title"))
    
    # S√©lecteur de langue
    st.subheader("üåê Langue / Language")
    language_options = ["Fran√ßais", "English"]
    current_lang_index = 0 if st.session_state.language == "fr" else 1
    language = st.selectbox(
        "S√©lectionnez votre langue :",
        language_options,
        index=current_lang_index,
        help="Change la langue de l'interface et des r√©ponses de l'IA."
    )
    if st.session_state.language != ("fr" if language == "Fran√ßais" else "en"):
        st.session_state.language = "fr" if language == "Fran√ßais" else "en"
        st.rerun() # Recharge pour appliquer le changement de langue

    st.divider()

    # Statut du Jeton Hugging Face
    st.subheader("üîë Jeton Hugging Face")
    hf_token_found = HfFolder.get_token() or os.environ.get("HF_TOKEN")
    if hf_token_found:
        st.success("‚úÖ Jeton HF trouv√© et configur√©.")
    else:
        st.warning("‚ö†Ô∏è Jeton HF non trouv√©.")
    st.info("Il est recommand√© de d√©finir la variable d'environnement `HF_TOKEN` avec votre jeton personnel Hugging Face pour √©viter les erreurs d'acc√®s (403).")
    st.markdown("[Obtenir un jeton HF](https://huggingface.co/settings/tokens)")

    st.divider()

    # Gestion du Mod√®le IA
    st.header("ü§ñ Mod√®le IA Gemma 3n")
    if st.session_state.model_loaded and check_model_persistence():
        st.success(f"‚úÖ Mod√®le charg√© ({st.session_state.model_status})")
        if st.session_state.model_load_time:
            load_time_str = time.strftime('%H:%M:%S', time.localtime(st.session_state.model_load_time))
            st.write(f"Heure de chargement : {load_time_str}")
        if hasattr(st.session_state.model, 'device'):
            st.write(f"Device utilis√© : `{st.session_state.model.device}`")
        
        col1_btn, col2_btn = st.columns(2)
        with col1_btn:
            if st.button("üîÑ Recharger le mod√®le", type="secondary"):
                st.session_state.model_loaded = False
                st.session_state.model = None
                st.session_state.processor = None
                st.session_state.global_model_cache.clear() # Vider le cache pour recharger
                st.session_state.model_persistence_check = False
                st.rerun()
        with col2_btn:
            if st.button("üíæ Forcer Persistance", type="secondary"):
                if force_model_persistence():
                    st.success("Persistance forc√©e avec succ√®s.")
                else:
                    st.error("√âchec de la persistance.")
                st.rerun()
    else:
        st.warning("‚ùå Mod√®le non charg√©.")
        if st.button(t("load_model"), type="primary"):
            with st.spinner("Chargement du mod√®le en cours..."):
                model, processor = load_model()
                if model and processor:
                    st.success("‚úÖ Mod√®le charg√© avec succ√®s !")
                else:
                    st.error("‚ùå √âchec du chargement du mod√®le.")
                st.rerun() # Recharger pour mettre √† jour le statut

# --- Onglets Principaux ---
tab1, tab2, tab3, tab4 = st.tabs(t("tabs"))

with tab1:
    st.header(t("image_analysis_title"))
    st.markdown(t("image_analysis_desc"))
    
    capture_option = st.radio(
        "Choisissez votre m√©thode :",
        ["üìÅ Upload d'image", "üì∑ Capture par webcam"],
        horizontal=True,
        key="image_capture_method"
    )
    
    uploaded_file = None
    captured_image = None
    
    if capture_option == "üìÅ Upload d'image":
        uploaded_file = st.file_uploader(
            t("choose_image"),
            type=['png', 'jpg', 'jpeg'],
            help="Formats accept√©s : PNG, JPG, JPEG (max 200MB). Privil√©giez des images claires.",
            accept_multiple_files=False,
            key="image_uploader"
        )
        if uploaded_file is not None:
            MAX_FILE_SIZE_BYTES = 200 * 1024 * 1024 # 200 MB
            if uploaded_file.size > MAX_FILE_SIZE_BYTES:
                st.error(t("file_too_large_error"))
                uploaded_file = None
            elif uploaded_file.size == 0:
                st.error(t("empty_file_error"))
                uploaded_file = None
            elif uploaded_file.size > (MAX_FILE_SIZE_BYTES * 0.8): # Avertissement si tr√®s volumineux
                st.warning(t("file_size_warning"))
    else: # Webcam capture
        st.markdown("**üì∑ Capture d'image par webcam**")
        st.info("üí° Positionnez votre plante malade devant la webcam et cliquez sur 'Prendre une photo'. Assurez-vous d'un bon √©clairage.")
        captured_image = st.camera_input("Prendre une photo de la plante", key="webcam_capture")
    
    image = None
    image_source = None
    
    if uploaded_file is not None:
        try:
            image = Image.open(uploaded_file)
            image_source = "upload"
        except Exception as e:
            st.error(f"‚ùå Erreur lors du traitement de l'image upload√©e : {e}")
            st.info("üí° Essayez avec une image diff√©rente ou un format diff√©rent (PNG, JPG, JPEG).")
    elif captured_image is not None:
        try:
            image = Image.open(captured_image)
            image_source = "webcam"
        except Exception as e:
            st.error(f"‚ùå Erreur lors du traitement de l'image captur√©e : {e}")
            st.info("üí° Essayez de reprendre la photo.")
    
    if image is not None:
        try:
            original_size = image.size
            image, was_resized = resize_image_if_needed(image, max_size=(800, 800))
            
            col1, col2 = st.columns([1, 1])
            with col1:
                st.image(image, caption=f"Image ({image_source})" if image_source else "Image", use_container_width=True)
                if was_resized:
                    st.warning(f"‚ö†Ô∏è L'image a √©t√© redimensionn√©e de {original_size} √† {image.size} pour optimiser le traitement.")
            
            with col2:
                st.markdown("**Informations de l'image :**")
                st.write(f"‚Ä¢ Format : {image.format}")
                st.write(f"‚Ä¢ Taille originale : {original_size[0]}x{original_size[1]} pixels")
                st.write(f"‚Ä¢ Taille actuelle : {image.size[0]}x{image.size[1]} pixels")
                st.write(f"‚Ä¢ Mode : {image.mode}")
            
            question = st.text_area(
                "Question sp√©cifique (optionnel) :",
                placeholder="Ex: Les feuilles ont des taches jaunes, que faire ?",
                height=100
            )
            
            if st.button(t("analyze_button"), disabled=not st.session_state.model_loaded, type="primary"):
                if not st.session_state.model_loaded:
                    st.error("‚ùå Mod√®le non charg√©. Veuillez le charger dans les r√©glages.")
                else:
                    with st.spinner("üîç Analyse d'image en cours..."):
                        result = analyze_image_multilingual(image, question)
                    
                    st.markdown(t("analysis_results"))
                    st.markdown("---")
                    st.markdown(result)
        except Exception as e:
            st.error(f"Erreur lors du traitement de l'image : {e}")

with tab2:
    st.header(t("text_analysis_title"))
    st.markdown(t("text_analysis_desc"))
    
    text_input = st.text_area(
        t("symptoms_desc"),
        placeholder="Ex: Mes tomates ont des taches brunes sur les feuilles et les fruits, une poudre blanche sur les tiges...",
        height=150
    )
    
    if st.button("üß† Analyser avec l'IA", disabled=not st.session_state.model_loaded, type="primary"):
        if not st.session_state.model_loaded:
            st.error("‚ùå Mod√®le non charg√©. Veuillez le charger dans les r√©glages.")
        elif not text_input.strip():
            st.error("‚ùå Veuillez saisir une description des sympt√¥mes.")
        else:
            with st.spinner("üîç Analyse de texte en cours..."):
                result = analyze_text_multilingual(text_input)
            
            st.markdown(t("analysis_results"))
            st.markdown("---")
            st.markdown(result)

with tab3:
    st.header(t("manual_title"))
    
    # Utilisation de la fonction de traduction pour le contenu du manuel
    manual_content = {
        "fr": """
        ### üöÄ **D√©marrage Rapide**
        1.  **Charger le mod√®le** : Cliquez sur 'Charger le mod√®le' dans les r√©glages (sidebar).
        2.  **Choisir le mode** : Allez √† l'onglet 'üì∏ Analyse d'Image' ou 'üí¨ Analyse de Texte'.
        3.  **Soumettre votre demande** : Upload d'image, capture webcam, ou description textuelle.
        4.  **Obtenir le diagnostic** : Lisez les r√©sultats avec recommandations.
        
        ### üì∏ **Analyse d'Image**
        *   **Formats accept√©s** : PNG, JPG, JPEG.
        *   **Qualit√©** : Privil√©giez des images claires, bien √©clair√©es, avec le probl√®me bien visible.
        *   **Redimensionnement** : Les images trop grandes sont automatiquement redimensionn√©es pour optimiser le traitement.
        
        ### üí¨ **Analyse de Texte**
        *   **Soyez pr√©cis** : D√©crivez les sympt√¥mes, le type de plante, les conditions de culture, et les actions d√©j√† tent√©es. Plus la description est d√©taill√©e, plus le diagnostic sera pertinent.
        
        ### üîç **Interpr√©tation des R√©sultats**
        *   Les r√©sultats incluent un diagnostic potentiel, les causes probables, des recommandations de traitement et des conseils de pr√©vention.
        *   Ces informations sont bas√©es sur l'IA et doivent √™tre consid√©r√©es comme un guide. Consultez un expert pour des cas critiques.
        
        ### üí° **Bonnes Pratiques**
        *   **Images multiples** : Si possible, prenez des photos sous diff√©rents angles.
        *   **√âclairage** : La lumi√®re naturelle est id√©ale.
        *   **Focus** : Assurez-vous que la zone affect√©e est nette et bien visible.
        
        ### üíæ **Persistance du Mod√®le**
        *   Une fois charg√©, le mod√®le est sauvegard√© dans le cache de l'application pour un chargement plus rapide lors des prochaines utilisations sur le m√™me environnement.
        *   Vous pouvez le recharger manuellement si n√©cessaire.
        
        ### üîí **Jeton Hugging Face (HF_TOKEN)**
        *   Pour garantir la stabilit√© et la performance lors du t√©l√©chargement de mod√®les depuis Hugging Face, il est fortement recommand√© de d√©finir la variable d'environnement `HF_TOKEN`.
        *   Cr√©ez un jeton de lecture sur [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) et d√©finissez-le dans votre environnement avant de lancer l'application.
        """,
        "en": """
        ### üöÄ **Quick Start**
        1.  **Load the model** : Click 'Load Model' in the settings (sidebar).
        2.  **Choose mode** : Go to 'üì∏ Image Analysis' or 'üí¨ Text Analysis' tab.
        3.  **Submit your request** : Upload an image, capture via webcam, or provide a text description.
        4.  **Get diagnosis** : Read the results with recommendations.
        
        ### üì∏ **Image Analysis**
        *   **Accepted formats** : PNG, JPG, JPEG.
        *   **Quality** : Prefer clear, well-lit images with the problem clearly visible.
        *   **Resizing** : Oversized images are automatically resized for processing optimization.
        
        ### üí¨ **Text Analysis**
        *   **Be specific** : Describe symptoms, plant type, growing conditions, and actions already taken. More detail leads to better accuracy.
        
        ### üîç **Result Interpretation**
        *   Results include a potential diagnosis, likely causes, treatment recommendations, and preventive advice.
        *   This AI-driven information is for guidance only. Consult a qualified expert for critical cases.
        
        ### üí° **Best Practices**
        *   **Multiple images** : If possible, take photos from different angles.
        *   **Lighting** : Natural light is ideal.
        *   **Focus** : Ensure the affected area is sharp and clearly visible.
        
        ### üíæ **Model Persistence**
        *   Once loaded, the model is cached for faster loading in future sessions on the same environment.
        *   You can manually reload it if needed.
        
        ### üîí **Hugging Face Token (HF_TOKEN)**
        *   To ensure stability and performance when downloading models from Hugging Face, it's highly recommended to set the environment variable `HF_TOKEN`.
        *   Create a read token on [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) and set it in your environment before launching the app.
        """
    }
    st.markdown(manual_content[st.session_state.language])

with tab4:
    st.header(t("about_title"))
    
    st.markdown("### üå± Notre Mission / Our Mission")
    st.markdown("AgriLens AI est une application de diagnostic des maladies de plantes utilisant l'intelligence artificielle pour aider les agriculteurs √† identifier et traiter les probl√®mes de leurs cultures.")
    
    st.markdown("### üöÄ Fonctionnalit√©s / Features")
    st.markdown("""
    ‚Ä¢ **Analyse d'images** : Diagnostic visuel des maladies
    ‚Ä¢ **Analyse de texte** : Conseils bas√©s sur les descriptions
    ‚Ä¢ **Recommandations pratiques** : Actions concr√®tes √† entreprendre
    ‚Ä¢ **Interface mobile** : Optimis√©e pour smartphones et tablettes
    ‚Ä¢ **Support multilingue** : Fran√ßais et Anglais
    """)
    
    st.markdown("### üîß Technologie / Technology")
    
    # D√©tecter l'environnement pour l'affichage
    is_local = os.path.exists(LOCAL_MODEL_PATH)
    
    if is_local:
        st.markdown("""
        ‚Ä¢ **Mod√®le** : Gemma 3n E4B IT (Local - D:/Dev/model_gemma)
        ‚Ä¢ **Framework** : Streamlit
        ‚Ä¢ **D√©ploiement** : Local
        """)
    else:
        st.markdown("""
        ‚Ä¢ **Mod√®le** : Gemma 3n E4B IT (Hugging Face - en ligne)
        ‚Ä¢ **Framework** : Streamlit
        ‚Ä¢ **D√©ploiement** : Hugging Face Spaces / en ligne
        """)
    
    # Informations du cr√©ateur
    st.markdown(f"### üë®‚Äçüíª Cr√©ateur de l'Application / Application Creator")
    st.markdown(f"**Sidoine Kolaol√© YEBADOKPO**")
    st.markdown(f"üìç Bohicon, R√©publique du B√©nin")
    st.markdown(f"üìû +229 01 96 91 13 46")
    st.markdown(f"üìß syebadokpo@gmail.com")
    st.markdown(f"üîó [linkedin.com/in/sidoineko](https://linkedin.com/in/sidoineko)")
    st.markdown(f"üìÅ [Hugging Face Portfolio](https://huggingface.co/Sidoineko)")
    
    # Informations de comp√©tition
    st.markdown(f"### üèÜ Version Comp√©tition Kaggle / Kaggle Competition Version")
    st.markdown("Cette premi√®re version d'AgriLens AI a √©t√© d√©velopp√©e sp√©cifiquement pour participer √† une comp√©tition Kaggle.")
    
    st.markdown("### ‚ö†Ô∏è Avertissement / Warning")
    st.markdown("Les r√©sultats fournis par l'IA sont √† titre indicatif uniquement et ne remplacent pas l'avis d'un expert agricole qualifi√©.")

# --- Pied de page ---
st.markdown("---")
st.markdown(t("footer"))