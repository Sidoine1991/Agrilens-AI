"""
================================================================================
AGRILENS AI - APPLICATION DE DIAGNOSTIC DES MALADIES DE PLANTES
================================================================================

Auteur: Sidoine Kolaol√© YEBADOKPO
Email: syebadokpo@gmail.com
Localisation: Bohicon, R√©publique du B√©nin
Version: 2.0 (avec mode mobile)
Date: Juillet 2025

DESCRIPTION:
-----------
AgriLens AI est une application d'intelligence artificielle con√ßue pour diagnostiquer
les maladies des plantes √† partir d'images ou de descriptions textuelles. L'application
utilise le mod√®le Gemma 3n de Google pour analyser les sympt√¥mes et fournir des
recommandations de traitement.

FONCTIONNALIT√âS PRINCIPALES:
---------------------------
1. Analyse d'images de plantes malades
2. Analyse de descriptions textuelles de sympt√¥mes
3. Interface multilingue (Fran√ßais/English)
4. Mode mobile et desktop
5. Fonctionnement offline
6. Persistance du mod√®le en cache
7. Support pour Hugging Face Spaces

ARCHITECTURE TECHNIQUE:
----------------------
- Framework: Streamlit (Python)
- Mod√®le IA: Gemma 3n E4B IT (Google)
- Biblioth√®ques: Transformers, PyTorch, PIL
- D√©ploiement: Hugging Face Spaces + Local
- Interface: Responsive (Mobile/Desktop)

STRUCTURE DU CODE:
-----------------
1. Configuration et imports
2. Syst√®me de traduction multilingue
3. Gestion du mode mobile/desktop
4. Chargement et persistance des mod√®les
5. Analyse d'images et de texte
6. Interface utilisateur (onglets)
7. Documentation et manuel

UTILISATION:
-----------
1. Lancer: streamlit run src/streamlit_app_multilingual.py
2. Charger le mod√®le via la sidebar
3. Choisir le mode d'analyse (image ou texte)
4. Soumettre les donn√©es pour diagnostic
5. Consulter les r√©sultats et recommandations

REQUIS SYST√àME:
--------------
- Python 3.8+
- RAM: 8GB minimum (16GB recommand√©)
- GPU: Optionnel (CUDA support√©)
- Espace disque: 5GB pour les mod√®les

LICENCE:
--------
Projet d√©velopp√© pour la Google - Gemma 3n Hackathon
Licence Creative Commons Attribution 4.0 International (CC BY 4.0)
Attribution : Sidoine Kolaol√© YEBADOKPO
Usage √©ducatif, commercial et de d√©monstration autoris√©

================================================================================
"""

import streamlit as st
import os
import io
from PIL import Image
import torch
import gc
import time
import sys
import psutil
from transformers import AutoProcessor, Gemma3nForConditionalGeneration
from huggingface_hub import HfFolder
from datetime import datetime

# Import optionnel pour Gemini (pour √©viter les erreurs si pas install√©)
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

# --- Configuration de la Page ---
# Configuration de base de l'interface Streamlit

# CSS personnalis√© pour s'assurer que le menu hamburger soit visible
st.markdown("""
<style>
/* S'assurer que le menu hamburger soit visible */
#MainMenu {visibility: visible !important;}
footer {visibility: hidden;}
header {visibility: visible;}
.stDeployButton {display: none;}
</style>
""", unsafe_allow_html=True)
st.set_page_config(
    page_title="AgriLens AI - Diagnostic des Plantes",
    page_icon="üå±",
    layout="centered",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://docs.streamlit.io/',
        'Report a bug': "https://github.com/streamlit/streamlit/issues",
        'About': "# AgriLens AI\nApplication de diagnostic des maladies de plantes par IA"
    }
)

# Configuration pour forcer l'affichage du menu
st.markdown("""
<script>
// Forcer l'affichage du menu hamburger
document.addEventListener('DOMContentLoaded', function() {
    const menuButton = document.querySelector('[data-testid="baseButton-secondary"]');
    if (menuButton) {
        menuButton.style.display = 'block';
    }
});
</script>
""", unsafe_allow_html=True)

# --- Mode Mobile Detection et Configuration ---
def is_mobile():
    """
    D√©tecte si l'utilisateur est en mode mobile.
    
    Returns:
        bool: True si le mode mobile est activ√©, False sinon
    """
    return st.session_state.get('mobile_mode', False)

def toggle_mobile_mode():
    """
    Bascule entre le mode desktop et mobile.
    Met √† jour l'√©tat de session et d√©clenche un rechargement de l'interface.
    """
    if 'mobile_mode' not in st.session_state:
        st.session_state.mobile_mode = False
    st.session_state.mobile_mode = not st.session_state.mobile_mode

# --- CSS pour Mode Mobile ---
# Styles CSS personnalis√©s pour l'interface mobile et desktop
MOBILE_CSS = """
<style>
    /* Mode Mobile - Interface simulant un smartphone */
    .mobile-container {
        max-width: 375px !important;
        margin: 0 auto !important;
        border: 2px solid #ddd !important;
        border-radius: 20px !important;
        padding: 20px !important;
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%) !important;
        box-shadow: 0 8px 32px rgba(0,0,0,0.1) !important;
        position: relative !important;
        overflow: hidden !important;
    }
    
    /* Indicateur de notch pour smartphone */
    .mobile-container::before {
        content: '';
        position: absolute;
        top: 0;
        left: 50%;
        transform: translateX(-50%);
        width: 60px;
        height: 4px;
        background: #333;
        border-radius: 0 0 10px 10px;
    }
    
    /* Header mobile avec gradient */
    .mobile-header {
        text-align: center;
        margin-bottom: 20px;
        padding: 10px;
        background: linear-gradient(135deg, #28a745 0%, #20c997 100%);
        border-radius: 15px;
        color: white;
        font-weight: bold;
    }
    
    /* Badge de statut offline */
    .mobile-status {
        display: inline-block;
        padding: 5px 10px;
        background: #28a745;
        color: white;
        border-radius: 20px;
        font-size: 12px;
        margin-top: 10px;
    }
    
    /* Boutons stylis√©s pour mobile */
    .mobile-button {
        background: linear-gradient(135deg, #007bff 0%, #0056b3 100%) !important;
        color: white !important;
        border: none !important;
        border-radius: 25px !important;
        padding: 12px 24px !important;
        font-weight: bold !important;
        box-shadow: 0 4px 15px rgba(0,123,255,0.3) !important;
        transition: all 0.3s ease !important;
    }
    
    .mobile-button:hover {
        transform: translateY(-2px) !important;
        box-shadow: 0 6px 20px rgba(0,123,255,0.4) !important;
    }
    
    /* Champs de saisie arrondis */
    .mobile-input {
        border-radius: 15px !important;
        border: 2px solid #e9ecef !important;
        padding: 12px !important;
        background: white !important;
    }
    
    /* Conteneurs d'onglets */
    .mobile-tab {
        background: #f8f9fa !important;
        border-radius: 15px !important;
        padding: 15px !important;
        margin: 10px 0 !important;
    }
    
    /* Styles pour les onglets en mode mobile */
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px !important;
    }
    
    .stTabs [data-baseweb="tab"] {
        border-radius: 15px !important;
        background: #e9ecef !important;
        color: #495057 !important;
        font-weight: bold !important;
        padding: 8px 16px !important;
    }
    
    .stTabs [aria-selected="true"] {
        background: linear-gradient(135deg, #28a745 0%, #20c997 100%) !important;
        color: white !important;
    }
    
    /* Styles pour les boutons en mode mobile */
    .stButton > button {
        border-radius: 25px !important;
        font-weight: bold !important;
        padding: 12px 24px !important;
    }
    
    /* Styles pour les inputs en mode mobile */
    .stTextInput > div > div > input {
        border-radius: 15px !important;
        border: 2px solid #e9ecef !important;
        padding: 12px !important;
    }
    
    /* Styles pour les file uploaders en mode mobile */
    .stFileUploader > div {
        border-radius: 15px !important;
        border: 2px dashed #28a745 !important;
        background: #f8f9fa !important;
    }
    
    /* Mode Desktop - Interface classique */
    .desktop-container {
        max-width: 100% !important;
        margin: 0 auto !important;
        padding: 20px !important;
    }
    
    /* Design responsive pour petits √©crans */
    @media (max-width: 768px) {
        .mobile-container {
            max-width: 100% !important;
            border-radius: 0 !important;
            margin: 0 !important;
        }
    }
</style>
"""

# Appliquer le CSS personnalis√©
st.markdown(MOBILE_CSS, unsafe_allow_html=True)

# --- Syst√®me de Traduction Multilingue ---
# Dictionnaire contenant toutes les traductions de l'interface utilisateur
# Structure: {"cl√©": {"fr": "texte fran√ßais", "en": "texte anglais"}}
TRANSLATIONS = {
    "title": {"fr": "AgriLens AI", "en": "AgriLens AI"},
    "subtitle": {"fr": "Votre assistant IA pour le diagnostic des maladies de plantes", "en": "Your AI Assistant for Plant Disease Diagnosis"},
    "config_title": {"fr": "Configuration", "en": "Configuration"},
    "load_model": {"fr": "Charger le mod√®le Gemma 3n E4B IT", "en": "Load Gemma 3n E4B IT Model"},
    "model_status": {"fr": "Statut du mod√®le :", "en": "Model Status:"},
    "not_loaded": {"fr": "Non charg√©", "en": "Not loaded"},
    "loaded": {"fr": "‚úÖ Charg√©", "en": "‚úÖ Loaded"},
    "error": {"fr": "‚ùå Erreur", "en": "‚ùå Error"},
    "tabs": {"fr": ["üì∏ Analyse d'Image", "üí¨ Analyse de Texte", "üìñ Manuel", "‚ÑπÔ∏è √Ä propos"], "en": ["üì∏ Image Analysis", "üí¨ Text Analysis", "üìñ Manual", "‚ÑπÔ∏è About"]},
    "image_analysis_title": {"fr": "üîç Diagnostic par Image", "en": "üîç Image Diagnosis"},
    "image_analysis_desc": {"fr": "T√©l√©chargez une photo de plante malade pour obtenir un diagnostic", "en": "Upload a photo of a diseased plant to get a diagnosis"},
    "choose_image": {"fr": "Choisissez une image...", "en": "Choose an image..."},
    "file_too_large_error": {"fr": "Erreur : Le fichier est trop volumineux. Maximum 200MB.", "en": "Error: File too large. Maximum 200MB."},
    "empty_file_error": {"fr": "Erreur : Le fichier est vide.", "en": "Error: File is empty."},
    "file_size_warning": {"fr": "Attention : Le fichier est tr√®s volumineux, le chargement peut prendre du temps.", "en": "Warning: File is very large, loading may take time."},
    "analyze_button": {"fr": "üî¨ Analyser avec l'IA", "en": "üî¨ Analyze with AI"},
    "analysis_results": {"fr": "## üìä R√©sultats de l'Analyse", "en": "## üìä Analysis Results"},
    "text_analysis_title": {"fr": "üí¨ Diagnostic par Texte", "en": "üí¨ Text Analysis"},
    "text_analysis_desc": {"fr": "D√©crivez les sympt√¥mes de votre plante pour obtenir des conseils", "en": "Describe your plant's symptoms to get advice"},
    "symptoms_desc": {"fr": "Description des sympt√¥mes :", "en": "Symptom description:"},
    "manual_title": {"fr": "üìñ Manuel d'utilisation", "en": "üìñ User Manual"},
    "about_title": {"fr": "‚ÑπÔ∏è √Ä propos d'AgriLens AI", "en": "‚ÑπÔ∏è About AgriLens AI"},
    "creator_title": {"fr": "üë®‚Äçüíª Cr√©ateur de l'Application", "en": "üë®‚Äçüíª Application Creator"},
    "creator_name": {"fr": "**Sidoine Kolaol√© YEBADOKPO**", "en": "**Sidoine Kolaol√© YEBADOKPO**"},
    "creator_location": {"fr": "Bohicon, R√©publique du B√©nin", "en": "Bohicon, Benin Republic"},
    "creator_phone": {"fr": "+229 01 96 91 13 46", "en": "+229 01 96 91 13 46"},
    "creator_email": {"fr": "syebadokpo@gmail.com", "en": "syebadokpo@gmail.com"},
    "creator_linkedin": {"fr": "linkedin.com/in/sidoineko", "en": "linkedin.com/in/sidoineko"},
    "creator_portfolio": {"fr": "Hugging Face Portfolio: https://huggingface.co/spaces/Sidoineko/portfolio", "en": "Hugging Face Portfolio: https://huggingface.co/spaces/Sidoineko/portfolio"},
    "competition_title": {"fr": "üèÜ Version Comp√©tition Kaggle", "en": "üèÜ Kaggle Competition Version"},
    "competition_text": {"fr": "Cette premi√®re version d'AgriLens AI a √©t√© d√©velopp√©e sp√©cifiquement pour participer √† la comp√©tition Kaggle. Elle repr√©sente notre premi√®re production publique et d√©montre notre expertise en IA appliqu√©e √† l'agriculture.", "en": "This first version of AgriLens AI was specifically developed to participate in the Kaggle competition. It represents our first public production and demonstrates our expertise in AI applied to agriculture."},
    "footer": {"fr": "*AgriLens AI - Diagnostic intelligent des plantes avec IA*", "en": "*AgriLens AI - Intelligent plant diagnosis with AI*"},
    "mobile_mode": {"fr": "üì± Mode Mobile", "en": "üì± Mobile Mode"},
    "desktop_mode": {"fr": "üíª Mode Desktop", "en": "üíª Desktop Mode"},
    "toggle_mode": {"fr": "üîÑ Changer de mode", "en": "üîÑ Toggle Mode"},
    "offline_status": {"fr": "Mode: OFFLINE", "en": "Mode: OFFLINE"},
    "online_status": {"fr": "Mode: ONLINE", "en": "Mode: ONLINE"},
    "mobile_demo": {"fr": "D√©mo App Mobile", "en": "Mobile App Demo"},
    "mobile_desc": {"fr": "Interface simulant l'application mobile offline", "en": "Interface simulating the offline mobile application"},
    "language_selection": {"fr": "S√©lectionnez votre langue :", "en": "Select your language:"},
    "language_help": {"fr": "Change la langue de l'interface et des r√©ponses de l'IA.", "en": "Changes the language of the interface and AI responses."},
    "hf_token_title": {"fr": "üîë Jeton Hugging Face", "en": "üîë Hugging Face Token"},
    "hf_token_found": {"fr": "‚úÖ Jeton HF trouv√© et configur√©.", "en": "‚úÖ HF token found and configured."},
    "hf_token_not_found": {"fr": "‚ö†Ô∏è Jeton HF non trouv√©.", "en": "‚ö†Ô∏è HF token not found."},
    "hf_token_info": {"fr": "Il est recommand√© de d√©finir la variable d'environnement `HF_TOKEN` avec votre jeton personnel Hugging Face pour √©viter les erreurs d'acc√®s (403).", "en": "It is recommended to set the `HF_TOKEN` environment variable with your personal Hugging Face token to avoid access errors (403)."},
    "get_hf_token": {"fr": "Obtenir un jeton HF", "en": "Get HF token"},
    "model_title": {"fr": "ü§ñ Mod√®le IA Gemma 3n", "en": "ü§ñ Gemma 3n AI Model"},
    "model_loaded": {"fr": "‚úÖ Mod√®le charg√©", "en": "‚úÖ Model loaded"},
    "model_not_loaded": {"fr": "‚ùå Mod√®le non charg√©.", "en": "‚ùå Model not loaded."},
    "load_time": {"fr": "Heure de chargement : ", "en": "Load time: "},
    "device_used": {"fr": "Device utilis√© : ", "en": "Device used: "},
    "reload_model": {"fr": "üîÑ Recharger le mod√®le", "en": "üîÑ Reload model"},
    "force_persistence": {"fr": "üíæ Forcer Persistance", "en": "üíæ Force Persistence"},
    "persistence_success": {"fr": "Persistance forc√©e avec succ√®s.", "en": "Persistence forced successfully."},
    "persistence_failed": {"fr": "√âchec de la persistance.", "en": "Persistence failed."},
    "loading_model": {"fr": "Chargement du mod√®le en cours...", "en": "Loading model..."},
    "model_loaded_success": {"fr": "‚úÖ Mod√®le charg√© avec succ√®s !", "en": "‚úÖ Model loaded successfully!"},
    "model_load_failed": {"fr": "‚ùå √âchec du chargement du mod√®le.", "en": "‚ùå Model loading failed."},
    "persistence_title": {"fr": "üíæ Persistance du Mod√®le", "en": "üíæ Model Persistence"},
    "persistence_loaded": {"fr": "‚úÖ Mod√®le charg√© et persistant en cache.", "en": "‚úÖ Model loaded and persistent in cache."},
    "persistence_warning": {"fr": "‚ö†Ô∏è Mod√®le charg√© mais non persistant. Cliquez sur 'Forcer Persistance'.", "en": "‚ö†Ô∏è Model loaded but not persistent. Click 'Force Persistence'."},
    "persistence_not_loaded": {"fr": "‚ö†Ô∏è Mod√®le non charg√©.", "en": "‚ö†Ô∏è Model not loaded."},
    "upload_image": {"fr": "üìÅ Upload d'image", "en": "üìÅ Upload image"},
    "webcam_capture": {"fr": "üì∑ Capture par webcam", "en": "üì∑ Webcam capture"},
    "choose_method": {"fr": "Choisissez votre m√©thode :", "en": "Choose your method:"},
    "webcam_title": {"fr": "**üì∑ Capture d'image par webcam**", "en": "**üì∑ Webcam image capture**"},
    "image_info_title": {"fr": "**Informations de l'image :**", "en": "**Image information:**"},
    "format_label": {"fr": "‚Ä¢ Format : ", "en": "‚Ä¢ Format: "},
    "original_size": {"fr": "‚Ä¢ Taille originale : ", "en": "‚Ä¢ Original size: "},
    "current_size": {"fr": "‚Ä¢ Taille actuelle : ", "en": "‚Ä¢ Current size: "},
    "mode_label": {"fr": "‚Ä¢ Mode : ", "en": "‚Ä¢ Mode: "},
    "pixels": {"fr": " pixels", "en": " pixels"},
    "symptoms_label": {"fr": "Description des sympt√¥mes :", "en": "Symptom description:"},
    "mission_title": {"fr": "### üå± Notre Mission / Our Mission", "en": "### üå± Our Mission"},
    "mission_text": {"fr": "AgriLens AI est une application de diagnostic des maladies de plantes utilisant l'intelligence artificielle pour aider les agriculteurs √† identifier et traiter les probl√®mes de leurs cultures.", "en": "AgriLens AI is a plant disease diagnosis application using artificial intelligence to help farmers identify and treat problems with their crops."},
    "features_title": {"fr": "### üöÄ Fonctionnalit√©s / Features", "en": "### üöÄ Features"},
    "features_text": {"fr": "‚Ä¢ **Analyse d'images** : Diagnostic visuel des maladies\n‚Ä¢ **Analyse de texte** : Conseils bas√©s sur les descriptions\n‚Ä¢ **Recommandations pratiques** : Actions concr√®tes √† entreprendre\n‚Ä¢ **Interface optimis√©e** : Pour une utilisation sur divers appareils\n‚Ä¢ **Support multilingue** : Fran√ßais et Anglais", "en": "‚Ä¢ **Image analysis** : Visual diagnosis of diseases\n‚Ä¢ **Text analysis** : Advice based on descriptions\n‚Ä¢ **Practical recommendations** : Concrete actions to take\n‚Ä¢ **Optimized interface** : For use on various devices\n‚Ä¢ **Multilingual support** : French and English"},
    "technology_title": {"fr": "### üîß Technologie / Technology", "en": "### üîß Technology"},
    "local_model_text": {"fr": "‚Ä¢ **Mod√®le** : Gemma 3n E4B IT (Local - {path})\n‚Ä¢ **Framework** : Streamlit\n‚Ä¢ **D√©ploiement** : Local", "en": "‚Ä¢ **Model** : Gemma 3n E4B IT (Local - {path})\n‚Ä¢ **Framework** : Streamlit\n‚Ä¢ **Deployment** : Local"},
    "online_model_text": {"fr": "‚Ä¢ **Mod√®le** : Gemma 3n E4B IT (Hugging Face - en ligne)\n‚Ä¢ **Framework** : Streamlit\n‚Ä¢ **D√©ploiement** : Hugging Face Spaces", "en": "‚Ä¢ **Model** : Gemma 3n E4B IT (Hugging Face - online)\n‚Ä¢ **Framework** : Streamlit\n‚Ä¢ **Deployment** : Hugging Face Spaces"},
    "warning_title": {"fr": "### ‚ö†Ô∏è Avertissement / Warning", "en": "### ‚ö†Ô∏è Warning"},
    "warning_text": {"fr": "Les r√©sultats fournis par l'IA sont √† titre indicatif uniquement et ne remplacent pas l'avis d'un expert agricole qualifi√©.", "en": "The results provided by AI are for guidance only and do not replace the advice of a qualified agricultural expert."},
    "support_title": {"fr": "### üìû Support", "en": "### üìû Support"},
    "support_text": {"fr": "Pour toute question ou probl√®me, consultez la documentation ou contactez le cr√©ateur.", "en": "For any questions or issues, consult the documentation or contact the creator."},
    "settings_button": {"fr": "‚öôÔ∏è R√©glages", "en": "‚öôÔ∏è Settings"},
    "specific_question": {"fr": "Question sp√©cifique (optionnel) :", "en": "Specific question (optional):"},
    "question_placeholder": {"fr": "Ex: Les feuilles ont des taches jaunes, que faire ?", "en": "Ex: The leaves have yellow spots, what to do?"},
    "webcam_info": {"fr": "üí° Positionnez votre plante malade devant la webcam et cliquez sur 'Prendre une photo'. Assurez-vous d'un bon √©clairage.", "en": "üí° Position your sick plant in front of the webcam and click 'Take a photo'. Make sure you have good lighting."},
    "take_photo": {"fr": "Prendre une photo de la plante", "en": "Take a photo of the plant"},
    "image_processing_error": {"fr": "‚ùå Erreur lors du traitement de l'image upload√©e : ", "en": "‚ùå Error processing uploaded image: "},
    "image_processing_error_webcam": {"fr": "‚ùå Erreur lors du traitement de l'image captur√©e : ", "en": "‚ùå Error processing captured image: "},
    "try_different_image": {"fr": "üí° Essayez avec une image diff√©rente ou un format diff√©rent (PNG, JPG, JPEG).", "en": "üí° Try with a different image or format (PNG, JPG, JPEG)."},
    "try_retake_photo": {"fr": "üí° Essayez de reprendre la photo.", "en": "üí° Try taking the photo again."},
    "image_resized_warning": {"fr": "‚ö†Ô∏è L'image a √©t√© redimensionn√©e de  √† {new_size} pour optimiser le traitement.", "en": "‚ö†Ô∏è Image has been resized from  to {new_size} to optimize processing."},
    "model_not_loaded_error": {"fr": "‚ùå Mod√®le non charg√©. Veuillez le charger dans les r√©glages.", "en": "‚ùå Model not loaded. Please load it in settings."},
    "analyzing_image": {"fr": "üîç Analyse d'image en cours...", "en": "üîç Analyzing image..."},
    "image_processing_general_error": {"fr": "Erreur lors du traitement de l'image : ", "en": "Error processing image: "},
    "symptoms_placeholder": {"fr": "Ex: Mes tomates ont des taches brunes sur les feuilles et les fruits, une poudre blanche sur les tiges...", "en": "Ex: My tomatoes have brown spots on leaves and fruits, white powder on stems..."},
    "culture_clarification": {"fr": "üå± Clarification de la Culture", "en": "üå± Culture Clarification"},
    "culture_question": {"fr": "Quelle est la culture concern√©e ?", "en": "What is the crop concerned?"},
    "culture_placeholder": {"fr": "Ex: Tomate, Piment, Ma√Øs, Haricot, Aubergine...", "en": "Ex: Tomato, Pepper, Corn, Bean, Eggplant..."},
    "culture_help": {"fr": "Pr√©cisez le type de plante pour un diagnostic plus pr√©cis", "en": "Specify the plant type for more accurate diagnosis"},
    "diagnosis_with_culture": {"fr": "üî¨ Diagnostic avec Culture Sp√©cifi√©e", "en": "üî¨ Diagnosis with Specified Culture"},
    "culture_specified": {"fr": "Culture sp√©cifi√©e : ", "en": "Specified culture: "},
    "export_diagnostic": {"fr": "üìÑ Exporter le Diagnostic", "en": "üìÑ Export Diagnosis"},
    "export_html": {"fr": "üíª Exporter en HTML", "en": "üíª Export as HTML"},
    "export_text": {"fr": "üìù Exporter en Texte", "en": "üìù Export as Text"},
    "download_html": {"fr": "T√©l√©charger HTML", "en": "Download HTML"},
    "download_text": {"fr": "T√©l√©charger Texte", "en": "Download Text"},
    "export_success": {"fr": "‚úÖ Diagnostic export√© avec succ√®s !", "en": "‚úÖ Diagnosis exported successfully!"},
    "export_error": {"fr": "‚ùå Erreur lors de l'export", "en": "‚ùå Export error"},
    "html_filename": {"fr": "diagnostic_agrilens_{date}.html", "en": "agrilens_diagnosis_{date}.html"},
    "text_filename": {"fr": "diagnostic_agrilens_{date}.txt", "en": "agrilens_diagnosis_{date}.txt"}
}

def t(key):
    """
    Fonction de traduction simple pour l'interface multilingue.
    
    Args:
        key (str): Cl√© de traduction √† rechercher dans le dictionnaire TRANSLATIONS
        
    Returns:
        str: Texte traduit dans la langue actuelle, ou la cl√© si la traduction n'existe pas
        
    Example:
        >>> t("title")  # Retourne "AgriLens AI" en fran√ßais ou anglais selon la langue active
    """
    if 'language' not in st.session_state:
        st.session_state.language = 'fr'
    lang = st.session_state.language
    return TRANSLATIONS.get(key, {}).get(lang, key)

# --- Initialisation de la langue ---
if 'language' not in st.session_state:
    st.session_state.language = 'fr'

# --- Cache global pour le mod√®le ---
if 'global_model_cache' not in st.session_state:
    st.session_state.global_model_cache = {}
if 'model_load_time' not in st.session_state:
    st.session_state.model_load_time = None
if 'model_persistence_check' not in st.session_state:
    st.session_state.model_persistence_check = False

def check_model_persistence():
    """
    V√©rifie si le mod√®le est toujours persistant en m√©moire et fonctionnel.
    
    Cette fonction contr√¥le si le mod√®le charg√© est toujours disponible
    et accessible dans la session Streamlit, ce qui est important pour
    √©viter les rechargements inutiles.
    
    Returns:
        bool: True si le mod√®le est persistant et fonctionnel, False sinon
        
    Note:
        Cette v√©rification est particuli√®rement importante sur Hugging Face Spaces
        o√π la m√©moire peut √™tre limit√©e et les mod√®les peuvent √™tre d√©charg√©s.
    """
    try:
        if hasattr(st.session_state, 'model') and st.session_state.model is not None:
            if hasattr(st.session_state.model, 'device'):
                device = st.session_state.model.device
                return True
        return False
    except Exception:
        return False

def force_model_persistence():
    """
    Force la persistance du mod√®le et du processeur dans le cache global.
    
    Cette fonction sauvegarde explicitement le mod√®le et le processeur
    dans le cache de session pour garantir leur disponibilit√© entre
    les interactions utilisateur.
    
    Returns:
        bool: True si la persistance a r√©ussi, False sinon
        
    Note:
        Cette fonction est utile pour √©viter les rechargements co√ªteux
        du mod√®le, particuli√®rement important pour les mod√®les volumineux
        comme Gemma 3n.
    """
    try:
        if hasattr(st.session_state, 'model') and st.session_state.model is not None:
            # Sauvegarder le mod√®le et le processeur
            st.session_state.global_model_cache['model'] = st.session_state.model
            st.session_state.global_model_cache['processor'] = st.session_state.processor
            st.session_state.global_model_cache['load_time'] = time.time()
            st.session_state.global_model_cache['model_type'] = type(st.session_state.model).__name__
            st.session_state.global_model_cache['processor_type'] = type(st.session_state.processor).__name__
            
            # Sauvegarder les informations sur le device
            if hasattr(st.session_state.model, 'device'):
                st.session_state.global_model_cache['device'] = st.session_state.model.device

            # V√©rifier que la sauvegarde a r√©ussi
            if st.session_state.global_model_cache.get('model') is not None:
                st.session_state.model_persistence_check = True
                return True
        return False
    except Exception:
        return False

def restore_model_from_cache():
    """Restaure le mod√®le et le processeur depuis le cache global."""
    try:
        if 'model' in st.session_state.global_model_cache and st.session_state.global_model_cache['model'] is not None:
            cached_model = st.session_state.global_model_cache['model']
            if hasattr(cached_model, 'device'):
                st.session_state.model = cached_model
                st.session_state.processor = st.session_state.global_model_cache.get('processor')
                st.session_state.model_loaded = True
                st.session_state.model_status = "Charg√© (cache)"
                if 'load_time' in st.session_state.global_model_cache:
                    st.session_state.model_load_time = st.session_state.global_model_cache['load_time']
                return True
        return False
    except Exception:
        return False

def diagnose_loading_issues():
    """Diagnostique les probl√®mes potentiels de chargement (d√©pendances, ressources, etc.)."""
    issues = []
    
    if not HfFolder.get_token() and not os.environ.get("HF_TOKEN"):
        issues.append("‚ö†Ô∏è **Jeton Hugging Face (HF_TOKEN) non configur√©.** Le t√©l√©chargement du mod√®le pourrait √©chouer ou √™tre ralenti.")

    try:
        import transformers; issues.append(f"‚úÖ Transformers v{transformers.__version__}")
        import torch; issues.append(f"‚úÖ PyTorch v{torch.__version__}")
        if torch.cuda.is_available(): issues.append(f"‚úÖ CUDA disponible : {torch.cuda.get_device_name(0)}")
        else: issues.append("‚ö†Ô∏è CUDA non disponible - utilisation CPU (plus lent)")
    except ImportError as e: issues.append(f"‚ùå D√©pendance manquante : ")

    try:
        mem = psutil.virtual_memory()
        issues.append(f"üíæ RAM disponible : {mem.available // (1024**3)} GB")
        if mem.available < 4 * 1024**3:
            issues.append("‚ö†Ô∏è RAM insuffisante (< 4GB) - Le chargement risque d'√©chouer.")
    except ImportError: issues.append("‚ö†Ô∏è Impossible de v√©rifier la m√©moire syst√®me")

    return issues

def resize_image_if_needed(image, max_size=(800, 800)):
    """Redimensionne une image PIL si elle d√©passe `max_size` tout en conservant les proportions."""
    width, height = image.size
    if width > max_size[0] or height > max_size[1]:
        ratio = min(max_size[0] / width, max_size[1] / height)
        new_width = int(width * ratio)
        new_height = int(height * ratio)
        resized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        return resized_image, True
    return image, False

def afficher_ram_disponible(context=""):
    """Affiche l'utilisation de la RAM."""
    try:
        mem = psutil.virtual_memory()
        st.info(f"üíæ RAM : {mem.available // (1024**3)} GB disponible")
        if mem.available < 4 * 1024**3:
            st.warning("‚ö†Ô∏è Moins de 4GB de RAM disponible, le chargement du mod√®le risque d'√©chouer !")
    except ImportError:
        st.warning("‚ö†Ô∏è Impossible de v√©rifier la RAM syst√®me.")

def generate_html_diagnostic(diagnostic_text, culture=None, image_info=None, timestamp=None):
    """
    G√©n√®re un fichier HTML format√© pour le diagnostic.
    
    Args:
        diagnostic_text (str): Le texte du diagnostic
        culture (str): La culture sp√©cifi√©e
        image_info (dict): Informations sur l'image
        timestamp (str): Horodatage de l'analyse
        
    Returns:
        str: Contenu HTML format√©
    """
    if timestamp is None:
        timestamp = datetime.now().strftime("%d/%m/%Y √† %H:%M")
    
    html_content = f"""
<!DOCTYPE html>
<html lang="{'fr' if st.session_state.language == 'fr' else 'en'}">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AgriLens AI - Diagnostic</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            min-height: 100vh;
        }}
        .container {{
            max-width: 800px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
        }}
        .header {{
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 3px solid #4CAF50;
        }}
        .logo {{
            font-size: 2.5em;
            color: #4CAF50;
            margin-bottom: 10px;
        }}
        .title {{
            color: #333;
            font-size: 1.8em;
            margin-bottom: 5px;
        }}
        .subtitle {{
            color: #666;
            font-size: 1.1em;
        }}
        .info-section {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 25px;
            border-left: 4px solid #4CAF50;
        }}
        .info-title {{
            font-weight: bold;
            color: #4CAF50;
            margin-bottom: 10px;
        }}
        .diagnostic-content {{
            background: #fff;
            padding: 25px;
            border-radius: 10px;
            border: 1px solid #e0e0e0;
            margin-bottom: 20px;
        }}
        .footer {{
            text-align: center;
            margin-top: 30px;
            padding-top: 20px;
            border-top: 1px solid #e0e0e0;
            color: #666;
            font-size: 0.9em;
        }}
        .highlight {{
            background: #fff3cd;
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #ffc107;
            margin: 15px 0;
        }}
        .warning {{
            background: #f8d7da;
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #dc3545;
            margin: 15px 0;
        }}
        .success {{
            background: #d4edda;
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #28a745;
            margin: 15px 0;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <div class="logo">üå±</div>
            <h1 class="title">AgriLens AI</h1>
            <p class="subtitle">Diagnostic Intelligent des Plantes</p>
        </div>
        
        <div class="info-section">
            <div class="info-title">üìã Informations de l'Analyse</div>
            <p><strong>Date et heure :</strong> {timestamp}</p>
            <p><strong>Mod√®le utilis√© :</strong> Gemma 3n E4B IT</p>
            {f'<p><strong>Culture analys√©e :</strong> {culture}</p>' if culture else ''}
            {f'<p><strong>Format image :</strong> {image_info.get("format", "N/A")}</p>' if image_info else ''}
            {f'<p><strong>Taille image :</strong> {image_info.get("size", "N/A")}</p>' if image_info else ''}
        </div>
        
        <div class="diagnostic-content">
            <h2>üî¨ R√©sultats du Diagnostic</h2>
            {diagnostic_text.replace('##', '<h3>').replace('**', '<strong>').replace('*', '<em>')}
        </div>
        
        <div class="highlight">
            <strong>‚ö†Ô∏è Avertissement :</strong> Ce diagnostic est fourni √† titre indicatif. 
            Pour des cas critiques, consultez un expert agricole qualifi√©.
        </div>
        
        <div class="footer">
            <p>G√©n√©r√© par AgriLens AI - Cr√©√© par Sidoine Kolaol√© YEBADOKPO</p>
            <p>Bohicon, R√©publique du B√©nin | syebadokpo@gmail.com</p>
        </div>
    </div>
</body>
</html>
"""
    return html_content

def generate_text_diagnostic(diagnostic_text, culture=None, image_info=None, timestamp=None):
    """
    G√©n√®re un fichier texte format√© pour le diagnostic.
    
    Args:
        diagnostic_text (str): Le texte du diagnostic
        culture (str): La culture sp√©cifi√©e
        image_info (dict): Informations sur l'image
        timestamp (str): Horodatage de l'analyse
        
    Returns:
        str: Contenu texte format√©
    """
    if timestamp is None:
        timestamp = datetime.now().strftime("%d/%m/%Y √† %H:%M")
    
    text_content = f"""
================================================================================
AGRILENS AI - DIAGNOSTIC INTELLIGENT DES PLANTES
================================================================================

üìã INFORMATIONS DE L'ANALYSE
---------------------------
Date et heure : {timestamp}
Mod√®le utilis√© : Gemma 3n E4B IT
{f'Culture analys√©e : {culture}' if culture else ''}
{f'Format image : {image_info.get("format", "N/A")}' if image_info else ''}
{f'Taille image : {image_info.get("size", "N/A")}' if image_info else ''}

üî¨ R√âSULTATS DU DIAGNOSTIC
-------------------------
{diagnostic_text}

‚ö†Ô∏è AVERTISSEMENT
---------------
Ce diagnostic est fourni √† titre indicatif. Pour des cas critiques, 
consultez un expert agricole qualifi√©.

================================================================================
G√©n√©r√© par AgriLens AI
Cr√©√© par Sidoine Kolaol√© YEBADOKPO
Bohicon, R√©publique du B√©nin
Email : syebadokpo@gmail.com
================================================================================
"""
    return text_content

# --- Fonctions d'Analyse avec Gemma 3n E4B IT ---
MODEL_ID_HF = "google/gemma-3n-E4B-it"
LOCAL_MODEL_PATH = "D:/Dev/model_gemma" # Chemin vers votre mod√®le local (ajustez si n√©cessaire)

def load_model_strategy(model_identifier, device_map=None, torch_dtype=None, quantization=None, force_persistence=False):
    """
    Charge un mod√®le et son processeur en utilisant des param√®tres sp√©cifiques.
    Retourne le mod√®le et le processeur, ou (None, None) en cas d'√©chec.
    """
    try:
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        common_args = {
            "trust_remote_code": True,
            "low_cpu_mem_usage": True,
            "device_map": device_map,
            "torch_dtype": torch_dtype,
            "token": os.environ.get("HF_TOKEN")
        }
        
        if quantization == "4bit":
            common_args.update({
                "load_in_4bit": True,
                "bnb_4bit_compute_dtype": torch.float16
            })
        elif quantization == "8bit":
            common_args.update({"load_in_8bit": True})
        
        processor = AutoProcessor.from_pretrained(model_identifier, trust_remote_code=True, token=os.environ.get("HF_TOKEN"))
        model = Gemma3nForConditionalGeneration.from_pretrained(model_identifier, **common_args)
        
        afficher_ram_disponible("apr√®s chargement")
        
        st.session_state.model = model
        st.session_state.processor = processor
        st.session_state.model_loaded = True
        st.session_state.model_status = f"Charg√© ({device_map or 'auto'})"
        st.session_state.model_load_time = time.time()
        
        if force_persistence:
            if force_model_persistence():
                st.success("Mod√®le charg√© et persistant.")
            else:
                st.warning("Mod√®le charg√© mais probl√®me de persistance.")
        
        return model, processor

    except ImportError as e:
        raise ImportError(f"Biblioth√®que manquante : . Installez-la avec `pip install transformers torch accelerate bitsandbytes`")
    except Exception as e:
        raise Exception(f"√âchec du chargement avec la strat√©gie : {e}")

def load_model():
    """Charge le mod√®le avec une strat√©gie adaptative bas√©e sur l'environnement."""
    try:
        st.info("üîç D√©but du processus de chargement du mod√®le...")
        
        # D√©tecter l'environnement Hugging Face Spaces
        is_hf_spaces = os.environ.get('SPACE_ID') is not None
        st.info(f"üåç Environnement d√©tect√© : {'Hugging Face Spaces' if is_hf_spaces else 'Local'}")
        
        if is_hf_spaces:
            st.info("üåê Environnement Hugging Face Spaces d√©tect√© - Utilisation de la strat√©gie optimis√©e")
            result = load_ultra_lightweight_for_hf_spaces()
            st.info(f"üìä R√©sultat du chargement HF Spaces : {result[0] is not None and result[1] is not None}")
            return result
        else:
            st.info("üíª Environnement local d√©tect√© - Chargement du mod√®le Gemma 3n complet")
            result = load_gemma_full()
            st.info(f"üìä R√©sultat du chargement local : {result[0] is not None and result[1] is not None}")
            return result
            
    except Exception as e:
        st.error(f"‚ùå Erreur lors du chargement du mod√®le : {str(e)}")
        st.error(f"üîç Type d'erreur : {type(e).__name__}")
        return None, None

def load_ultra_lightweight_for_hf_spaces():
    """Charge un mod√®le l√©ger pour Hugging Face Spaces (16GB RAM limit)"""
    try:
        st.info("üîÑ D√©but du chargement du mod√®le Gemma 3B IT pour Hugging Face Spaces...")
        
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        # Charger Gemma 3B IT (plus l√©ger que Gemma 3n E4B IT)
        model_id = "google/gemma-3b-it"
        st.info(f"üì¶ Mod√®le cible : {model_id}")
        
        st.info("üîß Configuration ultra-l√©g√®re en cours...")
        
        # Configuration ultra-l√©g√®re
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            device_map="cpu",
            torch_dtype=torch.float32,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        st.info("‚úÖ Mod√®le charg√©, chargement du tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        
        if model and tokenizer:
            st.success("‚úÖ Mod√®le Gemma 3B IT charg√© avec succ√®s pour Hugging Face Spaces")
            st.info(f"üìä Mod√®le type : {type(model).__name__}")
            st.info(f"üìä Tokenizer type : {type(tokenizer).__name__}")
            return model, tokenizer
        else:
            st.error("‚ùå √âchec du chargement du mod√®le l√©ger")
            return None, None
            
    except Exception as e:
        st.error(f"‚ùå Erreur lors du chargement du mod√®le l√©ger : {str(e)}")
        st.error(f"üîç Type d'erreur : {type(e).__name__}")
        return None, None

def load_gemma_full():
    """Charge le mod√®le Gemma 3n E4B IT complet pour les environnements locaux"""
    try:
        st.info("üîÑ Chargement du mod√®le Gemma 3n E4B IT complet...")
        
        # Configuration pour environnement local avec plus de ressources
        model = Gemma3nForConditionalGeneration.from_pretrained(
            MODEL_ID_HF,
            device_map="auto" if torch.cuda.is_available() else "cpu",
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        processor = AutoProcessor.from_pretrained(MODEL_ID_HF)
        
        if model and processor:
            st.success("‚úÖ Mod√®le Gemma 3n E4B IT charg√© avec succ√®s")
            return model, processor
        else:
            st.error("‚ùå √âchec du chargement du mod√®le complet")
            return None, None
            
    except Exception as e:
        st.error(f"‚ùå Erreur lors du chargement du mod√®le complet : {str(e)}")
        return None, None

def analyze_image_multilingual(image, prompt=""):
    """
    Analyse une image avec le mod√®le charg√© (Gemma 3B IT ou Gemma 3n) pour un diagnostic pr√©cis.
    """
    if not st.session_state.model_loaded:
        if not restore_model_from_cache():
            st.warning("Mod√®le non charg√©. Veuillez le charger via les r√©glages avant d'analyser.")
            return "‚ùå Mod√®le Gemma non charg√©. Veuillez d'abord charger le mod√®le dans les r√©glages."
        else:
            st.info("Mod√®le restaur√© depuis le cache pour l'analyse.")

    model, processor = st.session_state.model, st.session_state.processor
    if not model or not processor:
        return "‚ùå Mod√®le Gemma non disponible. Veuillez recharger le mod√®le."

    try:
        # D√©tecter le type de mod√®le charg√©
        is_gemma3b = "gemma-3b" in str(type(model)).lower()
        
        if is_gemma3b:
            # Utiliser la logique pour Gemma 3B IT (mod√®le l√©ger)
            return analyze_with_gemma3b_and_gemini(image, prompt)
        else:
            # Utiliser la logique pour Gemma 3n (mod√®le complet)
            return analyze_with_gemma3n(image, prompt)
            
    except Exception as e:
        error_msg = str(e)
        if "403" in error_msg or "Forbidden" in error_msg:
            return "‚ùå Erreur d'acc√®s Hugging Face (403). V√©rifiez votre HF_TOKEN."
        elif "out of memory" in error_msg.lower():
            return "‚ùå Erreur de m√©moire insuffisante. Essayez de recharger le mod√®le."
        else:
            return f"‚ùå Erreur lors de l'analyse : {error_msg}"

def analyze_with_gemma3b_and_gemini(image, prompt=""):
    """Analyse avec Gemma 3B IT + Gemini pour Hugging Face Spaces"""
    try:
        model, tokenizer = st.session_state.model, st.session_state.processor
        
        # Pr√©parer le prompt pour Gemma 3B IT
        if st.session_state.language == "fr":
            if "Culture sp√©cifi√©e :" in prompt:
                user_prompt = f"Analyse cette image de plante en te concentrant sur la culture mentionn√©e. {prompt} Fournis un diagnostic pr√©cis."
            else:
                user_prompt = f"Analyse cette image de plante et fournis un diagnostic pr√©cis. Question : {prompt}" if prompt else "Analyse cette image de plante et fournis un diagnostic pr√©cis."
        else:
            if "Culture sp√©cifi√©e :" in prompt:
                user_prompt = f"Analyze this plant image focusing on the mentioned crop. {prompt} Provide a precise diagnosis."
            else:
                user_prompt = f"Analyze this plant image and provide a precise diagnosis. Question: {prompt}" if prompt else "Analyze this plant image and provide a precise diagnosis."
        
        # Cr√©er le prompt avec l'image
        final_prompt = f"<image>\n{user_prompt}"
        
        # Pr√©parer les entr√©es
        inputs = tokenizer(text=final_prompt, images=image, return_tensors="pt")
        input_len = inputs["input_ids"].shape[-1]
        
        # G√©n√©rer la r√©ponse
        with torch.inference_mode():
            generation = model.generate(
                **inputs,
                max_new_tokens=500,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )
            response = tokenizer.decode(generation[0][input_len:], skip_special_tokens=True)
        
        final_response = response.strip()
        
        # Am√©liorer avec Gemini si disponible
        if GEMINI_AVAILABLE and os.environ.get('GOOGLE_API_KEY'):
            try:
                genai.configure(api_key=os.environ.get('GOOGLE_API_KEY'))
                model_gemini = genai.GenerativeModel('gemini-1.5-flash')
                
                gemini_prompt = f"""
                Tu es un expert en pathologie v√©g√©tale. Am√©liore ce diagnostic de maladie de plante :
                
                {final_response}
                
                Fournis une version am√©lior√©e avec :
                1. Diagnostic pr√©cis
                2. Causes possibles
                3. Sympt√¥mes d√©taill√©s
                4. Traitements recommand√©s
                5. Niveau d'urgence
                
                R√©ponds en {st.session_state.language}.
                """
                
                gemini_response = model_gemini.generate_content(gemini_prompt)
                final_response = gemini_response.text
            except Exception as e:
                st.warning(f"Gemini non disponible : {str(e)}")
                pass  # Continuer sans Gemini si erreur
        
        if st.session_state.language == "fr":
            return f"""
## üß† **Analyse par Gemma 3B IT + Gemini**

{final_response}
"""
        else:
            return f"""
## üß† **Analysis by Gemma 3B IT + Gemini**

{final_response}
"""
            
    except Exception as e:
        return f"‚ùå Erreur lors de l'analyse avec Gemma 3B IT : {str(e)}"

def analyze_with_gemma3n(image, prompt=""):
    """Analyse avec Gemma 3n E4B IT (mod√®le complet)"""
    try:
        model, processor = st.session_state.model, st.session_state.processor
        
        # Pr√©parer le prompt pour Gemma 3n
        if st.session_state.language == "fr":
            if "Culture sp√©cifi√©e :" in prompt:
                user_instruction = f"Analyse cette image de plante en te concentrant sp√©cifiquement sur la culture mentionn√©e. {prompt} Fournis un diagnostic pr√©cis et adapt√© √† cette culture."
            else:
                user_instruction = f"Analyse cette image de plante et fournis un diagnostic pr√©cis. Question sp√©cifique : {prompt}" if prompt else "Analyse cette image de plante et fournis un diagnostic pr√©cis."
            system_message = "Tu es un expert en pathologie v√©g√©tale sp√©cialis√© dans le diagnostic des maladies de plantes. R√©ponds de mani√®re structur√©e et pr√©cise, en incluant diagnostic, causes, sympt√¥mes, traitement et urgence. Si une culture sp√©cifique est mentionn√©e, concentre-toi sur les maladies typiques de cette culture."
        else:
            if "Culture sp√©cifi√©e :" in prompt:
                user_instruction = f"Analyze this plant image focusing specifically on the mentioned crop. {prompt} Provide a precise diagnosis adapted to this crop."
            else:
                user_instruction = f"Analyze this plant image and provide a precise diagnosis. Specific question: {prompt}" if prompt else "Analyze this plant image and provide a precise diagnosis."
            system_message = "You are an expert in plant pathology specialized in plant disease diagnosis. Respond in a structured and precise manner, including diagnosis, causes, symptoms, treatment, and urgency. If a specific crop is mentioned, focus on diseases typical of that crop."
        
        messages = [
            {"role": "system", "content": [{"type": "text", "text": system_message}]},
            {"role": "user", "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": user_instruction}
            ]}
        ]
        
        # Utiliser processor.apply_chat_template
        inputs = processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
        )
        
        device = getattr(model, 'device', 'cpu')
        if hasattr(inputs, 'to'):
            inputs = inputs.to(device)
        
        input_len = inputs["input_ids"].shape[-1]
        
        with torch.inference_mode():
            generation = model.generate(
                input_ids=inputs["input_ids"].to(device),
                attention_mask=inputs["attention_mask"].to(device),
                pixel_values=inputs["pixel_values"].to(device) if "pixel_values" in inputs else None,
                max_new_tokens=500,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )
            response = processor.decode(generation[0][input_len:], skip_special_tokens=True)

        final_response = response.strip()
        final_response = final_response.replace("<start_of_turn>", "").replace("<end_of_turn>", "").strip()
        
        if st.session_state.language == "fr":
            return f"""
## üß† **Analyse par Gemma 3n E4B IT**

{final_response}
"""
        else:
            return f"""
## üß† **Analysis by Gemma 3n E4B IT**

{final_response}
"""
            
    except Exception as e:
        error_msg = str(e)
        if "403" in error_msg or "Forbidden" in error_msg:
            return "‚ùå Erreur 403 - Acc√®s refus√©. Veuillez v√©rifier votre jeton Hugging Face (HF_TOKEN) et les quotas."
        elif "Number of images does not match number of special image tokens" in error_msg:
            return f"‚ùå Erreur : Le mod√®le n'a pas pu lier l'image au texte. Ceci est un probl√®me connu (#2751) li√© aux versions de Transformers/Gemma. Assurez-vous d'utiliser les versions sp√©cifi√©es dans requirements.txt."
        else:
            return f"‚ùå Erreur lors de l'analyse avec Gemma 3n : {str(e)}"

def analyze_text_multilingual(text):
    """Analyse un texte avec le mod√®le Gemma 3n E4B IT."""
    if not st.session_state.model_loaded:
        if not restore_model_from_cache():
            st.warning("Mod√®le non charg√©. Veuillez le charger via les r√©glages avant d'analyser.")
            return "‚ùå Mod√®le non charg√©. Veuillez le charger dans les r√©glages."
        else:
            st.info("Mod√®le restaur√© depuis le cache pour l'analyse.")

    model, processor = st.session_state.model, st.session_state.processor
    if not model or not processor:
        return "‚ùå Mod√®le Gemma non disponible. Veuillez recharger le mod√®le."
    
    try:
        if st.session_state.language == "fr":
            prompt_template = f"Tu es un assistant agricole expert. Analyse ce probl√®me de plante : \n\n**Description du probl√®me :**\n{text}\n\n**Instructions :**\n1. **Diagnostic** : Quel est le probl√®me principal ?\n2. **Causes** : Quelles sont les causes possibles ?\n3. **Traitement** : Quelles sont les actions √† entreprendre ?\n4. **Pr√©vention** : Comment √©viter le probl√®me √† l'avenir ?"
        else:
            prompt_template = f"You are an expert agricultural assistant. Analyze this plant problem: \n\n**Problem Description:**\n{text}\n\n**Instructions:**\n1. **Diagnosis**: What is the main problem?\n2. **Causes**: What are the possible causes?\n3. **Treatment**: What actions should be taken?\n4. **Prevention**: How to avoid the problem in the future?"
        
        messages = [{"role": "user", "content": [{"type": "text", "text": prompt_template}]}]
        
        inputs = processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
        )
        
        device = getattr(model, 'device', 'cpu')
        if hasattr(inputs, 'to'):
            inputs = inputs.to(device)
        
        input_len = inputs["input_ids"].shape[-1]
        
        with torch.inference_mode():
            generation = model.generate(
                input_ids=inputs["input_ids"].to(device), # Passer explicitement input_ids
                attention_mask=inputs["attention_mask"].to(device), # Passer explicitement attention_mask
                max_new_tokens=500,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )
            response = processor.decode(generation[0][input_len:], skip_special_tokens=True)
        
        cleaned_response = response.strip()
        cleaned_response = cleaned_response.replace("<start_of_turn>", "").replace("<end_of_turn>", "").strip()

        return cleaned_response
        
    except Exception as e:
        return f"‚ùå Erreur lors de l'analyse de texte : {e}"

# --- Interface Principale ---

# Boutons de contr√¥le
col1, col2, col3, col4 = st.columns([1, 1, 1, 1])
with col1:
    if st.button(t("toggle_mode"), type="secondary"):
        toggle_mobile_mode()
        st.rerun()
with col2:
    if st.button(t("settings_button"), type="secondary"):
        # Bascule la sidebar
        st.sidebar_state = not st.sidebar_state if hasattr(st, 'sidebar_state') else True
        st.rerun()

# Affichage conditionnel selon le mode
if is_mobile():
    # Mode Mobile
    st.markdown('<div class="mobile-container">', unsafe_allow_html=True)
    
    # Header mobile
    st.markdown(f'''
    <div class="mobile-header">
        <h1>{t("title")}</h1>
        <div class="mobile-status">{t("offline_status")} ‚úÖ</div>
    </div>
    ''', unsafe_allow_html=True)
    
    st.markdown(f'<p style="text-align: center; color: #666;">{t("mobile_desc")}</p>', unsafe_allow_html=True)
    
else:
    # Mode Desktop
    st.markdown('<div class="desktop-container">', unsafe_allow_html=True)
    st.title(t("title"))
    st.markdown(t("subtitle"))

# Initialisation des variables de session
if 'model_loaded' not in st.session_state:
    st.session_state.model_loaded = False
if 'model_status' not in st.session_state:
    st.session_state.model_status = "Non charg√©"
if 'model' not in st.session_state:
    st.session_state.model = None
if 'processor' not in st.session_state:
    st.session_state.processor = None
if 'global_model_cache' not in st.session_state:
    st.session_state.global_model_cache = {}
if 'model_persistence_check' not in st.session_state:
    st.session_state.model_persistence_check = False
if 'model_load_time' not in st.session_state:
    st.session_state.model_load_time = None

if not st.session_state.model_loaded:
    if restore_model_from_cache():
        st.success("üîÑ Mod√®le restaur√© automatiquement depuis le cache au d√©marrage.")
    else:
        st.info("üí° Cliquez sur 'Charger le mod√®le' dans les r√©glages pour commencer.")

with st.sidebar:
    st.header("‚öôÔ∏è " + t("config_title"))
    st.info("üìã Panneau des r√©glages ouvert")
    
    st.subheader("üåê " + t("language_selection").split(":")[0])
    language_options = ["Fran√ßais", "English"]
    if 'language' not in st.session_state:
        st.session_state.language = 'fr'
    current_lang_index = 0 if st.session_state.language == "fr" else 1
    
    language_choice = st.selectbox(
        t("language_selection"),
        language_options,
        index=current_lang_index,
        help=t("language_help")
    )
    if st.session_state.language != ("fr" if language_choice == "Fran√ßais" else "en"):
        st.session_state.language = "fr" if language_choice == "Fran√ßais" else "en"
        st.rerun()

    st.divider()

    st.subheader(t("hf_token_title"))
    hf_token_found = HfFolder.get_token() or os.environ.get("HF_TOKEN")
    if hf_token_found:
        st.success(t("hf_token_found"))
    else:
        st.warning(t("hf_token_not_found"))
    st.info(t("hf_token_info"))
    st.markdown(f"[{t('get_hf_token')}](https://huggingface.co/settings/tokens)")

    st.divider()

    st.header(t("model_title"))
    if st.session_state.model_loaded and check_model_persistence():
        st.success(f"{t('model_loaded')} ({st.session_state.model_status})")
        if st.session_state.model_load_time:
            load_time_str = time.strftime('%H:%M:%S', time.localtime(st.session_state.model_load_time))
            st.write(f"{t('load_time')}{load_time_str}")
        if hasattr(st.session_state.model, 'device'):
            st.write(f"{t('device_used')}`{st.session_state.model.device}`")
        
        col1_btn, col2_btn = st.columns(2)
        with col1_btn:
            if st.button(t("reload_model"), type="secondary"):
                st.session_state.model_loaded = False
                st.session_state.model = None
                st.session_state.processor = None
                st.session_state.global_model_cache.clear()
                st.session_state.model_persistence_check = False
                st.rerun()
        with col2_btn:
            if st.button(t("force_persistence"), type="secondary"):
                if force_model_persistence():
                    st.success(t("persistence_success"))
                else:
                    st.error(t("persistence_failed"))
                st.rerun()
    else:
        st.warning(t("model_not_loaded"))
        if st.button(t("load_model"), type="primary"):
            try:
                with st.spinner(t("loading_model")):
                    model, processor = load_model()
                    if model and processor:
                        # Mettre √† jour les variables de session
                        st.session_state.model = model
                        st.session_state.processor = processor
                        st.session_state.model_loaded = True
                        st.session_state.model_status = "Charg√© avec succ√®s"
                        st.session_state.model_load_time = time.time()
                        st.session_state.model_persistence_check = True
                        st.success(t("model_loaded_success"))
                    else:
                        st.error(t("model_load_failed"))
                        st.session_state.model_loaded = False
                        st.session_state.model_status = "√âchec du chargement"
            except Exception as e:
                st.error(f"‚ùå Erreur lors du chargement : {str(e)}")
                st.session_state.model_loaded = False
                st.session_state.model_status = f"Erreur : {str(e)}"
            st.rerun()

    st.divider()
    st.subheader(t("persistence_title"))
    if st.session_state.model_loaded and st.session_state.model_persistence_check:
        st.success(t("persistence_loaded"))
    elif st.session_state.model_loaded:
        st.warning(t("persistence_warning"))
    else:
        st.warning(t("persistence_not_loaded"))


# --- Onglets Principaux ---
tab1, tab2, tab3, tab4 = st.tabs(t("tabs"))

with tab1:
    st.header(t("image_analysis_title"))
    st.markdown(t("image_analysis_desc"))
    
    capture_option = st.radio(
        t("choose_method"),
        [t("upload_image"), t("webcam_capture")],
        horizontal=True,
        key="image_capture_method"
    )
    
    uploaded_file = None
    captured_image = None
    
    if capture_option == t("upload_image"):
        uploaded_file = st.file_uploader(
            t("choose_image"),
            type=['png', 'jpg', 'jpeg'],
            help="Formats accept√©s : PNG, JPG, JPEG (max 200MB). Privil√©giez des images claires.",
            accept_multiple_files=False,
            key="image_uploader"
        )
        if uploaded_file is not None:
            MAX_FILE_SIZE_BYTES = 200 * 1024 * 1024
            if uploaded_file.size > MAX_FILE_SIZE_BYTES:
                st.error(t("file_too_large_error"))
                uploaded_file = None
            elif uploaded_file.size == 0:
                st.error(t("empty_file_error"))
                uploaded_file = None
            elif uploaded_file.size > (MAX_FILE_SIZE_BYTES * 0.8):
                st.warning(t("file_size_warning"))
    else:
        st.markdown(t("webcam_title"))
        st.info(t("webcam_info"))
        captured_image = st.camera_input(t("take_photo"), key="webcam_capture")
    
    image = None
    image_source = None
    
    if uploaded_file is not None:
        try:
            image = Image.open(uploaded_file)
            image_source = "upload"
        except Exception as e:
            st.error(t("image_processing_error"))
            st.info(t("try_different_image"))
    elif captured_image is not None:
        try:
            image = Image.open(captured_image)
            image_source = "webcam"
        except Exception as e:
            st.error(t("image_processing_error_webcam"))
            st.info(t("try_retake_photo"))
    
    if image is not None:
        try:
            original_size = image.size
            image, was_resized = resize_image_if_needed(image, max_size=(800, 800))
            
            col1, col2 = st.columns([1, 1])
            with col1:
                st.image(image, caption=f"Image ()" if image_source else "Image", use_container_width=True)
                if was_resized:
                    st.warning(t("image_resized_warning").format(new_size=image.size))
            
            with col2:
                st.markdown(t("image_info_title"))
                st.write(f"{t('format_label')}{image.format}")
                st.write(f"{t('original_size')}{original_size[0]}x{original_size[1]}{t('pixels')}")
                st.write(f"{t('current_size')}{image.size[0]}x{image.size[1]}{t('pixels')}")
                st.write(f"{t('mode_label')}{image.mode}")
            
            # Section de clarification de la culture
            st.markdown("---")
            st.subheader(t("culture_clarification"))
            
            culture_input = st.text_input(
                t("culture_question"),
                placeholder=t("culture_placeholder"),
                help=t("culture_help")
            )
            
            question = st.text_area(
                t("specific_question"),
                placeholder=t("question_placeholder"),
                height=100
            )
            
            if st.button(t("analyze_button"), disabled=not st.session_state.model_loaded, type="primary"):
                if not st.session_state.model_loaded:
                    st.error(t("model_not_loaded_error"))
                else:
                    with st.spinner(t("analyzing_image")):
                        # Construire le prompt avec la culture sp√©cifi√©e
                        enhanced_prompt = ""
                        if culture_input:
                            enhanced_prompt += f"Culture sp√©cifi√©e : {culture_input}. "
                        if question:
                            enhanced_prompt += f"Question : {question}. "
                        
                        result = analyze_image_multilingual(image, enhanced_prompt)
                    
                    st.markdown(t("analysis_results"))
                    
                    # Afficher la culture sp√©cifi√©e si elle existe
                    if culture_input:
                        st.info(f"üå± {t('culture_specified')} **{culture_input}**")
                    
                    st.markdown("---")
                    st.markdown(result)
                    
                    # Section d'export du diagnostic
                    st.markdown("---")
                    st.subheader(t("export_diagnostic"))
                    
                    # Pr√©parer les informations pour l'export
                    timestamp = datetime.now().strftime("%d/%m/%Y √† %H:%M")
                    image_info = {
                        "format": image.format if hasattr(image, 'format') else "N/A",
                        "size": f"{image.size[0]}x{image.size[1]} pixels" if hasattr(image, 'size') else "N/A"
                    } if image else None
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # Export HTML
                        html_content = generate_html_diagnostic(result, culture_input, image_info, timestamp)
                        st.download_button(
                            label=t("download_html"),
                            data=html_content,
                            file_name=t("html_filename").format(date=datetime.now().strftime("%Y%m%d_%H%M")),
                            mime="text/html",
                            help=t("export_html")
                        )
                    
                    with col2:
                        # Export Texte
                        text_content = generate_text_diagnostic(result, culture_input, image_info, timestamp)
                        st.download_button(
                            label=t("download_text"),
                            data=text_content,
                            file_name=t("text_filename").format(date=datetime.now().strftime("%Y%m%d_%H%M")),
                            mime="text/plain",
                            help=t("export_text")
                        )
        except Exception as e:
            st.error(t("image_processing_general_error"))

with tab2:
    st.header(t("text_analysis_title"))
    st.markdown(t("text_analysis_desc"))
    
    text_input = st.text_area(
        t("symptoms_desc"),
        placeholder=t("symptoms_placeholder"),
        height=150
    )
    
    if st.button("üß† Analyser avec l'IA", disabled=not st.session_state.model_loaded, type="primary"):
        if not st.session_state.model_loaded:
            st.error("‚ùå Mod√®le non charg√©. Veuillez le charger dans les r√©glages.")
        elif not text_input.strip():
            st.error("‚ùå Veuillez saisir une description des sympt√¥mes.")
        else:
            with st.spinner("üîç Analyse de texte en cours..."):
                result = analyze_text_multilingual(text_input)
            
            st.markdown(t("analysis_results"))
            st.markdown("---")
            st.markdown(result)
            
            # Section d'export du diagnostic (analyse de texte)
            st.markdown("---")
            st.subheader(t("export_diagnostic"))
            
            # Pr√©parer les informations pour l'export
            timestamp = datetime.now().strftime("%d/%m/%Y √† %H:%M")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Export HTML
                html_content = generate_html_diagnostic(result, None, None, timestamp)
                st.download_button(
                    label=t("download_html"),
                    data=html_content,
                    file_name=t("html_filename").format(date=datetime.now().strftime("%Y%m%d_%H%M")),
                    mime="text/html",
                    help=t("export_html")
                )
            
            with col2:
                # Export Texte
                text_content = generate_text_diagnostic(result, None, None, timestamp)
                st.download_button(
                    label=t("download_text"),
                    data=text_content,
                    file_name=t("text_filename").format(date=datetime.now().strftime("%Y%m%d_%H%M")),
                    mime="text/plain",
                    help=t("export_text")
                )

with tab3:
    st.header(t("manual_title"))
    
    manual_content = {
        "fr": """
        ## üöÄ **GUIDE DE D√âMARRAGE RAPIDE**
        
        ### **√âtape 1 : Configuration Initiale**
        1. **Choisir la langue** : Dans la sidebar, s√©lectionnez Fran√ßais ou English
        2. **Charger le mod√®le** : Cliquez sur 'Charger le mod√®le Gemma 3n E4B IT'
        3. **Attendre le chargement** : Le processus peut prendre 1-2 minutes
        4. **V√©rifier le statut** : Le mod√®le doit afficher "‚úÖ Charg√©"

        ### **√âtape 2 : Premi√®re Analyse**
        1. **Aller dans l'onglet "üì∏ Analyse d'Image"**
        2. **T√©l√©charger une photo** de plante malade
        3. **Cliquer sur "üî¨ Analyser avec l'IA"**
        4. **Consulter les r√©sultats** avec recommandations

        ## üì± **UTILISATION DU MODE MOBILE**
        
        ### **Activation du Mode Mobile**
        - Cliquer sur le bouton "üîÑ Changer de mode" en haut de l'interface
        - L'interface se transforme en simulation d'application mobile
        - Statut "Mode: OFFLINE" visible pour les d√©monstrations

        ### **Avantages du Mode Mobile**
        - ‚úÖ **D√©monstration offline** : Parfait pour les pr√©sentations
        - ‚úÖ **Interface intuitive** : Similaire aux vraies applications mobiles
        - ‚úÖ **Accessibilit√©** : Fonctionne sur tous les appareils
        - ‚úÖ **Performance** : Optimis√© pour les ressources limit√©es

        ## üì∏ **ANALYSE D'IMAGES**

        ### **Types d'Images Accept√©es**
        - **Formats** : PNG, JPG, JPEG
        - **Taille maximale** : 200MB
        - **Qualit√© recommand√©e** : Images claires et bien √©clair√©es

        ### **Bonnes Pratiques pour les Photos**
        1. **√âclairage** : Utiliser la lumi√®re naturelle quand possible
        2. **Focus** : S'assurer que la zone malade est nette
        3. **Cadrage** : Inclure la plante enti√®re et les zones affect√©es
        4. **Angles multiples** : Prendre plusieurs photos sous diff√©rents angles

        ### **Processus d'Analyse**
        1. **T√©l√©chargement** : Glisser-d√©poser ou cliquer pour s√©lectionner
        2. **Pr√©paration** : L'image est automatiquement redimensionn√©e si n√©cessaire
        3. **Analyse IA** : Le mod√®le Gemma 3n analyse l'image
        4. **R√©sultats** : Diagnostic d√©taill√© avec recommandations

        ### **Interpr√©tation des R√©sultats**
        Les r√©sultats incluent :
        - üéØ **Diagnostic probable** : Nom de la maladie identifi√©e
        - üîç **Sympt√¥mes observ√©s** : Description d√©taill√©e des signes
        - üí° **Causes possibles** : Facteurs environnementaux ou pathog√®nes
        - üíä **Traitements recommand√©s** : Solutions pratiques
        - üõ°Ô∏è **Mesures pr√©ventives** : Conseils pour √©viter la r√©currence

        ## üí¨ **ANALYSE DE TEXTE**

        ### **Quand Utiliser l'Analyse de Texte**
        - Pas de photo disponible
        - Sympt√¥mes difficiles √† photographier
        - Besoin de conseils g√©n√©raux
        - V√©rification de diagnostic

        ### **Comment D√©crire les Sympt√¥mes**
        **Informations importantes √† inclure :**
        - üåø **Type de plante** : Nom de l'esp√®ce si connu
        - üé® **Couleur des feuilles** : Vert, jaune, brun, noir, etc.
        - üîç **Forme des taches** : Circulaires, irr√©guli√®res, lin√©aires
        - üìç **Localisation** : Feuilles, tiges, fruits, racines
        - ‚è∞ **√âvolution** : Depuis quand, progression rapide ou lente
        - üåç **Conditions** : Humidit√©, temp√©rature, saison

        ### **Exemple de Description Efficace**
        ```
        "Mes plants de tomates ont des taches brunes circulaires sur les feuilles inf√©rieures. 
        Les taches ont un contour jaune et apparaissent depuis une semaine. 
        Il a beaucoup plu r√©cemment et l'air est tr√®s humide. 
        Les taches s'√©tendent progressivement vers le haut de la plante."
        ```

        ## ‚öôÔ∏è **CONFIGURATION ET PARAM√àTRES**

        ### **Param√®tres de Langue**
        - **Fran√ßais** : Interface et r√©ponses en fran√ßais
        - **English** : Interface and responses in English
        - **Changement** : Via la sidebar, effet imm√©diat

        ### **Gestion du Mod√®le IA**
        - **Chargement** : Bouton "Charger le mod√®le" dans la sidebar
        - **Statut** : Indicateur visuel du statut du mod√®le
        - **Rechargement** : Option pour recharger le mod√®le si n√©cessaire
        - **Persistance** : Le mod√®le reste en m√©moire pour les analyses suivantes

        ### **Jeton Hugging Face (HF_TOKEN)**
        **Pourquoi l'utiliser ?**
        - √âvite les erreurs d'acc√®s (403)
        - Am√©liore la stabilit√© du t√©l√©chargement
        - Acc√®s prioritaire aux mod√®les

        **Comment l'obtenir :**
        1. Aller sur [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
        2. Cr√©er un nouveau jeton avec les permissions "read"
        3. Copier le jeton g√©n√©r√©
        4. D√©finir la variable d'environnement : `HF_TOKEN=votre_jeton`

        ## üéØ **CAS D'USAGE PRATIQUES**

        ### **Sc√©nario 1 : Diagnostic de Mildiou**
        1. **Sympt√¥mes** : Taches brunes sur feuilles de tomate
        2. **Photo** : Prendre une photo des feuilles affect√©es
        3. **Analyse** : L'IA identifie le mildiou pr√©coce
        4. **Traitement** : Recommandations de fongicides et mesures pr√©ventives

        ### **Sc√©nario 2 : Probl√®me de Nutrition**
        1. **Sympt√¥mes** : Feuilles jaunies, croissance ralentie
        2. **Description** : D√©crire les conditions de culture
        3. **Analyse** : L'IA sugg√®re une carence en azote
        4. **Solution** : Recommandations d'engrais et d'amendements

        ## üîß **D√âPANNAGE**

        ### **Probl√®mes Courants**

        #### **Le mod√®le ne se charge pas**
        **Solutions :**
        - V√©rifier la connexion internet
        - S'assurer d'avoir suffisamment de RAM (8GB minimum)
        - Red√©marrer l'application
        - V√©rifier le jeton HF_TOKEN

        #### **Erreur de m√©moire**
        **Solutions :**
        - Fermer d'autres applications
        - Red√©marrer l'ordinateur
        - Utiliser un mod√®le plus l√©ger
        - Lib√©rer de l'espace disque

        #### **Analyse trop lente**
        **Solutions :**
        - R√©duire la taille des images
        - Utiliser des images de meilleure qualit√©
        - V√©rifier la connexion internet
        - Patienter lors du premier chargement

        ### **Messages d'Erreur Courants**

        #### **"Erreur : Le fichier est trop volumineux"**
        - R√©duire la taille de l'image (maximum 200MB)
        - Utiliser un format de compression (JPG au lieu de PNG)

        #### **"Mod√®le non charg√©"**
        - Cliquer sur "Charger le mod√®le" dans la sidebar
        - Attendre la fin du chargement
        - V√©rifier les messages d'erreur

        ## üåç **UTILISATION EN ZONES RURALES**

        ### **Avantages pour les Agriculteurs**
        - **Accessibilit√©** : Fonctionne sans internet constant
        - **Simplicit√©** : Interface intuitive
        - **Rapidit√©** : Diagnostic en quelques secondes
        - **√âconomique** : Gratuit et sans abonnement

        ### **Recommandations d'Usage**
        1. **Formation** : Former les utilisateurs aux bonnes pratiques
        2. **Validation** : Confirmer les diagnostics critiques avec des experts
        3. **Documentation** : Garder des traces des analyses
        4. **Suivi** : Utiliser l'application pour le suivi des traitements

        ## ‚ö†Ô∏è **AVERTISSEMENTS IMPORTANTS**

        ### **Limitations de l'IA**
        - Les r√©sultats sont √† titre indicatif uniquement
        - L'IA peut faire des erreurs de diagnostic
        - Les conditions locales peuvent affecter les recommandations
        - L'√©volution des maladies peut √™tre impr√©visible

        ### **Responsabilit√©**
        - L'utilisateur reste responsable des d√©cisions prises
        - Consulter un expert pour les cas critiques
        - Suivre les consignes de s√©curit√© des produits
        - Adapter les traitements aux conditions locales

        ## üìû **SUPPORT ET CONTACT**

        ### **Informations de Contact**
        - **Cr√©ateur** : Sidoine Kolaol√© YEBADOKPO
        - **Localisation** : Bohicon, R√©publique du B√©nin
        - **T√©l√©phone** : +229 01 96 91 13 46
        - **Email** : syebadokpo@gmail.com
        - **LinkedIn** : linkedin.com/in/sidoineko
        - **Portfolio** : [Hugging Face Portfolio](https://huggingface.co/spaces/Sidoineko/portfolio)

        ### **Ressources Suppl√©mentaires**
        - **Documentation technique** : README.md du projet
        - **Code source** : Disponible sur GitHub
        - **D√©mo en ligne** : Hugging Face Spaces
        - **Version comp√©tition** : [Kaggle Notebook](https://www.kaggle.com/code/sidoineyebadokpo/agrilens-ai?scriptVersionId=253640926)

        ---

        *Manuel cr√©√© par Sidoine Kolaol√© YEBADOKPO - Version 2.0 - Juillet 2025*
        """,
        "en": """
        ## üöÄ **QUICK START GUIDE**
        
        ### **Step 1: Initial Configuration**
        1. **Choose language** : In the sidebar, select Fran√ßais or English
        2. **Load the model** : Click on 'Load Gemma 3n E4B IT Model'
        3. **Wait for loading** : The process may take 1-2 minutes
        4. **Check status** : The model should display "‚úÖ Loaded"

        ### **Step 2: First Analysis**
        1. **Go to the "üì∏ Image Analysis" tab**
        2. **Upload a photo** of a diseased plant
        3. **Click on "üî¨ Analyze with AI"**
        4. **Review the results** with recommendations

        ## üì± **MOBILE MODE USAGE**
        
        ### **Activating Mobile Mode**
        - Click the "üîÑ Toggle Mode" button at the top of the interface
        - The interface transforms into a mobile app simulation
        - "Mode: OFFLINE" status visible for demonstrations

        ### **Mobile Mode Advantages**
        - ‚úÖ **Offline demonstration** : Perfect for presentations
        - ‚úÖ **Intuitive interface** : Similar to real mobile applications
        - ‚úÖ **Accessibility** : Works on all devices
        - ‚úÖ **Performance** : Optimized for limited resources

        ## üì∏ **IMAGE ANALYSIS**

        ### **Accepted Image Types**
        - **Formats** : PNG, JPG, JPEG
        - **Maximum size** : 200MB
        - **Recommended quality** : Clear and well-lit images

        ### **Best Practices for Photos**
        1. **Lighting** : Use natural light when possible
        2. **Focus** : Ensure the diseased area is sharp
        3. **Framing** : Include the entire plant and affected areas
        4. **Multiple angles** : Take several photos from different angles

        ### **Analysis Process**
        1. **Upload** : Drag and drop or click to select
        2. **Preparation** : Image is automatically resized if necessary
        3. **AI Analysis** : The Gemma 3n model analyzes the image
        4. **Results** : Detailed diagnosis with recommendations

        ### **Interpreting Results**
        Results include:
        - üéØ **Probable diagnosis** : Name of the identified disease
        - üîç **Observed symptoms** : Detailed description of signs
        - üí° **Possible causes** : Environmental factors or pathogens
        - üíä **Recommended treatments** : Practical solutions
        - üõ°Ô∏è **Preventive measures** : Advice to prevent recurrence

        ## üí¨ **TEXT ANALYSIS**

        ### **When to Use Text Analysis**
        - No photo available
        - Symptoms difficult to photograph
        - Need for general advice
        - Diagnosis verification

        ### **How to Describe Symptoms**
        **Important information to include:**
        - üåø **Plant type** : Species name if known
        - üé® **Leaf color** : Green, yellow, brown, black, etc.
        - üîç **Spot shape** : Circular, irregular, linear
        - üìç **Location** : Leaves, stems, fruits, roots
        - ‚è∞ **Evolution** : Since when, rapid or slow progression
        - üåç **Conditions** : Humidity, temperature, season

        ### **Example of Effective Description**
        ```
        "My tomato plants have brown circular spots on the lower leaves. 
        The spots have a yellow border and appeared a week ago. 
        It has rained a lot recently and the air is very humid. 
        The spots are gradually spreading upward on the plant."
        ```

        ## ‚öôÔ∏è **CONFIGURATION AND PARAMETERS**

        ### **Language Settings**
        - **Fran√ßais** : Interface and responses in French
        - **English** : Interface and responses in English
        - **Change** : Via sidebar, immediate effect

        ### **AI Model Management**
        - **Loading** : "Load Model" button in sidebar
        - **Status** : Visual indicator of model status
        - **Reload** : Option to reload model if necessary
        - **Persistence** : Model remains in memory for subsequent analyses

        ### **Hugging Face Token (HF_TOKEN)**
        **Why use it?**
        - Avoids access errors (403)
        - Improves download stability
        - Priority access to models

        **How to get it:**
        1. Go to [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
        2. Create a new token with "read" permissions
        3. Copy the generated token
        4. Set environment variable: `HF_TOKEN=your_token`

        ## üéØ **PRACTICAL USE CASES**

        ### **Scenario 1: Blight Diagnosis**
        1. **Symptoms** : Brown spots on tomato leaves
        2. **Photo** : Take a photo of affected leaves
        3. **Analysis** : AI identifies early blight
        4. **Treatment** : Fungicide recommendations and preventive measures

        ### **Scenario 2: Nutrition Problem**
        1. **Symptoms** : Yellowed leaves, slowed growth
        2. **Description** : Describe growing conditions
        3. **Analysis** : AI suggests nitrogen deficiency
        4. **Solution** : Fertilizer and amendment recommendations

        ## üîß **TROUBLESHOOTING**

        ### **Common Problems**

        #### **Model won't load**
        **Solutions:**
        - Check internet connection
        - Ensure sufficient RAM (8GB minimum)
        - Restart the application
        - Verify HF_TOKEN

        #### **Memory error**
        **Solutions:**
        - Close other applications
        - Restart computer
        - Use a lighter model
        - Free up disk space

        #### **Analysis too slow**
        **Solutions:**
        - Reduce image size
        - Use better quality images
        - Check internet connection
        - Be patient during first loading

        ### **Common Error Messages**

        #### **"Error: File too large"**
        - Reduce image size (maximum 200MB)
        - Use compression format (JPG instead of PNG)

        #### **"Model not loaded"**
        - Click "Load Model" in sidebar
        - Wait for loading to complete
        - Check error messages

        ## üåç **RURAL AREA USAGE**

        ### **Advantages for Farmers**
        - **Accessibility** : Works without constant internet
        - **Simplicity** : Intuitive interface
        - **Speed** : Diagnosis in seconds
        - **Economic** : Free and no subscription

        ### **Usage Recommendations**
        1. **Training** : Train users in best practices
        2. **Validation** : Confirm critical diagnoses with experts
        3. **Documentation** : Keep records of analyses
        4. **Follow-up** : Use application for treatment monitoring

        ## ‚ö†Ô∏è **IMPORTANT WARNINGS**

        ### **AI Limitations**
        - Results are for guidance only
        - AI can make diagnostic errors
        - Local conditions can affect recommendations
        - Disease evolution can be unpredictable

        ### **Responsibility**
        - User remains responsible for decisions made
        - Consult expert for critical cases
        - Follow product safety instructions
        - Adapt treatments to local conditions

        ## üìû **SUPPORT AND CONTACT**

        ### **Contact Information**
        - **Creator** : Sidoine Kolaol√© YEBADOKPO
        - **Location** : Bohicon, Benin Republic
        - **Phone** : +229 01 96 91 13 46
        - **Email** : syebadokpo@gmail.com
        - **LinkedIn** : linkedin.com/in/sidoineko
        - **Portfolio** : [Hugging Face Portfolio](https://huggingface.co/spaces/Sidoineko/portfolio)

        ### **Additional Resources**
        - **Technical documentation** : Project README.md
        - **Source code** : Available on GitHub
        - **Online demo** : Hugging Face Spaces
        - **Competition version** : [Kaggle Notebook](https://www.kaggle.com/code/sidoineyebadokpo/agrilens-ai?scriptVersionId=253640926)

        ---

        *Manual created by Sidoine Kolaol√© YEBADOKPO - Version 2.0 - July 2025*
        """
    }
    st.markdown(manual_content[st.session_state.language])

with tab4:
    st.header(t("about_title"))
    
    st.markdown(t("mission_title"))
    st.markdown(t("mission_text"))
    
    st.markdown(t("features_title"))
    st.markdown(t("features_text"))
    
    st.markdown(t("technology_title"))
    
    is_local = os.path.exists(LOCAL_MODEL_PATH)
    
    if is_local:
        st.markdown(t("local_model_text").format(path=LOCAL_MODEL_PATH))
    else:
        st.markdown(t("online_model_text"))
    
    st.markdown(f"### {t('creator_title')}")
    st.markdown(f"{t('creator_name')}")
    st.markdown(f"üìç {t('creator_location')}")
    st.markdown(f"üìû {t('creator_phone')}")
    st.markdown(f"üìß {t('creator_email')}")
    st.markdown(f"üîó [{t('creator_linkedin')}](https://{t('creator_linkedin')})")
    st.markdown(f"üìÅ {t('creator_portfolio')}")
    
    st.markdown(f"### {t('competition_title')}")
    st.markdown(t("competition_text"))
    
    st.markdown(t("warning_title"))
    st.markdown(t("warning_text"))
    
    st.markdown(t("support_title"))
    st.markdown(t("support_text"))

# --- Pied de page ---
st.markdown("---")
st.markdown(t("footer"))

# Fermer les divs selon le mode
if is_mobile():
    st.markdown('</div>', unsafe_allow_html=True)  # Fermer mobile-container
else:
    st.markdown('</div>', unsafe_allow_html=True)  # Fermer desktop-container