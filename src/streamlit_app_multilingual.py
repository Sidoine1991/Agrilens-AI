# --- IMPORTS ---
import streamlit as st
import os
import io
from PIL import Image
import requests
import torch
import gc
import time
import sys
import psutil
import traceback

# --- CONFIGURATION DE LA PAGE ---
st.set_page_config(
    page_title="AgriLens AI - Analyse de Plantes",
    page_icon="üå±",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- INITIALISATION DES VARIABLES DE SESSION ---
# Ces variables permettent de maintenir l'√©tat de l'application entre les ex√©cutions.
if 'model_loaded' not in st.session_state:
    st.session_state.model_loaded = False
if 'model' not in st.session_state:
    st.session_state.model = None
if 'processor' not in st.session_state:
    st.session_state.processor = None
if 'model_status' not in st.session_state:
    st.session_state.model_status = "Non charg√©"
if 'model_load_time' not in st.session_state:
    st.session_state.model_load_time = None
if 'language' not in st.session_state:
    st.session_state.language = "fr"
if 'load_attempt_count' not in st.session_state:
    st.session_state.load_attempt_count = 0
if 'device' not in st.session_state:
    st.session_state.device = "cpu" # Valeur par d√©faut

# --- FONCTIONS D'AIDE SYST√àME ---

def check_model_health():
    """V√©rifie si le mod√®le et le processeur sont charg√©s et semblent op√©rationnels."""
    try:
        # S'assure que le mod√®le et le processeur existent et que le mod√®le a une propri√©t√© 'device'
        return (st.session_state.model is not None and
                st.session_state.processor is not None and
                hasattr(st.session_state.model, 'device'))
    except Exception:
        return False

def diagnose_loading_issues():
    """Diagnostique les probl√®mes potentiels avant le chargement du mod√®le (RAM, Disque, GPU)."""
    issues = []
    
    try:
        ram = psutil.virtual_memory()
        ram_gb = ram.total / (1024**3)
        if ram_gb < 8: # Seuil minimum recommand√©
            issues.append(f"‚ö†Ô∏è RAM faible: {ram_gb:.1f}GB (recommand√©: 8GB+)")
    except Exception:
        issues.append("‚ö†Ô∏è Impossible de v√©rifier la RAM.")
        
    try:
        disk_usage = psutil.disk_usage('/')
        disk_gb = disk_usage.free / (1024**3)
        if disk_gb < 10: # Seuil minimum recommand√©
            issues.append(f"‚ö†Ô∏è Espace disque faible: {disk_gb:.1f}GB libre sur '/'")
    except Exception:
        issues.append("‚ö†Ô∏è Impossible de v√©rifier l'espace disque.")
        
    if torch.cuda.is_available():
        try:
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            if gpu_memory < 6: # Seuil minimum recommand√© pour des mod√®les comme Gemma
                issues.append(f"‚ö†Ô∏è GPU m√©moire faible: {gpu_memory:.1f}GB (recommand√©: 6GB+)")
        except Exception:
            issues.append("‚ö†Ô∏è Erreur lors de la v√©rification de la m√©moire GPU.")
    else:
        issues.append("‚ÑπÔ∏è CUDA non disponible - Le mod√®le fonctionnera sur CPU (lentement)")
        
    return issues

def resize_image_if_needed(image, max_size=(1024, 1024)):
    """Redimensionne l'image si ses dimensions d√©passent max_size pour optimiser l'analyse."""
    if image.size[0] > max_size[0] or image.size[1] > max_size[1]:
        # Utilise LANCZOS pour un redimensionnement de haute qualit√©
        image.thumbnail(max_size, Image.Resampling.LANCZOS)
        return image, True
    return image, False

def afficher_ram_disponible(context=""):
    """Affiche l'utilisation de la RAM de mani√®re lisible dans l'interface Streamlit."""
    try:
        ram = psutil.virtual_memory()
        ram_used_gb = ram.used / (1024**3)
        ram_total_gb = ram.total / (1024**3)
        ram_percent = ram.percent
        st.write(f"üíæ RAM : {ram_used_gb:.1f}GB / {ram_total_gb:.1f}GB ({ram_percent:.1f}%)")
    except Exception:
        st.write("üíæ Impossible d'afficher l'utilisation de la RAM.")

# --- GESTION DES TRADUCTIONS ---

def t(key):
    """Fonction utilitaire pour g√©rer les traductions de textes dans l'application."""
    translations = {
        "fr": {
            "title": "üå± AgriLens AI - Assistant d'Analyse de Plantes",
            "subtitle": "Analysez vos plantes avec l'IA pour d√©tecter les maladies",
            "tabs": ["üì∏ Analyse d'Image", "üìù Analyse de Texte", "‚öôÔ∏è Configuration", "‚ÑπÔ∏è √Ä Propos"],
            "image_analysis_title": "üì∏ Analyse d'Image de Plante",
            "image_analysis_desc": "T√©l√©chargez ou capturez une image de votre plante pour obtenir un diagnostic.",
            "choose_image": "Choisissez une image de plante...",
            "text_analysis_title": "üìù Analyse de Description Textuelle",
            "text_analysis_desc": "D√©crivez les sympt√¥mes de votre plante pour obtenir un diagnostic personnalis√©.",
            "enter_description": "D√©crivez les sympt√¥mes de votre plante ici...",
            "config_title": "‚öôÔ∏è Configuration & Informations",
            "about_title": "‚ÑπÔ∏è √Ä Propos de l'Application",
            "load_model": "Charger le Mod√®le IA",
            "model_status": "Statut du Mod√®le IA"
        },
        "en": {
            "title": "üå± AgriLens AI - Plant Analysis Assistant",
            "subtitle": "Analyze your plants with AI to detect diseases",
            "tabs": ["üì∏ Image Analysis", "üìù Text Analysis", "‚öôÔ∏è Configuration", "‚ÑπÔ∏è About"],
            "image_analysis_title": "üì∏ Plant Image Analysis",
            "image_analysis_desc": "Upload or capture an image of your plant for a diagnosis.",
            "choose_image": "Choose a plant image...",
            "text_analysis_title": "üìù Textual Description Analysis",
            "text_analysis_desc": "Describe your plant's symptoms for a personalized diagnosis.",
            "enter_description": "Describe your plant's symptoms here...",
            "config_title": "‚öôÔ∏è Configuration & Information",
            "about_title": "‚ÑπÔ∏è About the Application",
            "load_model": "Load AI Model",
            "model_status": "AI Model Status"
        }
    }
    # Retourne le texte traduit ou la cl√© si la traduction n'existe pas
    return translations[st.session_state.language].get(key, key)

# --- CONSTANTES MOD√àLES ---
# Mod√®le principal recommand√© pour l'analyse de plantes, bas√© sur Gemma
MODEL_ID_HF = "google/gemma-3n-e2b-it"
# Mod√®le de fallback plus l√©ger, utilis√© si le mod√®le principal √©choue (non impl√©ment√© dans ce code, mais pr√©sent pour l'exemple)
MODEL_ID_FALLBACK = "microsoft/DialoGPT-medium"

# --- FONCTIONS DE CHARGEMENT ET D'ANALYSE DU MOD√àLE ---

def get_device_map():
    """D√©termine si le mod√®le doit √™tre charg√© sur GPU ou CPU."""
    if torch.cuda.is_available():
        st.session_state.device = "cuda"
        # 'auto' permet √† transformers de g√©rer la r√©partition sur les GPUs disponibles
        return "auto"
    else:
        st.session_state.device = "cpu"
        return "cpu"

def load_model():
    """
    Charge le mod√®le Gemma 3n et son processeur associ√©.
    Optimis√© pour les environnements comme Hugging Face Spaces avec des ressources potentiellement limit√©es.
    """
    try:
        from transformers import AutoProcessor, AutoModelForCausalLM
        
        # Limite le nombre de tentatives pour √©viter les boucles infinies en cas d'√©chec persistant.
        if st.session_state.load_attempt_count >= 3:
            st.error("‚ùå Trop de tentatives de chargement ont √©chou√©. Veuillez v√©rifier votre configuration et red√©marrer l'application.")
            return None, None
        st.session_state.load_attempt_count += 1
        
        st.info("üîç Diagnostic de l'environnement avant chargement...")
        issues = diagnose_loading_issues()
        if issues:
            with st.expander("üìä Diagnostic syst√®me", expanded=False):
                for issue in issues:
                    st.write(issue)
        
        # Nettoyage m√©moire agressif pour lib√©rer de la RAM et du cache GPU avant le chargement.
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        processor = None
        model = None
        device_map = get_device_map() # D√©termine si c'est CPU ou GPU
        
        try:
            st.info(f"Chargement du mod√®le depuis Hugging Face Hub : `{MODEL_ID_HF}`...")
            
            # Chargement du processeur (tokenizer, feature extractor, etc.)
            processor = AutoProcessor.from_pretrained(
                MODEL_ID_HF,
                trust_remote_code=True,
                cache_dir="/tmp/huggingface_cache" # Utilise un r√©pertoire temporaire pour le cache
            )
            
            # Param√®tres pour optimiser le chargement et l'inf√©rence, surtout sur des ressources limit√©es.
            model_kwargs = {
                "torch_dtype": torch.float16, # Utilise float16 pour r√©duire l'empreinte m√©moire GPU
                "trust_remote_code": True,
                "low_cpu_mem_usage": True, # Aide √† r√©duire l'utilisation CPU lors du chargement
                "device_map": device_map, # "auto" pour r√©partir sur les GPUs, "cpu" sinon
                "cache_dir": "/tmp/huggingface_cache",
            }
            
            # Limite la m√©moire GPU √† 4GB si un GPU est disponible. Ceci est crucial pour les GPU avec moins de m√©moire.
            if torch.cuda.is_available():
                model_kwargs["max_memory"] = {0: "4GB"}
            
            # Si on est sur CPU, on peut essayer d'autres optimisations, comme le offload sur disque.
            if device_map == "cpu":
                model_kwargs.update({
                    "torch_dtype": torch.float32, # float32 est souvent plus stable sur CPU
                    "offload_folder": "/tmp/model_offload" # Dossier pour d√©charger des parties du mod√®le si n√©cessaire
                })
            
            # Chargement du mod√®le lui-m√™me
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_ID_HF,
                **model_kwargs
            )
            
            st.success(f"‚úÖ Mod√®le `{MODEL_ID_HF}` charg√© avec succ√®s.")
            st.session_state.model_status = "Charg√© (Hub)"
            st.session_state.model_loaded = True
            st.session_state.model_load_time = time.time()
            st.session_state.load_attempt_count = 0 # R√©initialise le compteur en cas de succ√®s
            
            return model, processor
            
        except Exception as e:
            st.error(f"‚ùå √âchec du chargement du mod√®le depuis Hugging Face Hub : ")
            st.error("üí° Conseil : Le mod√®le peut √™tre trop volumineux pour les ressources disponibles (RAM/VRAM).")
            st.error(f"D√©tails de l'erreur : {e}")
            # Essayer de charger un mod√®le plus l√©ger si le principal √©choue (impl√©mentation √† ajouter si n√©cessaire)
            # if MODEL_ID_FALLBACK: ...
            return None, None
            
    except ImportError:
        st.error("‚ùå Erreur : Les biblioth√®ques `transformers` ou `torch` ne sont pas install√©es.")
        return None, None
    except Exception as e:
        st.error(f"‚ùå Erreur g√©n√©rale lors de l'initialisation du chargement du mod√®le : ")
        st.error(f"D√©tails de l'erreur : {e}")
        return None, None

# ==============================================================================
# FONCTION CORRIG√âE POUR L'ANALYSE D'IMAGE (MULTIMODALE)
# ==============================================================================

def analyze_image_multilingual(image, prompt_text=""):
    """
    Analyse une image de plante en utilisant le mod√®le Gemma et un prompt personnalis√©.
    Utilise la m√©thode apply_chat_template pour une gestion robuste des entr√©es multimodales.
    """
    if not st.session_state.model_loaded or not check_model_health():
        st.error("‚ùå Mod√®le IA non charg√© ou non fonctionnel. Veuillez le charger via la barre lat√©rale.")
        return None
        
    try:
        # S'assurer que l'image est en mode RGB pour le traitement
        if image.mode != 'RGB':
            image = image.convert('RGB')
            
        # Pr√©paration des messages au format attendu par apply_chat_template pour les mod√®les multimodaux
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image}, # L'image elle-m√™me
                    {"type": "text", "text": prompt_text} # Le prompt texte associ√©
                ]
            }
        ]
        
        # Application du template de chat pour obtenir les inputs format√©s pour le mod√®le
        inputs = st.session_state.processor.apply_chat_template(
            messages,
            add_generation_prompt=True, # Ajoute le pr√©fixe pour la g√©n√©ration de r√©ponse par le mod√®le
            tokenize=True,
            return_dict=True,
            return_tensors="pt" # Retourne des tensors PyTorch
        )
        
        # D√©placement des tensors vers le bon device (GPU ou CPU)
        inputs = {key: val.to(st.session_state.model.device) for key, val in inputs.items()}
        
        with st.spinner("üîç Analyse d'image en cours... (peut prendre plusieurs minutes sur CPU)"):
            input_len = inputs["input_ids"].shape[-1] # Nombre de tokens en entr√©e
            
            # G√©n√©ration de la r√©ponse par le mod√®le
            outputs = st.session_state.model.generate(
                **inputs,
                max_new_tokens=768, # Longueur maximale de la r√©ponse g√©n√©r√©e
                do_sample=True, # Permet une g√©n√©ration plus cr√©ative
                temperature=0.7, # Contr√¥le le caract√®re al√©atoire de la g√©n√©ration
                top_p=0.9 # Utilise le top-p sampling
            )
            
            # Extraction de la partie g√©n√©r√©e par le mod√®le (en excluant les tokens d'entr√©e)
            generation = outputs[0][input_len:]
            response = st.session_state.processor.decode(generation, skip_special_tokens=True) # D√©codage en texte
            
            return response.strip() # Retourne le texte de r√©ponse nettoy√©
            
    except Exception as e:
        st.error(f"‚ùå Erreur lors de l'analyse de l'image : ")
        st.error(traceback.format_exc()) # Affiche la trace compl√®te de l'erreur pour le d√©bogage
        return None

# ==============================================================================
# FONCTION CORRIG√âE POUR L'ANALYSE DE TEXTE
# ==============================================================================

def analyze_text_multilingual(text_description):
    """
    Analyse une description textuelle des sympt√¥mes d'une plante en utilisant le mod√®le Gemma.
    Retourne le diagnostic et les recommandations.
    """
    if not st.session_state.model_loaded or not check_model_health():
        st.error("‚ùå Mod√®le IA non charg√© ou non fonctionnel. Veuillez le charger via la barre lat√©rale.")
        return None
        
    try:
        # Construction d'un prompt structur√© pour demander un diagnostic pr√©cis au mod√®le.
        prompt = f"""Analyse la description des sympt√¥mes de cette plante et fournis un diagnostic d√©taill√© :
**Description des sympt√¥mes :**

**Instructions pour le diagnostic :**
1.  **Diagnostic probable :** Quel est le probl√®me principal (maladie, carence, etc.) ?
2.  **Causes possibles :** Quelles sont les raisons derri√®re ces sympt√¥mes ?
3.  **Recommandations de traitement :** Quels traitements sont les plus adapt√©s ?
4.  **Conseils de soins pr√©ventifs :** Comment √©viter que le probl√®me ne se reproduise ?
R√©ponds de mani√®re structur√©e, claire et en fran√ßais. Ne r√©p√®te pas la description des sympt√¥mes dans ta r√©ponse."""
        
        # Pr√©paration des entr√©es pour le mod√®le (ici, uniquement du texte).
        # Note: Pour les mod√®les chat comme Gemma, il est pr√©f√©rable d'utiliser apply_chat_template m√™me pour le texte seul.
        # Ici, j'utilise directement le processor pour un exemple simple, mais apply_chat_template est plus robuste.
        # Si tu utilises `apply_chat_template` pour l'image, il faut aussi l'utiliser ici pour la coh√©rence.
        # Dans le code original, `analyze_image_multilingual` utilise `apply_chat_template`.
        # Pour simplifier et pour cet exemple, je vais adapter le prompt pour √™tre utilis√© avec `apply_chat_template`
        # comme le mod√®le est un mod√®le de chat.

        messages = [
            {"role": "user", "content": prompt}
        ]
        inputs = st.session_state.processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt"
        )
        
        # D√©placement des tensors sur le bon device
        inputs = {key: val.to(st.session_state.model.device) for key, val in inputs.items()}
        
        # G√©n√©rer la r√©ponse
        with st.spinner("üîç Analyse textuelle en cours..."):
            outputs = st.session_state.model.generate(
                **inputs,
                max_new_tokens=512, # Longueur maximale de la r√©ponse
                do_sample=True,
                temperature=0.7,
                top_p=0.9
            )
            
            # D√©coder la r√©ponse
            response = st.session_state.processor.decode(outputs[0], skip_special_tokens=True)
            
            # Extraire uniquement la partie g√©n√©r√©e par le mod√®le (en retirant le prompt initial)
            # Il faut trouver un moyen fiable de s√©parer le prompt de la r√©ponse.
            # Si apply_chat_template formate bien, la r√©ponse commence apr√®s les tokens du prompt.
            # Une m√©thode plus s√ªre serait de reconstruire le prompt format√© et de le chercher dans la r√©ponse.
            # Pour cet exemple, je vais extraire la partie apr√®s le prompt original.
            
            # Trouve la position de fin du contenu du dernier message utilisateur
            # Cela suppose que `apply_chat_template` ajoute des s√©parateurs clairs.
            # Une approche plus robuste serait de s√©parer en fonction des tokens sp√©ciaux ou du r√¥le.
            
            # Si le prompt est exactement le contenu du dernier message et que le mod√®le r√©pond apr√®s:
            # Il faut faire attention si le mod√®le r√©p√®te une partie du prompt.
            # On va se baser sur la longueur des tokens d'entr√©e pour extraire la partie g√©n√©r√©e.
            
            input_len = inputs["input_ids"].shape[-1]
            response_only_tokens = outputs[0][input_len:]
            response_only = st.session_state.processor.decode(response_only_tokens, skip_special_tokens=True)
            
            return response_only.strip()
            
    except Exception as e:
        st.error(f"‚ùå Erreur lors de l'analyse textuelle : ")
        st.error(traceback.format_exc())
        return None

# --- INTERFACE UTILISATEUR STREAMLIT ---

st.title(t("title"))
st.markdown(t("subtitle"))

# --- BARRE LAT√âRALE (SIDEBAR) POUR LA CONFIGURATION ---
with st.sidebar:
    st.header(t("config_title"))
    
    # S√©lecteur de langue
    lang_selector_options = ["Fran√ßais", "English"]
    current_lang_index = 0 if st.session_state.language == "fr" else 1
    language_selected = st.selectbox(
        "üåê Langue / Language",
        lang_selector_options,
        index=current_lang_index,
        help="S√©lectionnez la langue de l'interface et des r√©ponses."
    )
    # Met √† jour la langue dans la session_state si elle change
    st.session_state.language = "fr" if language_selected == "Fran√ßais" else "en"
    
    st.divider()
    
    # Affichage du statut du mod√®le IA et bouton de chargement/rechargement
    st.header(t("model_status"))
    
    if st.session_state.model_loaded and check_model_health():
        st.success("‚úÖ Mod√®le charg√© et fonctionnel")
        st.write(f"**Statut :** `{st.session_state.model_status}`")
        if st.session_state.model_load_time:
            # Affiche l'heure de chargement de mani√®re lisible
            load_time_str = time.strftime('%H:%M:%S', time.localtime(st.session_state.model_load_time))
            st.write(f"**Heure de chargement :** {load_time_str}")
        
        # Bouton pour recharger le mod√®le si n√©cessaire
        if st.button("üîÑ Recharger le mod√®le", type="secondary"):
            st.session_state.model_loaded = False
            st.session_state.model = None
            st.session_state.processor = None
            st.session_state.model_status = "Non charg√©"
            st.session_state.load_attempt_count = 0
            st.info("Mod√®le d√©charg√©. Cliquez sur 'Charger le mod√®le IA' pour le recharger.")
            st.rerun() # Relance l'application pour r√©initialiser l'√©tat
    else:
        st.warning("‚ö†Ô∏è Mod√®le IA non charg√©")
        # Bouton pour charger le mod√®le
        if st.button(t("load_model"), type="primary"):
            with st.spinner("üîÑ Chargement du mod√®le IA en cours..."):
                load_model() # Appelle la fonction de chargement
            st.rerun() # Relance l'application pour mettre √† jour le statut

    st.divider()
    
    # Section pour afficher les ressources syst√®me
    st.subheader("üìä Ressources Syst√®me")
    afficher_ram_disponible() # Affiche l'utilisation de la RAM
    
    if torch.cuda.is_available():
        try:
            # Affiche l'utilisation de la m√©moire GPU
            gpu_memory_allocated = torch.cuda.memory_allocated(0) / (1024**3)
            gpu_total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            st.write(f"üöÄ GPU utilisation : {gpu_memory_allocated:.1f}GB / {gpu_total_memory:.1f}GB")
        except Exception:
            st.write("üöÄ GPU : Non disponible (utilisation CPU)")
    else:
        st.write("üöÄ GPU : Non disponible (utilisation CPU)")

# --- ONGLET PRINCIPAUX ---
# Cr√©e les onglets pour organiser l'application
tab1, tab2, tab3, tab4 = st.tabs(t("tabs"))

# --- ONGLET 1: ANALYSE D'IMAGE ---
with tab1:
    st.header(t("image_analysis_title"))
    st.markdown(t("image_analysis_desc"))
    
    # Option pour choisir entre l'upload d'un fichier ou la capture via webcam
    capture_option = st.radio(
        "Choisissez votre m√©thode de capture :",
        ["üìÅ Upload d'image" if st.session_state.language == "fr" else "üìÅ Upload Image",
         "üì∑ Capture par webcam" if st.session_state.language == "fr" else "üì∑ Webcam Capture"],
        horizontal=True
    )
    
    image_to_analyze_data = None
    if capture_option.startswith("üìÅ"):
        # Widget pour uploader un fichier image
        image_to_analyze_data = st.file_uploader(
            t("choose_image"),
            type=['png', 'jpg', 'jpeg']
        )
    else:
        # Widget pour capturer une image via la webcam
        image_to_analyze_data = st.camera_input("Prendre une photo de la plante")
    
    # Si une image a √©t√© fournie (upload√©e ou captur√©e)
    if image_to_analyze_data:
        image_to_analyze = Image.open(image_to_analyze_data)
        original_size = image_to_analyze.size
        # Redimensionne l'image si n√©cessaire avant l'analyse
        resized_image, was_resized = resize_image_if_needed(image_to_analyze)
        
        # Utilise des colonnes pour afficher l'image et les options d'analyse c√¥te √† c√¥te
        col1, col2 = st.columns([1, 1])
        with col1:
            st.image(resized_image, caption="Image √† analyser", use_container_width=True)
            if was_resized:
                st.info(f"‚ÑπÔ∏è Image redimensionn√©e de {original_size} √† {resized_image.size} pour l'analyse.")
        
        with col2:
            # Affiche les options d'analyse uniquement si le mod√®le est charg√©
            if st.session_state.model_loaded and check_model_health():
                st.subheader("Options d'analyse")
                # Prompt par d√©faut pour une analyse d√©taill√©e de l'image
                default_prompt = """Analyse cette image de plante et fournis un diagnostic complet :
1.  **√âtat g√©n√©ral de la plante :** D√©cris son apparence globale.
2.  **Identification des probl√®mes :** Liste les maladies, parasites ou carences visibles.
3.  **Diagnostic probable :** Indique le probl√®me le plus probable.
4.  **Causes possibles :** Explique ce qui a pu causer ce probl√®me.
5.  **Recommandations de traitement :** Propose des solutions concr√®tes.
6.  **Conseils pr√©ventifs :** Donne des astuces pour √©viter que le probl√®me ne revienne."""
                
                # Champ pour un prompt personnalis√©
                custom_prompt_input = st.text_area(
                    "Prompt personnalis√© (optionnel) :",
                    value=default_prompt,
                    height=250,
                    placeholder="Entrez une requ√™te sp√©cifique ici..."
                )
                
                # Bouton pour lancer l'analyse de l'image
                if st.button("üîç Analyser l'image", type="primary"):
                    analysis_result = analyze_image_multilingual(resized_image, prompt_text=custom_prompt_input)
                    
                    if analysis_result:
                        st.success("‚úÖ Analyse termin√©e !")
                        st.markdown("### üìã R√©sultats de l'analyse")
                        st.markdown(analysis_result) # Affiche le r√©sultat de l'analyse
                    else:
                        st.error("‚ùå √âchec de l'analyse de l'image.")
            else:
                st.warning("‚ö†Ô∏è Mod√®le IA non charg√©. Veuillez d'abord charger le mod√®le depuis la barre lat√©rale.")

# --- ONGLET 2: ANALYSE DE TEXTE ---
with tab2:
    st.header(t("text_analysis_title"))
    st.markdown(t("text_analysis_desc"))
    
    # Zone de texte pour saisir la description des sympt√¥mes
    text_description_input = st.text_area(
        t("enter_description"),
        height=200,
        placeholder="Ex: Feuilles de tomate avec des taches jaunes et brunes, les bords s'enroulent vers le haut..."
    )
    
    # Bouton pour lancer l'analyse textuelle
    if st.button("üîç Analyser la description", type="primary"):
        if text_description_input.strip(): # V√©rifie que le champ n'est pas vide
            if st.session_state.model_loaded and check_model_health():
                analysis_result = analyze_text_multilingual(text_description_input)
                
                if analysis_result:
                    st.success("‚úÖ Analyse termin√©e !")
                    st.markdown("### üìã R√©sultats de l'analyse")
                    st.markdown(analysis_result) # Affiche le r√©sultat de l'analyse
                else:
                    st.error("‚ùå √âchec de l'analyse textuelle.")
            else:
                st.warning("‚ö†Ô∏è Mod√®le IA non charg√©. Veuillez d'abord charger le mod√®le depuis la barre lat√©rale.")
        else:
            st.warning("‚ö†Ô∏è Veuillez entrer une description des sympt√¥mes.")

# --- ONGLET 3: CONFIGURATION & INFORMATIONS ---
with tab3:
    st.header(t("config_title"))
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("üîß Informations Syst√®me")
        try:
            # Affiche les informations syst√®me d√©taill√©es
            ram = psutil.virtual_memory()
            st.write(f"**RAM Totale :** {ram.total / (1024**3):.1f} GB")
            st.write(f"**RAM Utilis√©e :** {ram.used / (1024**3):.1f} GB ({ram.percent:.1f}%)")
            disk = psutil.disk_usage('/')
            st.write(f"**Espace Disque Libre (/) :** {disk.free / (1024**3):.1f} GB")
            if torch.cuda.is_available():
                st.write(f"**GPU D√©tect√© :** {torch.cuda.get_device_name(0)}")
            else:
                st.write("**GPU :** Non disponible")
        except Exception as e:
            st.error(f"Erreur lors de la r√©cup√©ration des informations syst√®me : {e}")
            
    with col2:
        st.subheader("üìä Statistiques du Mod√®le IA")
        if st.session_state.model_loaded and check_model_health():
            st.write("**Statut :** ‚úÖ Charg√© et fonctionnel")
            st.write(f"**Type de mod√®le :** `{type(st.session_state.model).__name__}`")
            st.write(f"**Device utilis√© :** `{st.session_state.model.device}`")
        else:
            st.write("**Statut :** ‚ùå Non charg√©")

# --- ONGLET 4: √Ä PROPOS ---
with tab4:
    st.header(t("about_title"))
    st.markdown("""
    ### üå± AgriLens AI : L'expert agronome dans votre poche
    **AgriLens AI** a √©t√© con√ßu pour le **Google - The Gemma 3n Impact Challenge**. Notre mission est de fournir aux agriculteurs et jardiniers du monde entier un outil de diagnostic puissant, gratuit et **fonctionnant sans connexion Internet**.
    #### Notre Vision
    Dans de nombreuses r√©gions du monde, l'acc√®s √† l'expertise agricole est limit√© et la connectivit√© internet est peu fiable. Ces obstacles entra√Ænent des pertes de r√©coltes qui auraient pu √™tre √©vit√©es. AgriLens AI r√©pond directement √† ce probl√®me en exploitant les capacit√©s **offline et multimodales** du mod√®le Gemma 3n.
    #### Fonctionnalit√©s Cl√©s pour l'Impact
    -   **‚úÖ 100% Offline :** Apr√®s le t√©l√©chargement initial du mod√®le, l'application fonctionne sans aucune connexion, garantissant l'acc√®s et la confidentialit√© des donn√©es, m√™me dans les zones les plus recul√©es.
    -   **üì∏ Analyse Visuelle Instantan√©e :** Prenez une photo de votre plante et obtenez un diagnostic d√©taill√© en quelques instants.
    -   **üó£Ô∏è Support Multilingue :** L'interface et les prompts sont con√ßus pour √™tre facilement traduisibles, brisant les barri√®res linguistiques.
    #### Technologie
    -   **Mod√®le IA :** Google Gemma 3n (`google/gemma-3n-e2b-it`).
    -   **Framework :** Streamlit pour une interface rapide et interactive.
    -   **Biblioth√®ques :** `transformers`, `torch`, `Pillow`, `psutil`.
    
    ---
    *D√©velopp√© avec passion pour un impact durable dans le secteur agricole.*
    """)

# --- PIED DE PAGE ---
st.divider()
st.markdown("<div style='text-align: center; color: #666;'>Projet pour le Google - The Gemma 3n Impact Challenge</div>", unsafe_allow_html=True)