import streamlit as st
import os
import io
from PIL import Image
import requests
import torch
import google.generativeai as genai
import gc
import time
import sys
import psutil
import signal
from contextlib import contextmanager

# Cache global pour le mod√®le (persiste entre les reruns)
if 'global_model_cache' not in st.session_state:
    st.session_state.global_model_cache = {}
if 'model_load_time' not in st.session_state:
    st.session_state.model_load_time = None
if 'model_persistence_check' not in st.session_state:
    st.session_state.model_persistence_check = False

@contextmanager
def timeout(seconds):
    """Context manager pour timeout"""
    def signal_handler(signum, frame):
        raise TimeoutError(f"Timeout apr√®s {seconds} secondes")
    
    # Enregistrer l'ancien handler
    old_handler = signal.signal(signal.SIGALRM, signal_handler)
    signal.alarm(seconds)
    
    try:
        yield
    finally:
        # Restaurer l'ancien handler
        signal.alarm(0)
        signal.signal(signal.SIGALRM, old_handler)

def check_model_persistence():
    """V√©rifie si le mod√®le est toujours persistant en m√©moire"""
    try:
        if hasattr(st.session_state, 'model') and st.session_state.model is not None:
            # Test simple pour v√©rifier que le mod√®le fonctionne
            if hasattr(st.session_state.model, 'device'):
                device = st.session_state.model.device
                return True
        return False
    except Exception:
        return False

def force_model_persistence():
    """Force la persistance du mod√®le en m√©moire"""
    try:
        if hasattr(st.session_state, 'model') and st.session_state.model is not None:
            # Log de d√©bogage
            st.write("üîç DEBUG: Tentative de persistance du mod√®le...")
            st.write(f"üîç DEBUG: Mod√®le pr√©sent: {st.session_state.model is not None}")
            st.write(f"üîç DEBUG: Type du mod√®le: {type(st.session_state.model).__name__}")
            
            # Cr√©er une r√©f√©rence forte au mod√®le
            st.session_state.global_model_cache['model'] = st.session_state.model
            st.session_state.global_model_cache['processor'] = st.session_state.processor
            st.session_state.global_model_cache['load_time'] = time.time()
            st.session_state.global_model_cache['model_type'] = type(st.session_state.model).__name__
            st.session_state.global_model_cache['processor_type'] = type(st.session_state.processor).__name__
            
            # V√©rification imm√©diate
            if st.session_state.global_model_cache.get('model') is not None:
                st.session_state.model_persistence_check = True
                
                # V√©rification suppl√©mentaire
                if hasattr(st.session_state.global_model_cache['model'], 'device'):
                    st.session_state.global_model_cache['device'] = st.session_state.global_model_cache['model'].device
                
                st.write("üîç DEBUG: Persistance r√©ussie!")
                st.write(f"üîç DEBUG: Cache contient: {list(st.session_state.global_model_cache.keys())}")
                return True
            else:
                st.write("üîç DEBUG: √âchec de la persistance - mod√®le non trouv√© dans le cache")
        else:
            st.write("üîç DEBUG: √âchec de la persistance - mod√®le non pr√©sent dans session_state")
        return False
    except Exception as e:
        st.error(f"Erreur lors de la persistance forc√©e : {e}")
        st.write(f"üîç DEBUG: Exception lors de la persistance: {e}")
        return False

def restore_model_from_cache():
    """Restaure le mod√®le depuis le cache global"""
    try:
        st.write("üîç DEBUG: Tentative de restauration depuis le cache...")
        st.write(f"üîç DEBUG: Cache disponible: {list(st.session_state.global_model_cache.keys())}")
        
        if 'model' in st.session_state.global_model_cache and st.session_state.global_model_cache['model'] is not None:
            # V√©rifier que le mod√®le est toujours valide
            cached_model = st.session_state.global_model_cache['model']
            st.write(f"üîç DEBUG: Mod√®le trouv√© dans le cache: {cached_model is not None}")
            
            if hasattr(cached_model, 'device'):
                # Le mod√®le semble valide
                st.session_state.model = cached_model
                st.session_state.processor = st.session_state.global_model_cache['processor']
                st.session_state.model_loaded = True
                st.session_state.model_status = "Charg√© (cache)"
                
                # Mettre √† jour le temps de chargement si disponible
                if 'load_time' in st.session_state.global_model_cache:
                    st.session_state.model_load_time = st.session_state.global_model_cache['load_time']
                
                st.write("üîç DEBUG: Restauration r√©ussie!")
                return True
            else:
                st.write("üîç DEBUG: Mod√®le dans le cache mais pas d'attribut 'device'")
        else:
            st.write("üîç DEBUG: Mod√®le non trouv√© dans le cache")
        return False
    except Exception as e:
        st.error(f"Erreur lors de la restauration depuis le cache : {e}")
        st.write(f"üîç DEBUG: Exception lors de la restauration: {e}")
        return False

def diagnose_loading_issues():
    """Diagnostique les probl√®mes potentiels de chargement"""
    issues = []
    
    # V√©rifier l'environnement
    if os.path.exists("D:/Dev/model_gemma"):
        issues.append("‚úÖ Mod√®le local d√©tect√©")
    else:
        issues.append("üåê Mode Hugging Face d√©tect√©")
    
    # V√©rifier les d√©pendances
    try:
        import transformers
        issues.append(f"‚úÖ Transformers version: {transformers.__version__}")
    except ImportError:
        issues.append("‚ùå Transformers non install√©")
    
    try:
        import torch
        issues.append(f"‚úÖ PyTorch version: {torch.__version__}")
        if torch.cuda.is_available():
            issues.append(f"‚úÖ CUDA disponible: {torch.cuda.get_device_name(0)}")
        else:
            issues.append("‚ö†Ô∏è CUDA non disponible - utilisation CPU")
    except ImportError:
        issues.append("‚ùå PyTorch non install√©")
    
    # V√©rifier la m√©moire disponible
    try:
        import psutil
        memory = psutil.virtual_memory()
        issues.append(f"üíæ M√©moire disponible: {memory.available // (1024**3)} GB")
    except ImportError:
        issues.append("‚ö†Ô∏è Impossible de v√©rifier la m√©moire")
    
    return issues

def resize_image_if_needed(image, max_size=(800, 800)):
    """
    Redimensionne l'image si elle d√©passe la taille maximale sp√©cifi√©e
    """
    width, height = image.size
    
    if width > max_size[0] or height > max_size[1]:
        # Calculer le ratio pour maintenir les proportions
        ratio = min(max_size[0] / width, max_size[1] / height)
        new_width = int(width * ratio)
        new_height = int(height * ratio)
        
        # Redimensionner l'image
        resized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        return resized_image, True  # True indique que l'image a √©t√© redimensionn√©e
    else:
        return image, False  # False indique que l'image n'a pas √©t√© redimensionn√©e

def afficher_ram_disponible(context=""):
    mem = psutil.virtual_memory()
    st.info(f"üíæ RAM disponible {context}: {mem.available // (1024**2)} MB ({mem.available // (1024**3)} GB)")
    if mem.available < 4 * 1024**3:
        st.warning("‚ö†Ô∏è Moins de 4GB de RAM disponible, le chargement du mod√®le risque d'√©chouer !")

# Configuration de la page
st.set_page_config(
    page_title="AgriLens AI - Plant Disease Diagnosis",
    page_icon="üå±",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# Configuration pour √©viter les erreurs 403
st.markdown("""
<script>
// D√©sactiver les v√©rifications CORS pour les uploads
window.addEventListener('load', function() {
    if (typeof window.parent !== 'undefined' && window.parent !== window) {
        // Si l'app est dans un iframe (comme sur Hugging Face Spaces)
        console.log('Application d√©tect√©e dans un iframe - configuration sp√©ciale activ√©e');
    }
});
</script>
""", unsafe_allow_html=True)

# CSS pour mobile
st.markdown("""
<style>
@media (max-width: 600px) {
    .main {
        max-width: 100vw !important;
        padding: 0.5rem !important;
    }
    .stButton button, .stTextInput input, .stTextArea textarea {
        width: 100% !important;
        font-size: 1.1rem !important;
    }
    .stSidebar {
        width: 100vw !important;
        min-width: 100vw !important;
    }
    .result-box {
        font-size: 1.05rem !important;
    }
    .stMarkdown, .stHeader, .stSubheader {
        font-size: 1.1rem !important;
    }
    .stFileUploader, .stImage {
        width: 100% !important;
    }
}
</style>
""", unsafe_allow_html=True)

# Initialisation des variables de session
if 'model_loaded' not in st.session_state:
    st.session_state.model_loaded = False
if 'model_status' not in st.session_state:
    st.session_state.model_status = "Non charg√©"
if 'language' not in st.session_state:
    st.session_state.language = "fr"

# === AJOUT : Chargement automatique du mod√®le local au d√©marrage ===
is_local = os.path.exists("models/gemma-3n-transformers-gemma-3n-e2b-it-v1")
if is_local and not st.session_state.model_loaded:
                st.info("üîÑ Chargement automatique du mod√®le local : models/gemma-3n-transformers-gemma-3n-e2b-it-v1 ...")
                try:
                    from transformers import AutoProcessor, Gemma3nForConditionalGeneration
                    model_path = "models/gemma-3n-transformers-gemma-3n-e2b-it-v1"
                    
                    # Nettoyer la m√©moire avant le chargement
                    gc.collect()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    
                    # Afficher la RAM disponible avant chargement
                    afficher_ram_disponible("AVANT chargement automatique")
                    
                    # Charger le processeur avec la configuration locale
                    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
                    
                    # Charger le mod√®le avec la configuration locale (ultra-agressive)
                    # Utiliser bfloat16 comme sp√©cifi√© dans config.json et forcer CPU
                    model = Gemma3nForConditionalGeneration.from_pretrained(
                        model_path,
                        torch_dtype=torch.bfloat16,  # Utiliser bfloat16 comme dans config.json
                        trust_remote_code=True,
                        low_cpu_mem_usage=True,
                        device_map=None,  # Forcer CPU
                        max_memory=None,  # Pas de limite de m√©moire
                        offload_folder=None,  # Pas d'offload
                        offload_state_dict=False,  # Pas d'offload
                        load_in_4bit=False,  # Pas de quantification
                        load_in_8bit=False,  # Pas de quantification
                        attn_implementation="eager"  # Impl√©mentation CPU
                    )
                    
                    st.session_state.model = model
                    st.session_state.processor = processor
                    st.session_state.model_loaded = True
                    st.session_state.model_status = "Charg√© automatiquement (local)"
                    st.session_state.model_load_time = time.time()
                    
                    # Afficher la RAM disponible apr√®s chargement
                    afficher_ram_disponible("APR√àS chargement automatique")
                    
                    st.success("‚úÖ Mod√®le local charg√© automatiquement au d√©marrage !")
                except Exception as e:
                    st.session_state.model_loaded = False
                    st.session_state.model_status = "Erreur chargement automatique"
                    st.error(f"‚ùå Erreur lors du chargement automatique du mod√®le local : {e}")
                    st.write(f"üîç DEBUG: Exception d√©taill√©e: {str(e)}")
elif not is_local and not st.session_state.model_loaded:
    # Ne pas charger automatiquement sur HF Spaces - laisser l'utilisateur choisir
    st.info("üîÑ Environnement Hugging Face Spaces d√©tect√©")
    st.info("‚ö†Ô∏è Chargement automatique d√©sactiv√© pour √©viter les erreurs de m√©moire")
    st.info("üí° Utilisez le bouton 'Charger le mod√®le' pour charger un mod√®le compatible")
    st.session_state.model_loaded = False
    st.session_state.model_status = "En attente de chargement manuel"

# Configuration Gemini API
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)
    gemini_model = genai.GenerativeModel('gemini-1.5-flash')
else:
    gemini_model = None

# Dictionnaires de traductions
translations = {
    "fr": {
        "title": "üå± AgriLens AI - Diagnostic des Plantes",
        "subtitle": "**Application de diagnostic des maladies de plantes avec IA**",
        "config_title": "‚öôÔ∏è Configuration",
        "load_model": "Charger un mod√®le IA",
        "model_status": "**Statut du mod√®le :**",
        "not_loaded": "Non charg√©",
        "loaded": "‚úÖ Charg√©",
        "error": "‚ùå Erreur",
        "tabs": ["üì∏ Analyse d'Image", "üí¨ Analyse de Texte", "üìñ Manuel", "‚ÑπÔ∏è √Ä propos"],
        "image_analysis_title": "üîç Diagnostic par Image",
        "image_analysis_desc": "T√©l√©chargez une photo de plante malade pour obtenir un diagnostic",
        "choose_image": "Choisissez une image...",
        "analyze_button": "üî¨ Analyser avec l'IA",
        "text_analysis_title": "üí¨ Diagnostic par Texte",
        "text_analysis_desc": "D√©crivez les sympt√¥mes de votre plante pour obtenir des conseils",
        "symptoms_desc": "Description des sympt√¥mes :",
        "analysis_results": "## üìä R√©sultats de l'Analyse",
        "manual_title": "üìñ Manuel Utilisateur",
        "about_title": "‚ÑπÔ∏è √Ä propos d'AgriLens AI",
        "creator_title": "üë®‚Äçüíª Cr√©ateur de l'Application",
        "creator_name": "**Sidoine Kolaol√© YEBADOKPO**",
        "creator_location": "Bohicon, R√©publique du B√©nin",
        "creator_phone": "+229 01 96 91 13 46",
        "creator_email": "syebadokpo@gmail.com",
        "creator_linkedin": "linkedin.com/in/sidoineko",
        "creator_portfolio": "Hugging Face Portfolio: Sidoineko/portfolio",
        "competition_title": "üèÜ Version Comp√©tition Kaggle",
        "competition_text": "Cette premi√®re version d'AgriLens AI a √©t√© d√©velopp√©e sp√©cifiquement pour participer √† la comp√©tition Kaggle. Elle repr√©sente notre premi√®re production publique et d√©montre notre expertise en IA appliqu√©e √† l'agriculture.",
        "footer": "*AgriLens AI - Diagnostic intelligent des plantes avec IA*"
    },
    "en": {
        "title": "üå± AgriLens AI - Plant Disease Diagnosis",
        "subtitle": "**AI-powered plant disease diagnosis application**",
        "config_title": "‚öôÔ∏è Configuration",
        "load_model": "Load AI Model",
        "model_status": "**Model Status:**",
        "not_loaded": "Not loaded",
        "loaded": "‚úÖ Loaded",
        "error": "‚ùå Error",
        "tabs": ["üì∏ Image Analysis", "üí¨ Text Analysis", "üìñ Manual", "‚ÑπÔ∏è About"],
        "image_analysis_title": "üîç Image Diagnosis",
        "image_analysis_desc": "Upload a photo of a diseased plant to get a diagnosis",
        "choose_image": "Choose an image...",
        "analyze_button": "üî¨ Analyze with AI",
        "text_analysis_title": "üí¨ Text Diagnosis",
        "text_analysis_desc": "Describe your plant symptoms to get advice",
        "symptoms_desc": "Symptom description:",
        "analysis_results": "## üìä Analysis Results",
        "manual_title": "üìñ User Manual",
        "about_title": "‚ÑπÔ∏è About AgriLens AI",
        "creator_title": "üë®‚Äçüíª Application Creator",
        "creator_name": "**Sidoine Kolaol√© YEBADOKPO**",
        "creator_location": "Bohicon, Benin Republic",
        "creator_phone": "+229 01 96 91 13 46",
        "creator_email": "syebadokpo@gmail.com",
        "creator_linkedin": "linkedin.com/in/sidoineko",
        "creator_portfolio": "Hugging Face Portfolio: Sidoineko/portfolio",
        "competition_title": "üèÜ Kaggle Competition Version",
        "competition_text": "This first version of AgriLens AI was specifically developed to participate in the Kaggle competition. It represents our first public production and demonstrates our expertise in AI applied to agriculture.",
        "footer": "*AgriLens AI - Intelligent plant diagnosis with AI*"
    }
}

def t(key):
    return translations[st.session_state.language][key]

# Classe processeur personnalis√©e pour Gemma 3n
class Gemma3nProcessor:
    def __init__(self, tokenizer, image_processor):
        self.tokenizer = tokenizer
        self.image_processor = image_processor
    
    def __call__(self, text=None, images=None, return_tensors=None, **kwargs):
        # Traiter le texte avec le tokenizer
        if text is not None:
            text_inputs = self.tokenizer(
                text, 
                return_tensors=return_tensors, 
                padding=True, 
                truncation=True,
                **kwargs
            )
        
        # Traiter l'image avec l'image processor
        if images is not None:
            image_inputs = self.image_processor(
                images, 
                return_tensors=return_tensors,
                **kwargs
            )
            
            # Combiner les inputs
            if text is not None:
                inputs = {**text_inputs, **image_inputs}
            else:
                inputs = image_inputs
        else:
            inputs = text_inputs
        
        return inputs
    
    def decode(self, token_ids, skip_special_tokens=True, **kwargs):
        return self.tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens, **kwargs)

def load_model():
    """Charge le mod√®le avec gestion d'erreurs et fallback pour HF Spaces"""
    
    # V√©rifier si le mod√®le est d√©j√† charg√©
    if hasattr(st.session_state, 'model') and st.session_state.model is not None:
        if check_model_persistence():
            st.success("‚úÖ Mod√®le d√©j√† charg√© et persistant")
            return st.session_state.model, st.session_state.processor
    
    # D√©tecter l'environnement Hugging Face Spaces
    is_hf_spaces = os.environ.get('SPACE_ID') is not None
    
    if is_hf_spaces:
        st.warning("üö® Environnement Hugging Face Spaces d√©tect√©")
        st.info("‚ö†Ô∏è Utilisation de mod√®les l√©gers compatibles avec les contraintes m√©moire")
        
        # Strat√©gie HF Spaces : Mod√®les ultra-l√©gers uniquement
        try:
            return load_ultra_lightweight_for_hf_spaces()
        except Exception as e:
            st.error(f"‚ùå Erreur avec le mod√®le ultra-l√©ger : {e}")
            st.info("üîÑ Basculement vers le pipeline basique...")
            try:
                return load_basic_pipeline()
            except Exception as e2:
                st.error(f"‚ùå Erreur avec le pipeline basique : {e2}")
                return None, None
    
    # Strat√©gies pour environnement local avec plus de m√©moire
    if torch.cuda.is_available():
        st.info("üöÄ GPU d√©tect√© - Tentative de chargement Gemma 3n")
        strategies = [load_gemma_full, load_conservative, load_ultra_lightweight_for_hf_spaces, load_basic_pipeline]
    else:
        st.warning("GPU non disponible, utilisation du CPU")
        strategies = [load_conservative, load_ultra_lightweight_for_hf_spaces, load_basic_pipeline]
    
    for i, strategy in enumerate(strategies):
        try:
            st.info(f"üîÑ Tentative {i+1}/{len(strategies)} : {strategy.__name__}")
            model, processor = strategy()
            if model is not None:
                return model, processor
        except Exception as e:
            st.warning(f"‚ö†Ô∏è √âchec de la strat√©gie {strategy.__name__} : {e}")
            continue
    
    st.error("‚ùå Toutes les strat√©gies de chargement ont √©chou√©")
    return None, None

def load_ultra_lightweight_for_hf_spaces():
    """Charge Gemma 3B (plus l√©ger que Gemma 3n) pour HF Spaces"""
    st.info("ü™∂ Chargement de Gemma 3B pour HF Spaces...")
    
    # Nettoyer la m√©moire
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    try:
        # Importer les modules n√©cessaires
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        # Mod√®le Gemma 3B (plus l√©ger que Gemma 3n E4B)
        model_id = "google/gemma-3b-it"
        
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float32,
            low_cpu_mem_usage=True,
            device_map="cpu",
            trust_remote_code=True
        )
        
        # Cr√©er un processeur simple
        processor = SimpleTextProcessor(tokenizer)
        
        # Stocker dans session_state
        st.session_state.model = model
        st.session_state.processor = processor
        st.session_state.tokenizer = tokenizer
        st.session_state.model_loaded = True
        st.session_state.model_status = "Charg√© (Gemma 3B HF Spaces)"
        st.session_state.model_load_time = time.time()
        st.session_state.is_gemma_3b = True
        
        st.success("‚úÖ Gemma 3B charg√© avec succ√®s pour HF Spaces !")
        return model, processor
        
    except Exception as e:
        st.error(f"‚ùå Erreur lors du chargement Gemma 3B : {e}")
        return None, None

def load_basic_pipeline():
    """Charge un pipeline basique sans mod√®le de vision"""
    st.info("üîß Chargement du pipeline basique...")
    
    try:
        # Pipeline simple pour analyse de texte
        from transformers import pipeline
        
        classifier = pipeline(
            "text-classification",
            model="distilbert/distilbert-base-uncased",
            device=-1  # CPU
        )
        
        # Cr√©er un processeur simple
        processor = SimpleTextProcessor(None)
        
        # Stocker dans session_state
        st.session_state.classifier = classifier
        st.session_state.processor = processor
        st.session_state.model_loaded = True
        st.session_state.model_status = "Charg√© (Pipeline basique)"
        st.session_state.model_load_time = time.time()
        st.session_state.is_basic_pipeline = True
        
        st.success("‚úÖ Pipeline basique charg√© avec succ√®s !")
        return classifier, processor
        
    except Exception as e:
        st.error(f"‚ùå Erreur lors du chargement du pipeline basique : {e}")
        return None, None

def load_gemma_full():
    """Charge le mod√®le Gemma 3n complet (pour environnement local uniquement)"""
    st.info("üöÄ Chargement du mod√®le Gemma 3n complet...")
    
    # Nettoyer la m√©moire
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    try:
        model_id = "google/gemma-3n-E4B-it"
        processor = AutoProcessor.from_pretrained(model_id)
        
        model = Gemma3nForConditionalGeneration.from_pretrained(
            model_id,
            torch_dtype=torch.float16,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            device_map="auto"
        )
        
        # Stocker dans session_state
        st.session_state.model = model
        st.session_state.processor = processor
        st.session_state.model_loaded = True
        st.session_state.model_status = "Charg√© (Gemma 3n complet)"
        st.session_state.model_load_time = time.time()
        st.session_state.is_gemma_full = True
        
        st.success("‚úÖ Mod√®le Gemma 3n complet charg√© avec succ√®s !")
        return model, processor
        
    except Exception as e:
        st.error(f"‚ùå Erreur lors du chargement Gemma complet : {e}")
        return None, None

def load_conservative():
    """Charge le mod√®le avec des param√®tres conservateurs"""
    st.info("üîÑ Chargement conservateur...")
    
    # Nettoyer la m√©moire
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    try:
        model_id = "google/gemma-3n-E4B-it"
        processor = AutoProcessor.from_pretrained(model_id)
        
        model = Gemma3nForConditionalGeneration.from_pretrained(
            model_id,
            torch_dtype=torch.float32,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            device_map="cpu"
        )
        
        # Stocker dans session_state
        st.session_state.model = model
        st.session_state.processor = processor
        st.session_state.model_loaded = True
        st.session_state.model_status = "Charg√© (Conservateur)"
        st.session_state.model_load_time = time.time()
        st.session_state.is_conservative = True
        
        st.success("‚úÖ Mod√®le charg√© avec succ√®s (mode conservateur) !")
        return model, processor
        
    except Exception as e:
        st.error(f"‚ùå Erreur lors du chargement conservateur : {e}")
        return None, None

class SimpleTextProcessor:
    """Processeur simple pour les mod√®les l√©gers"""
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
    
    def __call__(self, text, images=None, return_tensors="pt", **kwargs):
        if self.tokenizer:
            return self.tokenizer(text, return_tensors=return_tensors, **kwargs)
        else:
            return {"input_ids": torch.tensor([[1, 2, 3]]), "attention_mask": torch.tensor([[1, 1, 1]])}
    
    def decode(self, tokens, skip_special_tokens=True):
        if self.tokenizer:
            return self.tokenizer.decode(tokens, skip_special_tokens=skip_special_tokens)
        else:
            return "Analyse basique disponible"

def analyze_image_multilingual(image, prompt=""):
    """Analyse une image avec le mod√®le disponible (Gemma 3n ou mod√®le alternatif)."""
    if not st.session_state.model_loaded:
        if not restore_model_from_cache():
            st.warning("Mod√®le non charg√©. Veuillez le charger via les r√©glages avant d'analyser.")
            return "‚ùå Mod√®le non charg√©. Veuillez d'abord charger le mod√®le dans les r√©glages."
        else:
            st.info("Mod√®le restaur√© depuis le cache pour l'analyse.")

    model = st.session_state.model
    if not model:
        return "‚ùå Mod√®le non disponible. Veuillez recharger le mod√®le."
    
    # D√©tecter le type de mod√®le charg√©
    is_gemma_3b = getattr(st.session_state, 'is_gemma_3b', False)
    is_basic_pipeline = getattr(st.session_state, 'is_basic_pipeline', False)
    is_gemma_full = getattr(st.session_state, 'is_gemma_full', False)
    is_conservative = getattr(st.session_state, 'is_conservative', False)
    
    if is_gemma_3b:
        return analyze_image_with_gemma3b_and_gemini(image, prompt)
    elif is_basic_pipeline:
        return analyze_image_with_basic_pipeline(image, prompt)
    elif is_gemma_full or is_conservative:
        return analyze_image_with_gemma_model(image, prompt)
    else:
        # Fallback vers l'analyse basique
        return analyze_image_with_basic_pipeline(image, prompt)

def analyze_image_with_gemma_model(image, prompt=""):
    """Analyse une image avec le mod√®le Gemma 3n E4B IT complet."""
    processor = st.session_state.processor
    model = st.session_state.model

    try:
        # Pr√©parer le prompt textuel pour Gemma 3n
        if st.session_state.language == "fr":
            if prompt:
                text_prompt = f"""
Tu es un expert en pathologie v√©g√©tale. Analyse cette image de plante et fournis un diagnostic pr√©cis.

**Question sp√©cifique :** {prompt}

**Instructions :**
1. **Diagnostic pr√©cis** : Identifie la maladie sp√©cifique avec son nom scientifique
2. **Causes** : Explique les causes probables (champignons, bact√©ries, virus, carences, etc.)
3. **Sympt√¥mes d√©taill√©s** : Liste tous les sympt√¥mes observables dans l'image
4. **Traitement sp√©cifique** : Donne des recommandations de traitement pr√©cises
5. **Actions pr√©ventives** : Conseils pour √©viter la propagation
6. **Urgence** : Indique si c'est urgent ou non

**Format de r√©ponse :**
## üîç **Diagnostic Pr√©cis**
[Nom de la maladie et causes]

## üìã **Sympt√¥mes D√©taill√©s**
[Liste des sympt√¥mes observ√©s]

## üíä **Traitement Recommand√©**
[Actions sp√©cifiques √† entreprendre]

## üõ°Ô∏è **Actions Pr√©ventives**
[Mesures pour √©viter la propagation]

## ‚ö†Ô∏è **Niveau d'Urgence**
[Urgent/Mod√©r√©/Faible]

R√©ponds de mani√®re structur√©e et pr√©cise.
"""
            else:
                text_prompt = """
Tu es un expert en pathologie v√©g√©tale. Analyse cette image de plante et fournis un diagnostic pr√©cis.

**Instructions :**
1. **Diagnostic pr√©cis** : Identifie la maladie sp√©cifique avec son nom scientifique
2. **Causes** : Explique les causes probables (champignons, bact√©ries, virus, carences, etc.)
3. **Sympt√¥mes d√©taill√©s** : Liste tous les sympt√¥mes observables dans l'image
4. **Traitement sp√©cifique** : Donne des recommandations de traitement pr√©cises
5. **Actions pr√©ventives** : Conseils pour √©viter la propagation
6. **Urgence** : Indique si c'est urgent ou non

**Format de r√©ponse :**
## üîç **Diagnostic Pr√©cis**
[Nom de la maladie et causes]

## üìã **Sympt√¥mes D√©taill√©s**
[Liste des sympt√¥mes observ√©s]

## üíä **Traitement Recommand√©**
[Actions sp√©cifiques √† entreprendre]

## üõ°Ô∏è **Actions Pr√©ventives**
[Mesures pour √©viter la propagation]

## ‚ö†Ô∏è **Niveau d'Urgence**
[Urgent/Mod√©r√©/Faible]

R√©ponds de mani√®re structur√©e et pr√©cise.
"""
        else:
            if prompt:
                text_prompt = f"""
You are an expert in plant pathology. Analyze this plant image and provide a precise diagnosis.

**Specific Question:** {prompt}

**Instructions:**
1. **Precise Diagnosis**: Identify the specific disease with its scientific name
2. **Causes**: Explain probable causes (fungi, bacteria, viruses, deficiencies, etc.)
3. **Detailed Symptoms**: List all observable symptoms in the image
4. **Specific Treatment**: Give precise treatment recommendations
5. **Preventive Actions**: Advice to prevent spread
6. **Urgency**: Indicate if urgent or not

**Response Format:**
## üîç **Precise Diagnosis**
[Disease name and causes]

## üìã **Detailed Symptoms**
[List of observed symptoms]

## üíä **Recommended Treatment**
[Specific actions to take]

## üõ°Ô∏è **Preventive Actions**
[Measures to prevent spread]

## ‚ö†Ô∏è **Urgency Level**
[Urgent/Moderate/Low]

Respond in a structured and precise manner.
"""
            else:
                text_prompt = """
You are an expert in plant pathology. Analyze this plant image and provide a precise diagnosis.

**Instructions:**
1. **Precise Diagnosis**: Identify the specific disease with its scientific name
2. **Causes**: Explain probable causes (fungi, bacteria, viruses, deficiencies, etc.)
3. **Detailed Symptoms**: List all observable symptoms in the image
4. **Specific Treatment**: Give precise treatment recommendations
5. **Preventive Actions**: Advice to prevent spread
6. **Urgency**: Indicate if urgent or not

**Response Format:**
## üîç **Precise Diagnosis**
[Disease name and causes]

## üìã **Detailed Symptoms**
[List of observed symptoms]

## üíä **Recommended Treatment**
[Specific actions to take]

## üõ°Ô∏è **Preventive Actions**
[Measures to prevent spread]

## ‚ö†Ô∏è **Urgency Level**
[Urgent/Moderate/Low]

Respond in a structured and precise manner.
"""
        
        # Utiliser le format correct pour Gemma 3n avec le token <image>
        final_prompt = f"<image>\n{text_prompt}"
        
        # √âtape 2: Utiliser le processeur pour combiner texte et image
        try:
            inputs = processor(text=final_prompt, images=image, return_tensors="pt").to(model.device)
        except Exception as e:
            if "Number of images does not match number of special image tokens" in str(e):
                # Fallback: essayer sans le token <image>
                inputs = processor(text=text_prompt, images=image, return_tensors="pt").to(model.device)
            else:
                raise e
        
        input_len = inputs["input_ids"].shape[-1]
        
        # √âtape 3: G√©n√©rer la r√©ponse
        with torch.inference_mode():
            generation = model.generate(
                **inputs,
                max_new_tokens=500,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )
            response = processor.decode(generation[0][input_len:], skip_special_tokens=True)

        final_response = response.strip()
        
        if st.session_state.language == "fr":
            return f"""
## üß† **Analyse par Gemma 3n E4B IT**

{final_response}
"""
        else:
            return f"""
## üß† **Analysis by Gemma 3n E4B IT**

{final_response}
"""
            
    except Exception as e:
        error_message = str(e)
        if "403" in error_message or "Forbidden" in error_message:
            return "‚ùå Erreur 403 - Acc√®s refus√©. Veuillez v√©rifier votre jeton Hugging Face (HF_TOKEN) et les quotas."
        elif "Number of images does not match number of special image tokens" in error_message:
            return "‚ùå Erreur : Le mod√®le n'a pas pu lier l'image au texte. Assurez-vous que la structure du prompt est correcte."
        else:
            return f"‚ùå Erreur lors de l'analyse d'image : {e}"

def analyze_image_with_gemma3b_and_gemini(image, prompt=""):
    """Analyse une image avec Gemma 3B + Gemini pour l'interpr√©tation."""
    model = st.session_state.model
    processor = st.session_state.processor
    
    try:
        # Convertir l'image en description textuelle basique
        image_description = f"Image de plante avec dimensions {image.size[0]}x{image.size[1]} pixels"
        
        # Pr√©parer le prompt pour Gemma 3B
        if st.session_state.language == "fr":
            if prompt:
                text_prompt = f"""
Tu es un expert en pathologie v√©g√©tale. Analyse cette description d'image de plante et fournis un diagnostic.

**Description de l'image :** {image_description}
**Question sp√©cifique :** {prompt}

**Instructions :**
1. **Diagnostic g√©n√©ral** : Donne des conseils g√©n√©raux sur les maladies v√©g√©tales
2. **Recommandations** : Conseils de base pour l'identification et le traitement
3. **Actions pr√©ventives** : Mesures g√©n√©rales de pr√©vention

**Note :** Cette analyse est bas√©e sur une description textuelle. Pour une analyse pr√©cise, utilisez un mod√®le sp√©cialis√© en vision.
"""
            else:
                text_prompt = f"""
Tu es un expert en pathologie v√©g√©tale. Analyse cette description d'image de plante et fournis un diagnostic.

**Description de l'image :** {image_description}

**Instructions :**
1. **Diagnostic g√©n√©ral** : Donne des conseils g√©n√©raux sur les maladies v√©g√©tales
2. **Recommandations** : Conseils de base pour l'identification et le traitement
3. **Actions pr√©ventives** : Mesures g√©n√©rales de pr√©vention

**Note :** Cette analyse est bas√©e sur une description textuelle. Pour une analyse pr√©cise, utilisez un mod√®le sp√©cialis√© en vision.
"""
        else:
            if prompt:
                text_prompt = f"""
You are an expert in plant pathology. Analyze this image description and provide a diagnosis.

**Image description:** {image_description}
**Specific question:** {prompt}

**Instructions:**
1. **General diagnosis**: Provide general advice on plant diseases
2. **Recommendations**: Basic guidance for identification and treatment
3. **Preventive actions**: General prevention measures

**Note:** This analysis is based on a text description. For precise analysis, use a specialized vision model.
"""
            else:
                text_prompt = f"""
You are an expert in plant pathology. Analyze this image description and provide a diagnosis.

**Image description:** {image_description}

**Instructions:**
1. **General diagnosis**: Provide general advice on plant diseases
2. **Recommendations**: Basic guidance for identification and treatment
3. **Preventive actions**: General prevention measures

**Note:** This analysis is based on a text description. For precise analysis, use a specialized vision model.
"""
        
        # G√©n√©rer la r√©ponse avec Gemma 3B
        inputs = processor(text_prompt, return_tensors="pt", truncation=True, max_length=512)
        
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=300,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )
        
        gemma_response = processor.decode(outputs[0], skip_special_tokens=True)
        gemma_response = gemma_response.replace(text_prompt, "").strip()
        
        # Utiliser Gemini pour am√©liorer l'interpr√©tation
        if gemini_model:
            try:
                gemini_prompt = f"""
Tu es un expert en pathologie v√©g√©tale. Am√©liore et structure cette analyse de diagnostic de plante :

**Analyse brute de Gemma 3B :**
{gemma_response}

**Instructions :**
1. **Structure** l'analyse de mani√®re claire et professionnelle
2. **Ajoute** des d√©tails techniques si n√©cessaire
3. **Am√©liore** la pr√©sentation avec des sections bien d√©finies
4. **V√©rifie** la coh√©rence et la pr√©cision des informations

**Format de r√©ponse :**
## üîç **Diagnostic Pr√©cis**
## üìã **Sympt√¥mes D√©taill√©s**
## üíä **Traitement Recommand√©**
## üõ°Ô∏è **Actions Pr√©ventives**
"""
                
                gemini_response = gemini_model.generate_content(gemini_prompt)
                final_response = gemini_response.text
                
                if st.session_state.language == "fr":
                    return f"""
## ü§ñ **Analyse Gemma 3B + Gemini**

{final_response}

**üîß Technologies utilis√©es :**
- **Gemma 3B** : Analyse initiale et diagnostic
- **Gemini** : Am√©lioration et structuration de l'interpr√©tation
"""
                else:
                    return f"""
## ü§ñ **Gemma 3B + Gemini Analysis**

{final_response}

**üîß Technologies used:**
- **Gemma 3B** : Initial analysis and diagnosis
- **Gemini** : Enhancement and structuring of interpretation
"""
                    
            except Exception as gemini_error:
                st.warning(f"‚ö†Ô∏è Erreur Gemini, utilisation de la r√©ponse Gemma 3B brute : {gemini_error}")
                final_response = gemma_response
        else:
            final_response = gemma_response
        
        if st.session_state.language == "fr":
            return f"""
## ü™∂ **Analyse par Gemma 3B**

{final_response}

**‚ö†Ô∏è Note :** Cette analyse utilise Gemma 3B (version all√©g√©e) optimis√©e pour HF Spaces.
"""
        else:
            return f"""
## ü™∂ **Analysis by Gemma 3B**

{final_response}

**‚ö†Ô∏è Note:** This analysis uses Gemma 3B (lightweight version) optimized for HF Spaces.
"""
            
    except Exception as e:
        return f"‚ùå Erreur lors de l'analyse avec Gemma 3B : {e}"

def analyze_image_with_ultra_lightweight_model(image, prompt=""):
    """Analyse une image avec un mod√®le ultra-l√©ger (texte uniquement)."""
    model = st.session_state.model
    processor = st.session_state.processor
    
    try:
        # Convertir l'image en description textuelle basique
        image_description = f"Image de plante avec dimensions {image.size[0]}x{image.size[1]} pixels"
        
        # Pr√©parer le prompt
        if st.session_state.language == "fr":
            if prompt:
                text_prompt = f"""
Tu es un expert en pathologie v√©g√©tale. Analyse cette description d'image de plante et fournis un diagnostic.

**Description de l'image :** {image_description}
**Question sp√©cifique :** {prompt}

**Instructions :**
1. **Diagnostic g√©n√©ral** : Donne des conseils g√©n√©raux sur les maladies v√©g√©tales
2. **Recommandations** : Conseils de base pour l'identification et le traitement
3. **Actions pr√©ventives** : Mesures g√©n√©rales de pr√©vention

**Note :** Cette analyse est bas√©e sur une description textuelle. Pour une analyse pr√©cise, utilisez un mod√®le sp√©cialis√© en vision.
"""
            else:
                text_prompt = f"""
Tu es un expert en pathologie v√©g√©tale. Analyse cette description d'image de plante et fournis un diagnostic.

**Description de l'image :** {image_description}

**Instructions :**
1. **Diagnostic g√©n√©ral** : Donne des conseils g√©n√©raux sur les maladies v√©g√©tales
2. **Recommandations** : Conseils de base pour l'identification et le traitement
3. **Actions pr√©ventives** : Mesures g√©n√©rales de pr√©vention

**Note :** Cette analyse est bas√©e sur une description textuelle. Pour une analyse pr√©cise, utilisez un mod√®le sp√©cialis√© en vision.
"""
        else:
            if prompt:
                text_prompt = f"""
You are an expert in plant pathology. Analyze this image description and provide a diagnosis.

**Image description:** {image_description}
**Specific question:** {prompt}

**Instructions:**
1. **General diagnosis**: Provide general advice on plant diseases
2. **Recommendations**: Basic guidance for identification and treatment
3. **Preventive actions**: General prevention measures

**Note:** This analysis is based on a text description. For precise analysis, use a specialized vision model.
"""
            else:
                text_prompt = f"""
You are an expert in plant pathology. Analyze this image description and provide a diagnosis.

**Image description:** {image_description}

**Instructions:**
1. **General diagnosis**: Provide general advice on plant diseases
2. **Recommendations**: Basic guidance for identification and treatment
3. **Preventive actions**: General prevention measures

**Note:** This analysis is based on a text description. For precise analysis, use a specialized vision model.
"""
        
        # G√©n√©rer la r√©ponse avec le mod√®le ultra-l√©ger
        inputs = processor(text_prompt, return_tensors="pt", truncation=True, max_length=512)
        
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=300,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )
        
        response = processor.decode(outputs[0], skip_special_tokens=True)
        final_response = response.replace(text_prompt, "").strip()
        
        if st.session_state.language == "fr":
            return f"""
## ü™∂ **Analyse par Mod√®le Ultra-L√©ger**

{final_response}

**‚ö†Ô∏è Note :** Cette analyse utilise un mod√®le ultra-l√©ger optimis√© pour HF Spaces. Pour une analyse d'image pr√©cise, utilisez un environnement avec plus de m√©moire.
"""
        else:
            return f"""
## ü™∂ **Analysis by Ultra-Lightweight Model**

{final_response}

**‚ö†Ô∏è Note:** This analysis uses an ultra-lightweight model optimized for HF Spaces. For precise image analysis, use an environment with more memory.
"""
            
    except Exception as e:
        return f"‚ùå Erreur lors de l'analyse avec le mod√®le ultra-l√©ger : {e}"

def analyze_image_with_lightweight_model(image, prompt=""):
    """Analyse une image avec le mod√®le l√©ger (texte uniquement + API externe pour l'image)."""
    model = st.session_state.model
    tokenizer = st.session_state.tokenizer
    
    try:
        # Convertir l'image en description textuelle basique
        image_description = f"Image de plante avec dimensions {image.size[0]}x{image.size[1]} pixels"
        
        # Pr√©parer le prompt
        if st.session_state.language == "fr":
            if prompt:
                text_prompt = f"""
Tu es un expert en pathologie v√©g√©tale. Analyse cette description d'image de plante et fournis un diagnostic.

**Description de l'image :** {image_description}
**Question sp√©cifique :** {prompt}

**Instructions :**
1. **Diagnostic g√©n√©ral** : Donne des conseils g√©n√©raux sur les maladies v√©g√©tales
2. **Recommandations** : Conseils de base pour l'identification et le traitement
3. **Actions pr√©ventives** : Mesures g√©n√©rales de pr√©vention

**Note :** Cette analyse est bas√©e sur une description textuelle. Pour une analyse pr√©cise, utilisez un mod√®le sp√©cialis√© en vision.
"""
            else:
                text_prompt = f"""
Tu es un expert en pathologie v√©g√©tale. Analyse cette description d'image de plante et fournis un diagnostic.

**Description de l'image :** {image_description}

**Instructions :**
1. **Diagnostic g√©n√©ral** : Donne des conseils g√©n√©raux sur les maladies v√©g√©tales
2. **Recommandations** : Conseils de base pour l'identification et le traitement
3. **Actions pr√©ventives** : Mesures g√©n√©rales de pr√©vention

**Note :** Cette analyse est bas√©e sur une description textuelle. Pour une analyse pr√©cise, utilisez un mod√®le sp√©cialis√© en vision.
"""
        else:
            if prompt:
                text_prompt = f"""
You are an expert in plant pathology. Analyze this image description and provide a diagnosis.

**Image description:** {image_description}
**Specific question:** {prompt}

**Instructions:**
1. **General diagnosis**: Provide general advice on plant diseases
2. **Recommendations**: Basic guidance for identification and treatment
3. **Preventive actions**: General prevention measures

**Note:** This analysis is based on a text description. For precise analysis, use a specialized vision model.
"""
            else:
                text_prompt = f"""
You are an expert in plant pathology. Analyze this image description and provide a diagnosis.

**Image description:** {image_description}

**Instructions:**
1. **General diagnosis**: Provide general advice on plant diseases
2. **Recommendations**: Basic guidance for identification and treatment
3. **Preventive actions**: General prevention measures

**Note:** This analysis is based on a text description. For precise analysis, use a specialized vision model.
"""
        
        # G√©n√©rer la r√©ponse avec le mod√®le l√©ger
        inputs = tokenizer(text_prompt, return_tensors="pt", truncation=True, max_length=512)
        
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=300,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        final_response = response.replace(text_prompt, "").strip()
        
        if st.session_state.language == "fr":
            return f"""
## üß† **Analyse par Mod√®le L√©ger**

{final_response}

**‚ö†Ô∏è Note :** Cette analyse utilise un mod√®le l√©ger optimis√© pour le texte. Pour une analyse d'image pr√©cise, utilisez un environnement avec plus de m√©moire.
"""
        else:
            return f"""
## üß† **Analysis by Lightweight Model**

{final_response}

**‚ö†Ô∏è Note:** This analysis uses a lightweight model optimized for text. For precise image analysis, use an environment with more memory.
"""
            
    except Exception as e:
        return f"‚ùå Erreur lors de l'analyse avec le mod√®le l√©ger : {e}"

def analyze_image_with_basic_pipeline(image, prompt=""):
    """Analyse avec le pipeline basique (fonctionnalit√©s tr√®s limit√©es)."""
    
    try:
        # Description basique de l'image
        image_description = f"Plant image with dimensions {image.size[0]}x{image.size[1]} pixels"
        
        if st.session_state.language == "fr":
            return f"""
## üß† **Analyse Basique**

**Description de l'image :** {image_description}

**‚ö†Ô∏è Limitation :** Le mod√®le actuel ne peut analyser que du texte. Pour une analyse d'image compl√®te, utilisez un environnement avec plus de m√©moire.

**Conseils g√©n√©raux :**
- V√©rifiez les sympt√¥mes visibles sur les feuilles, tiges et racines
- Consultez un expert en pathologie v√©g√©tale pour un diagnostic pr√©cis
- Prenez des photos d√©taill√©es sous diff√©rents angles
- Notez les conditions environnementales (humidit√©, temp√©rature, etc.)
"""
        else:
            return f"""
## üß† **Basic Analysis**

**Image description:** {image_description}

**‚ö†Ô∏è Limitation:** The current model can only analyze text. For complete image analysis, use an environment with more memory.

**General advice:**
- Check visible symptoms on leaves, stems, and roots
- Consult a plant pathology expert for precise diagnosis
- Take detailed photos from different angles
- Note environmental conditions (humidity, temperature, etc.)
"""
    except Exception as e:
        return f"‚ùå Erreur lors de l'analyse basique : {e}"

def analyze_text_multilingual(text):
    """Analyse un texte avec le mod√®le Gemma 3n E4B IT"""
    # V√©rification compl√®te du mod√®le avec cache
    if not st.session_state.model_loaded and not check_model_persistence():
        # Essayer de restaurer depuis le cache
        if restore_model_from_cache():
            st.info("üîÑ Mod√®le restaur√© depuis le cache pour l'analyse")
        else:
            return "‚ùå Mod√®le non charg√©. Veuillez le charger dans les r√©glages."
    
    # V√©rifier que le mod√®le et le processeur sont disponibles
    if not hasattr(st.session_state, 'model') or st.session_state.model is None:
        # Essayer de restaurer depuis le cache
        if restore_model_from_cache():
            st.info("üîÑ Mod√®le restaur√© depuis le cache")
        else:
            st.session_state.model_loaded = False
            return "‚ùå Mod√®le perdu en m√©moire. Veuillez recharger le mod√®le."
    
    if not hasattr(st.session_state, 'processor') or st.session_state.processor is None:
        st.session_state.model_loaded = False
        return "‚ùå Processeur perdu en m√©moire. Veuillez recharger le mod√®le."
    
    try:
        model, processor = st.session_state.model, st.session_state.processor
        
        # V√©rification finale
        if not model or not processor:
            st.session_state.model_loaded = False
            return "‚ùå Mod√®le Gemma non disponible. Veuillez recharger le mod√®le."
        
        if st.session_state.language == "fr":
            prompt = f"Tu es un assistant agricole expert. Analyse ce probl√®me : {text}"
        else:
            prompt = f"You are an expert agricultural assistant. Analyze this problem: {text}"
        
        # Pr√©parer les messages
        messages = [
            {
                "role": "user",
                "content": [{"type": "text", "text": prompt}]
            }
        ]
        
        # Traiter les entr√©es
        inputs = processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
        )
        
        # G√©rer le device de mani√®re s√©curis√©e
        device = getattr(model, 'device', 'cpu')
        if hasattr(inputs, 'to'):
            inputs = inputs.to(device)
        
        input_len = inputs["input_ids"].shape[-1]
        
        # G√©n√©rer la r√©ponse
        with torch.inference_mode():
            generation = model.generate(
                **inputs,
                max_new_tokens=300,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )
            generation = generation[0][input_len:]
        
        response = processor.decode(generation, skip_special_tokens=True)
        return response
        
    except Exception as e:
        return f"‚ùå Erreur lors de l'analyse de texte : {e}"

def analyze_with_gemini(gemma_description, image_info=""):
    """
    Analyse approfondie avec Gemini API pour diagnostic pr√©cis
    """
    if not gemini_model:
        return "‚ùå Gemini API non configur√©e. V√©rifiez votre cl√© API Google."
    
    try:
        if st.session_state.language == "fr":
            prompt = f"""
Tu es un expert en pathologie v√©g√©tale et en agriculture. Analyse cette description d'une plante malade et fournis un diagnostic pr√©cis.

**Description de Gemma :**
{gemma_description}

**Informations suppl√©mentaires :**
{image_info}

**Instructions :**
1. **Diagnostic pr√©cis** : Identifie la maladie sp√©cifique avec son nom scientifique
2. **Causes** : Explique les causes probables (champignons, bact√©ries, virus, carences, etc.)
3. **Sympt√¥mes d√©taill√©s** : Liste tous les sympt√¥mes observables
4. **Traitement sp√©cifique** : Donne des recommandations de traitement pr√©cises
5. **Actions pr√©ventives** : Conseils pour √©viter la propagation
6. **Urgence** : Indique si c'est urgent ou non

**Format de r√©ponse :**
## üîç **Diagnostic Pr√©cis**
[Nom de la maladie et causes]

## üìã **Sympt√¥mes D√©taill√©s**
[Liste des sympt√¥mes]

## üíä **Traitement Recommand√©**
[Actions sp√©cifiques √† entreprendre]

## üõ°Ô∏è **Actions Pr√©ventives**
[Mesures pour √©viter la propagation]

## ‚ö†Ô∏è **Niveau d'Urgence**
[Urgent/Mod√©r√©/Faible]

R√©ponds de mani√®re structur√©e et pr√©cise.
"""
        else:
            prompt = f"""
You are an expert in plant pathology and agriculture. Analyze this description of a diseased plant and provide a precise diagnosis.

**Gemma's Description:**
{gemma_description}

**Additional Information:**
{image_info}

**Instructions:**
1. **Precise Diagnosis**: Identify the specific disease with its scientific name
2. **Causes**: Explain probable causes (fungi, bacteria, viruses, deficiencies, etc.)
3. **Detailed Symptoms**: List all observable symptoms
4. **Specific Treatment**: Give precise treatment recommendations
5. **Preventive Actions**: Advice to prevent spread
6. **Urgency**: Indicate if urgent or not

**Response Format:**
## üîç **Precise Diagnosis**
[Disease name and causes]

## üìã **Detailed Symptoms**
[List of symptoms]

## üíä **Recommended Treatment**
[Specific actions to take]

## üõ°Ô∏è **Preventive Actions**
[Measures to prevent spread]

## ‚ö†Ô∏è **Urgency Level**
[Urgent/Moderate/Low]

Respond in a structured and precise manner.
"""
        
        response = gemini_model.generate_content(prompt)
        return response.text
        
    except Exception as e:
        return f"‚ùå Erreur lors de l'analyse Gemini : {e}"

# Interface principale
st.title(t("title"))
st.markdown(t("subtitle"))

# V√©rification automatique de la persistance du mod√®le au d√©marrage
if 'model_loaded' not in st.session_state:
    st.session_state.model_loaded = False
if 'model_status' not in st.session_state:
    st.session_state.model_status = "Non charg√©"

# V√©rification automatique de la persistance
if st.session_state.model_loaded:
    # V√©rifier si le mod√®le est toujours disponible
    if not check_model_persistence():
        # Essayer de restaurer depuis le cache
        if restore_model_from_cache():
            st.success("üîÑ Mod√®le restaur√© automatiquement depuis le cache")
        else:
            st.warning("‚ö†Ô∏è Mod√®le perdu en m√©moire - rechargement n√©cessaire")
            st.session_state.model_loaded = False
            st.session_state.model_status = "Non charg√©"

# Sidebar pour la configuration
with st.sidebar:
    st.header(t("config_title"))
    
    # S√©lecteur de langue
    st.subheader("üåê S√©lection de langue / Language Selection")
    language = st.selectbox(
        "Language / Langue",
        ["Fran√ßais", "English"],
        index=0 if st.session_state.language == "fr" else 1
    )
    
    if language == "Fran√ßais":
        st.session_state.language = "fr"
    else:
        st.session_state.language = "en"
    
    # V√©rification automatique de la persistance du mod√®le
    if not st.session_state.model_loaded and not check_model_persistence():
        # Essayer de restaurer depuis le cache
        if restore_model_from_cache():
            st.success("‚úÖ Mod√®le restaur√© depuis le cache")
        else:
            st.info("üîÑ Mod√®le non trouv√© en cache - chargement n√©cessaire")
    
    # Chargement du mod√®le
    if st.button(t("load_model"), type="primary"):
        with st.spinner("Chargement du mod√®le..." if st.session_state.language == "fr" else "Loading model..."):
            try:
                model, processor = load_model()
                if model and processor:
                    # Stocker le mod√®le dans la session avec v√©rification
                    st.session_state.model = model
                    st.session_state.processor = processor
                    st.session_state.model_loaded = True
                    st.session_state.model_status = t("loaded")
                    st.session_state.model_load_time = time.time()
                    
                    # Forcer la persistance en cache global
                    if force_model_persistence():
                        if is_local:
                            st.success("‚úÖ Mod√®le Gemma 3n E4B IT charg√© et persist√© avec succ√®s (mode local) !")
                            st.info("üîÑ Le mod√®le local est maintenant sauvegard√© en cache pour la persistance")
                        else:
                            st.success("‚úÖ Mod√®le Gemma 3n E4B IT charg√© et persist√© avec succ√®s (mode Hugging Face) !")
                            st.info("üîÑ Le mod√®le en ligne est maintenant sauvegard√© en cache pour la persistance")
                    else:
                        st.warning("‚ö†Ô∏è Mod√®le charg√© mais probl√®me de persistance d√©tect√©")
                        
                else:
                    st.session_state.model_loaded = False
                    st.session_state.model_status = t("error")
                    st.error("√âchec du chargement du mod√®le" if st.session_state.language == "fr" else "Model loading failed")
            except Exception as e:
                st.session_state.model_loaded = False
                st.session_state.model_status = t("error")
                st.error(f"Erreur lors du chargement : {e}")
    
    # Affichage du statut avec indicateur de persistance
    status_emoji = "‚úÖ" if st.session_state.model_loaded else "‚ùå"
    persistence_emoji = "üîí" if st.session_state.model_persistence_check else "‚ö†Ô∏è"
    
    st.info(f"{status_emoji} {t('model_status')} {st.session_state.model_status} {persistence_emoji}")
    
    # Indicateur de persistance
    if st.session_state.model_loaded:
        if st.session_state.model_persistence_check:
            st.success("üîí Mod√®le persist√© en cache - stable entre les sessions")
        else:
            st.warning("‚ö†Ô∏è Mod√®le charg√© mais pas encore persist√©")
    
    # V√©rification de la persistance du mod√®le
    if st.session_state.model_loaded or check_model_persistence():
        # V√©rifier que le mod√®le est toujours disponible
        if hasattr(st.session_state, 'model') and st.session_state.model is not None:
            if hasattr(st.session_state, 'processor') and st.session_state.processor is not None:
                # D√©tecter l'environnement pour l'affichage
                is_local = os.path.exists("D:/Dev/model_gemma")
                
                # Afficher le statut de persistance
                if st.session_state.model_persistence_check:
                    if is_local:
                        st.success("‚úÖ Mod√®le Gemma 3n E4B IT charg√© et PERSISTANT (mode local - cache activ√©)")
                        st.info("üîÑ Le mod√®le local est sauvegard√© en cache et persiste entre les sessions")
                    else:
                        st.success("‚úÖ Mod√®le Gemma 3n E4B IT charg√© et PERSISTANT (mode Hugging Face - cache activ√©)")
                        st.info("ÔøΩÔøΩ Le mod√®le en ligne est sauvegard√© en cache et persiste entre les sessions")
                else:
                    if is_local:
                        st.success("‚úÖ Mod√®le Gemma 3n E4B IT charg√© (mode local)")
                        st.info("Le mod√®le local est pr√™t pour l'analyse d'images et de texte")
                    else:
                        st.success("‚úÖ Mod√®le Gemma 3n E4B IT charg√© (mode Hugging Face)")
                        st.info("Le mod√®le en ligne est pr√™t pour l'analyse d'images et de texte")
                
                # Diagnostic du mod√®le
                with st.expander("üîç Diagnostic du mod√®le"):
                    st.write(f"**Mode d'ex√©cution :** {'üè† Local (D:/Dev/model_gemma)' if is_local else 'üåê Hugging Face (en ligne)'}")
                    st.write(f"**Mod√®le charg√© :** {type(st.session_state.model).__name__}")
                    st.write(f"**Processeur charg√© :** {type(st.session_state.processor).__name__}")
                    st.write(f"**Device du mod√®le :** {st.session_state.model.device}")
                    st.write(f"**M√©moire utilis√©e :** {torch.cuda.memory_allocated() / 1024**3:.2f} GB" if torch.cuda.is_available() else "CPU uniquement")
                    if st.session_state.model_load_time:
                        load_time_str = time.strftime('%H:%M:%S', time.localtime(st.session_state.model_load_time))
                        st.write(f"**Heure de chargement :** {load_time_str}")
                    st.write(f"**Cache global :** {'‚úÖ Actif' if st.session_state.global_model_cache else '‚ùå Inactif'}")
                
                # Boutons de gestion
                col1, col2, col3 = st.columns(3)
                with col1:
                    if st.button("üîÑ Recharger le mod√®le", type="secondary"):
                        st.session_state.model_loaded = False
                        st.session_state.model = None
                        st.session_state.processor = None
                        st.session_state.global_model_cache.clear()
                        st.rerun()
                with col2:
                    if st.button("üíæ Forcer la persistance", type="secondary"):
                        if force_model_persistence():
                            st.success("‚úÖ Persistance forc√©e avec succ√®s")
                        else:
                            st.error("‚ùå √âchec de la persistance forc√©e")
                        st.rerun()
                with col3:
                    if st.button("üîç Debug Cache", type="secondary"):
                        st.write("=== DEBUG CACHE ===")
                        st.write(f"Session state keys: {list(st.session_state.keys())}")
                        st.write(f"Global cache keys: {list(st.session_state.global_model_cache.keys())}")
                        st.write(f"Model loaded: {st.session_state.model_loaded}")
                        st.write(f"Model in session: {hasattr(st.session_state, 'model') and st.session_state.model is not None}")
                        st.write(f"Model in cache: {'model' in st.session_state.global_model_cache and st.session_state.global_model_cache['model'] is not None}")
                        if 'model' in st.session_state.global_model_cache:
                            cached_model = st.session_state.global_model_cache['model']
                            st.write(f"Cached model type: {type(cached_model).__name__}")
                            st.write(f"Cached model has device: {hasattr(cached_model, 'device')}")
                        st.rerun()
            else:
                st.warning("‚ö†Ô∏è Processeur manquant - rechargement n√©cessaire")
                st.session_state.model_loaded = False
                st.session_state.model = None
        else:
            st.warning("‚ö†Ô∏è Mod√®le perdu en m√©moire - rechargement n√©cessaire")
            st.session_state.model_loaded = False
            st.session_state.processor = None
    else:
        # D√©tecter l'environnement pour l'affichage
        is_local = os.path.exists("D:/Dev/model_gemma")
        
        st.warning("‚ö†Ô∏è Mod√®le Gemma 3n E4B IT non charg√©")
        if is_local:
            st.info("Cliquez sur 'Charger le mod√®le' pour activer l'analyse (mode local)")
        else:
            st.info("Cliquez sur 'Charger le mod√®le' pour activer l'analyse (mode Hugging Face)")

# Onglets principaux
tab1, tab2, tab3, tab4 = st.tabs(t("tabs"))

with tab1:
    st.header(t("image_analysis_title"))
    st.markdown(t("image_analysis_desc"))
    
    # Options de capture d'image
    capture_option = st.radio(
        "Choisissez votre m√©thode :" if st.session_state.language == "fr" else "Choose your method:",
        ["üìÅ Upload d'image" if st.session_state.language == "fr" else "üìÅ Upload Image", 
         "üì∑ Capture par webcam" if st.session_state.language == "fr" else "üì∑ Webcam Capture"],
        horizontal=True
    )
    
    uploaded_file = None
    captured_image = None
    
    if capture_option == "üìÅ Upload d'image" or capture_option == "üìÅ Upload Image":
        try:
            uploaded_file = st.file_uploader(
                t("choose_image"), 
                type=['png', 'jpg', 'jpeg'],
                help="Formats accept√©s : PNG, JPG, JPEG (max 200MB)" if st.session_state.language == "fr" else "Accepted formats: PNG, JPG, JPEG (max 200MB)",
                accept_multiple_files=False,
                key="image_uploader"
            )
        except Exception as e:
            st.error(f"‚ùå Erreur lors de l'upload : {e}")
    
    else:  # Webcam capture
        st.markdown("**üì∑ Capture d'image par webcam**" if st.session_state.language == "fr" else "**üì∑ Webcam Image Capture**")
        st.info("üí° Positionnez votre plante malade devant la webcam et cliquez sur 'Prendre une photo'" if st.session_state.language == "fr" else "üí° Position your diseased plant in front of the webcam and click 'Take Photo'")
        
        captured_image = st.camera_input(
            "Prendre une photo de la plante" if st.session_state.language == "fr" else "Take a photo of the plant",
            key="webcam_capture"
        )
    
    # Traitement de l'image (upload ou webcam)
    image = None
    image_source = None
    
    if uploaded_file is not None:
        try:
            image = Image.open(uploaded_file)
            image_source = "upload"
        except Exception as e:
            st.error(f"‚ùå Erreur lors du traitement de l'image upload√©e : {e}")
            st.info("üí° Essayez avec une image diff√©rente ou un format diff√©rent (PNG, JPG, JPEG)")
    elif captured_image is not None:
        try:
            image = Image.open(captured_image)
            image_source = "webcam"
        except Exception as e:
            st.error(f"‚ùå Erreur lors du traitement de l'image captur√©e : {e}")
            st.info("üí° Essayez de reprendre la photo")
    
    if image is not None:
        try:
            # Redimensionner l'image si n√©cessaire
            original_size = image.size
            image, was_resized = resize_image_if_needed(image, max_size=(800, 800))
            
            col1, col2 = st.columns([1, 1])
            with col1:
                if image_source == "upload":
                    st.image(image, caption="Image upload√©e" if st.session_state.language == "fr" else "Uploaded Image", use_container_width=True)
                else:
                    st.image(image, caption="Image captur√©e par webcam" if st.session_state.language == "fr" else "Webcam Captured Image", use_container_width=True)
            
            with col2:
                st.markdown("**Informations de l'image :**")
                st.write(f"‚Ä¢ Format : {image.format}")
                st.write(f"‚Ä¢ Taille originale : {original_size[0]}x{original_size[1]} pixels")
                st.write(f"‚Ä¢ Taille actuelle : {image.size[0]}x{image.size[1]} pixels")
                st.write(f"‚Ä¢ Mode : {image.mode}")
                
                if was_resized:
                    st.warning("‚ö†Ô∏è L'image a √©t√© automatiquement redimensionn√©e pour optimiser le traitement")
            
            question = st.text_area(
                "Question sp√©cifique (optionnel) :",
                placeholder="Ex: Quelle est cette maladie ? Que faire pour la traiter ?",
                height=100
            )
            
            if st.button(t("analyze_button"), disabled=not st.session_state.model_loaded, type="primary"):
                if not st.session_state.model_loaded:
                    st.error("‚ùå Mod√®le Gemma non charg√©. Veuillez d'abord charger le mod√®le dans les r√©glages.")
                    st.info("üí° L'analyse d'image n√©cessite le mod√®le Gemma 3n E4B IT. Chargez-le dans les r√©glages.")
                else:
                    with st.spinner("üîç Analyse en cours..."):
                        result = analyze_image_multilingual(image, question)
                    
                    st.markdown(t("analysis_results"))
                    st.markdown("---")
                    st.markdown(result)
        except Exception as e:
            error_msg = str(e)
            if "403" in error_msg or "Forbidden" in error_msg:
                st.error("‚ùå Erreur 403 - Acc√®s refus√© lors du traitement de l'image")
                st.warning("üîí Cette erreur indique un probl√®me d'autorisation c√¥t√© serveur.")
                st.info("üí° Solutions possibles :")
                st.info("‚Ä¢ V√©rifiez les logs de votre espace Hugging Face")
                st.info("‚Ä¢ Essayez avec une image plus petite (< 1MB)")
                st.info("‚Ä¢ Rafra√Æchissez la page et r√©essayez")
                st.info("‚Ä¢ Contactez le support Hugging Face si le probl√®me persiste")
            else:
                st.error(f"‚ùå Erreur lors du traitement de l'image : {e}")
                st.info("üí° Essayez avec une image diff√©rente ou un format diff√©rent (PNG, JPG, JPEG)")

with tab2:
    st.header(t("text_analysis_title"))
    st.markdown(t("text_analysis_desc"))
    
    text_input = st.text_area(
        t("symptoms_desc"),
        placeholder="Ex: Mes tomates ont des taches brunes sur les feuilles et les fruits...",
        height=150
    )
    
    if st.button("üß† Analyser avec l'IA", disabled=not st.session_state.model_loaded, type="primary"):
        if not st.session_state.model_loaded:
            st.error("‚ùå Veuillez d'abord charger le mod√®le dans les r√©glages")
        elif not text_input.strip():
            st.error("‚ùå Veuillez saisir une description")
        else:
            with st.spinner("üîç Analyse en cours..."):
                result = analyze_text_multilingual(text_input)
            
            st.markdown(t("analysis_results"))
            st.markdown("---")
            st.markdown(result)

with tab3:
    st.header(t("manual_title"))
    
    if st.session_state.language == "fr":
        st.markdown("""
        ### üöÄ **D√©marrage Rapide**
        1. **Charger le mod√®le** : Cliquez sur 'Charger le mod√®le' dans les r√©glages
        2. **Choisir le mode** : Analyse d'image ou analyse de texte
        3. **Soumettre votre demande** : Upload d'image ou description
        4. **Obtenir le diagnostic** : R√©sultats avec recommandations
        
        ### üì∏ **Analyse d'Image**
        ‚Ä¢ **Formats accept√©s** : PNG, JPG, JPEG
        ‚Ä¢ **Taille recommand√©e** : 500x500 pixels minimum
        ‚Ä¢ **Qualit√©** : Image claire et bien √©clair√©e
        ‚Ä¢ **Focus** : Centrer sur la zone malade
        ‚Ä¢ **Question optionnelle** : Pr√©cisez votre pr√©occupation
        
        ### üí¨ **Analyse de Texte**
        ‚Ä¢ **Description d√©taill√©e** : Sympt√¥mes observ√©s
        ‚Ä¢ **Contexte** : Type de plante, conditions
        ‚Ä¢ **Historique** : √âvolution du probl√®me
        ‚Ä¢ **Actions d√©j√† tent√©es** : Traitements appliqu√©s
        
        ### üîç **Interpr√©tation des R√©sultats**
        ‚Ä¢ **Diagnostic** : Identification de la maladie
        ‚Ä¢ **Causes possibles** : Facteurs d√©clencheurs
        ‚Ä¢ **Recommandations** : Actions √† entreprendre
        ‚Ä¢ **Pr√©vention** : Mesures pr√©ventives
        
        ### üí° **Bonnes Pratiques**
        ‚Ä¢ **Images multiples** : Diff√©rents angles de la maladie
        ‚Ä¢ **√âclairage naturel** : √âviter les ombres
        ‚Ä¢ **Description pr√©cise** : D√©tails des sympt√¥mes
        ‚Ä¢ **Suivi r√©gulier** : Surveiller l'√©volution
        ‚Ä¢ **Consultation expert** : Pour cas complexes
        """)
    else:
        st.markdown("""
        ### üöÄ **Quick Start**
        1. **Load the model** : Click 'Load Model' in settings
        2. **Choose mode** : Image analysis or text analysis
        3. **Submit your request** : Upload image or description
        4. **Get diagnosis** : Results with recommendations
        
        ### üì∏ **Image Analysis**
        ‚Ä¢ **Accepted formats** : PNG, JPG, JPEG
        ‚Ä¢ **Recommended size** : 500x500 pixels minimum
        ‚Ä¢ **Quality** : Clear and well-lit image
        ‚Ä¢ **Focus** : Center on the diseased area
        ‚Ä¢ **Optional question** : Specify your concern
        
        ### üí¨ **Text Analysis**
        ‚Ä¢ **Detailed description** : Observed symptoms
        ‚Ä¢ **Context** : Plant type, conditions
        ‚Ä¢ **History** : Problem evolution
        ‚Ä¢ **Actions already tried** : Applied treatments
        
        ### üîç **Result Interpretation**
        ‚Ä¢ **Diagnosis** : Disease identification
        ‚Ä¢ **Possible causes** : Triggering factors
        ‚Ä¢ **Recommendations** : Actions to take
        ‚Ä¢ **Prevention** : Preventive measures
        
        ### üí° **Best Practices**
        ‚Ä¢ **Multiple images** : Different angles of the disease
        ‚Ä¢ **Natural lighting** : Avoid shadows
        ‚Ä¢ **Precise description** : Symptom details
        ‚Ä¢ **Regular monitoring** : Track evolution
        ‚Ä¢ **Expert consultation** : For complex cases
        """)

with tab4:
    st.header(t("about_title"))
    
    st.markdown("### üå± Notre Mission / Our Mission")
    st.markdown("AgriLens AI est une application de diagnostic des maladies de plantes utilisant l'intelligence artificielle pour aider les agriculteurs √† identifier et traiter les probl√®mes de leurs cultures.")
    
    st.markdown("### üöÄ Fonctionnalit√©s / Features")
    st.markdown("""
    ‚Ä¢ **Analyse d'images** : Diagnostic visuel des maladies
    ‚Ä¢ **Analyse de texte** : Conseils bas√©s sur les descriptions
    ‚Ä¢ **Recommandations pratiques** : Actions concr√®tes √† entreprendre
    ‚Ä¢ **Interface mobile** : Optimis√©e pour smartphones et tablettes
    ‚Ä¢ **Support multilingue** : Fran√ßais et Anglais
    """)
    
    st.markdown("### üîß Technologie / Technology")
    
    # D√©tecter l'environnement pour l'affichage
    is_local = os.path.exists("D:/Dev/model_gemma")
    
    if is_local:
        st.markdown("""
        ‚Ä¢ **Mod√®le** : Gemma 3n E4B IT (Local - D:/Dev/model_gemma)
        ‚Ä¢ **Framework** : Streamlit
        ‚Ä¢ **D√©ploiement** : Local
        """)
    else:
        st.markdown("""
        ‚Ä¢ **Mod√®le** : Gemma 3n E4B IT (Hugging Face - en ligne)
        ‚Ä¢ **Framework** : Streamlit
        ‚Ä¢ **D√©ploiement** : Hugging Face Spaces
        """)
    
    # Informations du cr√©ateur
    st.markdown(f"### {t('creator_title')}")
    st.markdown(f"{t('creator_name')}")
    st.markdown(f"üìç {t('creator_location')}")
    st.markdown(f"üìû {t('creator_phone')}")
    st.markdown(f"üìß {t('creator_email')}")
    st.markdown(f"üîó {t('creator_linkedin')}")
    st.markdown(f"üìÅ {t('creator_portfolio')}")
    
    # Informations de comp√©tition
    st.markdown(f"### {t('competition_title')}")
    st.markdown(t("competition_text"))
    
    st.markdown("### ‚ö†Ô∏è Avertissement / Warning")
    st.markdown("Les r√©sultats fournis sont √† titre indicatif uniquement. Pour un diagnostic professionnel, consultez un expert qualifi√©.")
    
    st.markdown("### üìû Support")
    st.markdown("Pour toute question ou probl√®me, consultez la documentation ou contactez l'√©quipe de d√©veloppement.")

# Footer
st.markdown("---")
st.markdown(t("footer")) 