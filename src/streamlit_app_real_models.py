import streamlit as st
import requests
import json
import base64
import io
from PIL import Image
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor
import random

# Configuration de la page
st.set_page_config(
    page_title="AgriLens AI - Vrais ModÃ¨les",
    page_icon="ğŸŒ¾",
    layout="wide"
)

# Titre principal
st.title("ğŸŒ¾ AgriLens AI")
st.markdown("### Assistant IA pour l'Agriculture (Vrais ModÃ¨les Hugging Face)")

# Sidebar
st.sidebar.header("âš™ï¸ Configuration")

# SÃ©lection du modÃ¨le
model_options = {
    "microsoft/DialoGPT-medium": "DialoGPT Medium (Chat - RecommandÃ©)",
    "microsoft/DialoGPT-small": "DialoGPT Small (Chat Rapide)",
    "google/gemma-3n-E4B-it": "Gemma 3N (Multimodal - Images + Texte)",
    "gpt2": "GPT-2 (Texte GÃ©nÃ©ral)",
    "distilgpt2": "DistilGPT-2 (LÃ©ger)"
}

selected_model = st.sidebar.selectbox(
    "ğŸ¤– ModÃ¨le Ã  utiliser",
    list(model_options.keys()),
    format_func=lambda x: model_options[x],
    index=0
)

# Token API (optionnel)
api_token = st.sidebar.text_input(
    "ğŸ”‘ Token Hugging Face (optionnel)",
    type="password",
    help="Pour de meilleures performances"
)

# Mode d'utilisation
mode = st.sidebar.selectbox(
    "Mode d'utilisation",
    ["ğŸ“· Analyse d'Image", "ğŸ’¬ Mode Chat", "ğŸ“¤ Upload d'Image"]
)

# Initialisation des modÃ¨les
@st.cache_resource
def load_model_and_tokenizer(model_id):
    """Charge le modÃ¨le et le tokenizer"""
    try:
        st.info(f"ğŸ”„ Chargement du modÃ¨le {model_id}...")
        
        if "gemma" in model_id.lower():
            # Pour Gemma 3N (multimodal)
            processor = AutoProcessor.from_pretrained(model_id)
            model = AutoModelForCausalLM.from_pretrained(
                model_id,
                torch_dtype=torch.float16,
                device_map="auto" if torch.cuda.is_available() else None,
            )
            return model, processor, "multimodal"
        else:
            # Pour les modÃ¨les texte (DialoGPT, GPT-2, etc.)
            tokenizer = AutoTokenizer.from_pretrained(model_id)
            model = AutoModelForCausalLM.from_pretrained(
                model_id,
                torch_dtype=torch.float16,
                device_map="auto" if torch.cuda.is_available() else None,
            )
            return model, tokenizer, "text"
            
    except Exception as e:
        st.error(f"âŒ Erreur lors du chargement : {e}")
        return None, None, None

# Charger le modÃ¨le
model, tokenizer_or_processor, model_type = load_model_and_tokenizer(selected_model)

if model is None:
    st.error("âŒ Impossible de charger le modÃ¨le. VÃ©rifiez votre connexion internet.")
    st.stop()

# Fonction pour gÃ©nÃ©rer une rÃ©ponse avec DialoGPT
def generate_dialogpt_response(prompt, model, tokenizer, max_length=200):
    """GÃ©nÃ¨re une rÃ©ponse avec DialoGPT"""
    try:
        # Encoder l'entrÃ©e utilisateur
        new_user_input_ids = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors='pt')
        
        # GÃ©nÃ©rer la rÃ©ponse
        with torch.inference_mode():
            chat_history_ids = model.generate(
                new_user_input_ids, 
                max_length=max_length, 
                pad_token_id=tokenizer.eos_token_id,
                temperature=0.7,
                do_sample=True,
                num_return_sequences=1
            )
        
        # DÃ©coder la rÃ©ponse
        response = tokenizer.decode(chat_history_ids[:, new_user_input_ids.shape[-1]:][0], skip_special_tokens=True)
        
        return response.strip()
        
    except Exception as e:
        return f"âŒ Erreur DialoGPT : {e}"

# Fonction pour gÃ©nÃ©rer une rÃ©ponse avec Gemma 3N
def generate_gemma_response(messages, model, processor):
    """GÃ©nÃ¨re une rÃ©ponse avec Gemma 3N multimodal"""
    try:
        # PrÃ©parer les messages pour Gemma 3N
        formatted_messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": "Tu es un expert en agriculture. Analyse les images et rÃ©ponds aux questions de maniÃ¨re prÃ©cise et utile."}]
            }
        ]
        
        # Ajouter le message utilisateur
        formatted_messages.append({
            "role": "user",
            "content": messages
        })
        
        # GÃ©nÃ©rer la rÃ©ponse
        with torch.inference_mode():
            output = model.generate(
                formatted_messages,
                max_new_tokens=200,
                temperature=0.7,
                do_sample=True
            )
        
        # Extraire la rÃ©ponse
        if isinstance(output, list) and len(output) > 0:
            last_message = output[0]["generated_text"][-1]
            if "content" in last_message:
                return last_message["content"]
        
        return "RÃ©ponse gÃ©nÃ©rÃ©e par Gemma 3N"
        
    except Exception as e:
        return f"âŒ Erreur Gemma 3N : {e}"

# Fonction pour analyser une image avec le vrai modÃ¨le
def analyze_image_real(image, prompt="", model_id="google/gemma-3n-E4B-it"):
    """Analyse une image avec le vrai modÃ¨le multimodal"""
    try:
        # Convertir l'image en base64
        buffered = io.BytesIO()
        image.save(buffered, format="JPEG")
        img_str = base64.b64encode(buffered.getvalue()).decode()
        
        if "gemma" in model_id.lower() and model_type == "multimodal":
            # Utiliser Gemma 3N pour l'analyse d'image
            messages = [
                {"type": "image", "url": f"data:image/jpeg;base64,{img_str}"},
                {"type": "text", "text": prompt if prompt else "Analyse cette image agricole en dÃ©tail. Identifie les plantes, les maladies, les conditions de croissance, et donne des recommandations."}
            ]
            
            return generate_gemma_response(messages, model, tokenizer_or_processor)
        else:
            # Pour les modÃ¨les texte uniquement, utiliser l'analyse simulÃ©e
            width, height = image.size
            analysis_prompt = f"Tu es un expert en agriculture. Analyse cette image agricole: {prompt}. L'image fait {width}x{height} pixels. Donne une analyse dÃ©taillÃ©e et des recommandations."
            
            if model_type == "text":
                return generate_dialogpt_response(analysis_prompt, model, tokenizer_or_processor)
            else:
                return f"âŒ ModÃ¨le non compatible avec l'analyse d'image : {model_id}"
        
    except Exception as e:
        return f"âŒ Erreur lors de l'analyse : {e}"

# Fonction pour le chat avec le vrai modÃ¨le
def chat_with_real_model(prompt, model_id):
    """Chat avec le vrai modÃ¨le sÃ©lectionnÃ©"""
    try:
        if "gemma" in model_id.lower() and model_type == "multimodal":
            # Utiliser Gemma 3N pour le chat
            messages = [
                {"type": "text", "text": f"Tu es un expert en agriculture. RÃ©ponds Ã  cette question : {prompt}"}
            ]
            return generate_gemma_response(messages, model, tokenizer_or_processor)
        elif model_type == "text":
            # Utiliser DialoGPT ou autre modÃ¨le texte
            chat_prompt = f"Tu es un expert en agriculture. Question : {prompt}\n\nRÃ©ponse :"
            return generate_dialogpt_response(chat_prompt, model, tokenizer_or_processor)
        else:
            return f"âŒ ModÃ¨le non supportÃ© : {model_id}"
            
    except Exception as e:
        return f"âŒ Erreur de chat : {e}"

# Interface principale
if mode == "ğŸ“· Analyse d'Image":
    st.header("ğŸ“· Analyse d'Image")
    st.info(f"ğŸ¤– Utilise le vrai modÃ¨le {selected_model} pour analyser vos images !")
    
    # Afficher les informations du modÃ¨le
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ“Š Informations ModÃ¨le")
    st.sidebar.info(f"**ModÃ¨le** : {selected_model}\n**Type** : {model_type}\n**Statut** : âœ… ChargÃ©")
    
    # Onglets
    tab1, tab2, tab3 = st.tabs(["ğŸ“¤ Upload", "ğŸŒ URL", "ğŸ“¸ Webcam"])
    
    with tab1:
        st.subheader("ğŸ“¤ Upload d'Image")
        uploaded_file = st.file_uploader(
            "Choisissez une image...",
            type=['png', 'jpg', 'jpeg', 'gif', 'bmp']
        )
        
        if uploaded_file is not None:
            image = Image.open(uploaded_file)
            st.image(image, caption="Image uploadÃ©e", use_container_width=True)
            
            # Prompt personnalisÃ©
            custom_prompt = st.text_area(
                "ğŸ’­ Question spÃ©cifique (optionnel)",
                placeholder="Ex: Identifie les maladies prÃ©sentes sur ces feuilles"
            )
            
            if st.button("ğŸ” Analyser avec le Vrai ModÃ¨le", type="primary"):
                with st.spinner(f"ğŸ”„ Analyse avec {selected_model}..."):
                    result = analyze_image_real(image, custom_prompt, selected_model)
                    st.markdown("### ğŸ“Š RÃ©sultats de l'Analyse")
                    st.write(result)
    
    with tab2:
        st.subheader("ğŸŒ Image depuis URL")
        url = st.text_input(
            "Entrez l'URL de l'image",
            placeholder="https://example.com/image.jpg"
        )
        
        if url:
            if st.button("ğŸ“¥ TÃ©lÃ©charger et Analyser", type="primary"):
                try:
                    response = requests.get(url, timeout=10)
                    response.raise_for_status()
                    image = Image.open(io.BytesIO(response.content))
                    st.image(image, caption="Image tÃ©lÃ©chargÃ©e", use_container_width=True)
                    
                    with st.spinner(f"ğŸ”„ Analyse avec {selected_model}..."):
                        result = analyze_image_real(image, "", selected_model)
                        st.markdown("### ğŸ“Š RÃ©sultats de l'Analyse")
                        st.write(result)
                except Exception as e:
                    st.error(f"âŒ Erreur : {e}")
    
    with tab3:
        st.subheader("ğŸ“¸ Capture Webcam")
        camera_input = st.camera_input("ğŸ“¸ Prenez une photo")
        
        if camera_input is not None:
            image = Image.open(camera_input)
            st.image(image, caption="Image capturÃ©e", use_container_width=True)
            
            custom_prompt = st.text_area(
                "ğŸ’­ Question spÃ©cifique (optionnel)",
                placeholder="Ex: Identifie les maladies prÃ©sentes sur ces feuilles"
            )
            
            if st.button("ğŸ” Analyser avec le Vrai ModÃ¨le", type="primary"):
                with st.spinner(f"ğŸ”„ Analyse avec {selected_model}..."):
                    result = analyze_image_real(image, custom_prompt, selected_model)
                    st.markdown("### ğŸ“Š RÃ©sultats de l'Analyse")
                    st.write(result)

elif mode == "ğŸ’¬ Mode Chat":
    st.header("ğŸ’¬ Mode Chat")
    st.info(f"ğŸ¤– Chat avec le vrai modÃ¨le {selected_model} !")
    
    # Afficher les informations du modÃ¨le
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ“Š Informations ModÃ¨le")
    st.sidebar.info(f"**ModÃ¨le** : {selected_model}\n**Type** : {model_type}\n**Statut** : âœ… ChargÃ©")
    
    # Historique des messages
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Afficher l'historique
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Zone de saisie
    st.markdown("---")
    st.subheader("ğŸ’­ Posez votre question")
    
    user_question = st.text_input(
        "Votre question sur l'agriculture :",
        placeholder="Ex: Comment cultiver des tomates ?",
        key="chat_input"
    )
    
    # Bouton pour envoyer
    if st.button("ğŸš€ Envoyer", type="primary"):
        if user_question.strip():
            # Ajouter le message utilisateur
            st.session_state.messages.append({"role": "user", "content": user_question})
            
            # GÃ©nÃ©rer la rÃ©ponse avec le vrai modÃ¨le
            try:
                with st.spinner(f"ğŸ”„ GÃ©nÃ©ration avec {selected_model}..."):
                    response = chat_with_real_model(user_question, selected_model)
                
                st.session_state.messages.append({"role": "assistant", "content": response})
                st.rerun()
                
            except Exception as e:
                error_msg = f"âŒ Erreur : {e}"
                st.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})
                st.rerun()
        else:
            st.warning("âš ï¸ Veuillez saisir une question.")
    
    # Bouton pour effacer l'historique
    if st.button("ğŸ—‘ï¸ Effacer l'historique"):
        st.session_state.messages = []
        st.rerun()

elif mode == "ğŸ“¤ Upload d'Image":
    st.header("ğŸ“¤ Upload d'Image Simple")
    st.info(f"ğŸ¯ Version simplifiÃ©e avec {selected_model}")
    
    uploaded_file = st.file_uploader(
        "Choisissez une image agricole...",
        type=['png', 'jpg', 'jpeg']
    )
    
    if uploaded_file is not None:
        image = Image.open(uploaded_file)
        st.image(image, caption="Image Ã  analyser", use_container_width=True)
        
        if st.button("ğŸš€ Analyser avec le Vrai ModÃ¨le", type="primary"):
            with st.spinner(f"ğŸ”„ Analyse avec {selected_model}..."):
                result = analyze_image_real(image, "", selected_model)
                st.markdown("### ğŸ“Š Analyse")
                st.write(result)

# Informations sur les modÃ¨les
st.sidebar.markdown("---")
st.sidebar.markdown("### â„¹ï¸ ModÃ¨les Disponibles")
st.sidebar.info(
    "**DialoGPT** : Chat conversationnel\n"
    "**Gemma 3N** : Multimodal (Images + Texte)\n"
    "**GPT-2** : GÃ©nÃ©ration de texte\n"
    "**DistilGPT-2** : Version lÃ©gÃ¨re"
)

# Footer
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: #666;'>
        ğŸŒ¾ AgriLens AI - Vrais ModÃ¨les<br>
        Powered by Hugging Face Transformers
    </div>
    """,
    unsafe_allow_html=True
) 