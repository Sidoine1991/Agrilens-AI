# ============================================================
# PHASE 2 : DIAGNOSTIC VISUEL MULTIMODAL (CPU only, RAM safe)
# ============================================================

print("\n--- PHASE 2: Lancement du diagnostic visuel AgriLens AI ---")

try:
    import torch
    from PIL import Image
    from transformers import AutoProcessor, AutoModelForImageTextToText
    from IPython.display import display

    # ‚ö†Ô∏è Mode CPU uniquement pour √©viter tout crash m√©moire
    device = torch.device("cpu")
    print("üß† Ex√©cution forc√©e sur :", device)

    # üìç Emplacement du mod√®le pr√©charg√©
    GEMMA_PATH = "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1"

    # üîÑ Lib√©ration m√©moire √©ventuelle pr√©c√©dente
    try: del tokenizer
    except: pass
    try: del model
    except: pass
    torch.cuda.empty_cache()

    # üì¶ Chargement du processor et mod√®le multimodal
    processor = AutoProcessor.from_pretrained(GEMMA_PATH)
    model = AutoModelForImageTextToText.from_pretrained(GEMMA_PATH).to(device)
    print("‚úÖ Mod√®le multimodal charg√© avec succ√®s.")

    # üì∏ Chargement de l‚Äôimage √† analyser
    image_path = "/kaggle/input/tomato/tomato_early_blight.jpg"
    image = Image.open(image_path).convert("RGB").resize((224, 224))
    print(f"‚úÖ Image charg√©e : {image_path}")
    display(image)

    # üí¨ Prompt structur√© pour un diagnostic complet
    prompt = (
        "Analyse cette feuille de tomate. D√©cris les sympt√¥mes visibles : taille, forme, couleur et r√©partition des l√©sions. "
        "Donne ensuite un diagnostic structur√© en 5 parties :\n"
        "1. Nom de la maladie probable\n"
        "2. Agent pathog√®ne suspect√©\n"
        "3. Mode d'infection et de transmission\n"
        "4. Conditions climatiques favorables √† la maladie\n"
        "5. M√©thodes de lutte (pr√©ventives et curatives)"
    )

    # üß† Construction du message multimodal
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt}
            ]
        }
    ]

    # üéØ Pr√©paration des inputs pour le mod√®le
    inputs = processor.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt"
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # üîÆ G√©n√©ration contr√¥l√©e (m√©moire s√©curis√©e)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,  # Limite raisonnable pour √©viter le red√©marrage
            do_sample=True,
            temperature=0.7
        )

    # üìú D√©codage du texte g√©n√©r√©
    input_len = inputs["input_ids"].shape[-1]
    result = processor.batch_decode(outputs[:, input_len:], skip_special_tokens=True)[0]

    # üìã Affichage du r√©sultat final
    print("\n--- ‚úÖ Diagnostic fourni par AgriLens AI ---\n")
    print(result)

except Exception as e:
    print(f"\n‚ùå Erreur : {type(e).__name__} ‚Üí {e}")