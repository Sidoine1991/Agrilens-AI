# ===================================================================
# SIDOINE - FINAL UNIFIED SETUP FOR T4 GPU - USING ORIGINAL VARIABLE NAMES
# This block loads the text model into 'tokenizer' and 'model'.
# ===================================================================

# Step 0.1: Force single GPU usage
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
print(f"CUDA_VISIBLE_DEVICES set to {os.environ['CUDA_VISIBLE_DEVICES']}.")

# Step 1: Install necessary libraries from your original working notebook
print("\nStep 1: Installing libraries...\n")
!pip install timm --upgrade -q
!pip install accelerate -q
!pip install git+https://github.com/huggingface/transformers.git --upgrade -q
print("\n--- Libraries are ready.")

# Step 2: Import all necessary libraries
import kagglehub
from PIL import Image
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor, AutoModelForImageTextToText, GenerationConfig
print("\nStep 2: All required libraries imported.")

# Step 3: Get model path
GEMMA_PATH = kagglehub.model_download("google/gemma-3n/transformers/gemma-3n-e2b-it")
print(f"\nStep 3: Model path located at:\n{GEMMA_PATH}")

# Step 4: Load text model using "auto" settings into original variable names
print("\nStep 4: Loading Text-Only Model into 'tokenizer' and 'model'...")
tokenizer = AutoTokenizer.from_pretrained(GEMMA_PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    GEMMA_PATH,
    trust_remote_code=True,
    torch_dtype="auto",
    device_map="auto"
)
print("--- Text-Only Model Loaded.")

# Step 5: Define a generation config
generation_config = GenerationConfig(max_new_tokens=512, do_sample=True, temperature=0.7)
print("\nStep 5: Default Generation Config created.")

print("\n\n✅ ✅ ✅ SETUP IS COMPLETE AND TEXT MODEL IS READY! ✅ ✅ ✅")
