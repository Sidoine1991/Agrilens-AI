# =================================================================================
# Test Simple - Gemma 3n E2B IT (Transformers √† jour)
# Version simple avec Transformers mis √† jour depuis la source
# =================================================================================

import os
import torch
import time

# Installation de Transformers depuis la source (CRUCIAL pour Gemma 3n)
print("üîß Installation de Transformers depuis la source...")
os.system("pip install git+https://github.com/huggingface/transformers.git")

# Installation des autres d√©pendances
print("üì¶ Installation des autres d√©pendances...")
os.system("pip install bitsandbytes accelerate torch pillow requests")

# Import apr√®s installation
from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig

# Configuration
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
MODEL_PATH = "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1"

def simple_test():
    """Test simple de g√©n√©ration avec Transformers √† jour"""
    
    print("üöÄ Test simple Gemma 3n (Transformers √† jour)")
    print("=" * 50)
    
    try:
        # 1. V√©rification du chemin
        print(f"üìÅ V√©rification du chemin: {MODEL_PATH}")
        if not os.path.exists(MODEL_PATH):
            print(f"‚ùå Chemin inexistant: {MODEL_PATH}")
            return False
        
        # 2. Chargement du mod√®le
        print("üì• Chargement du mod√®le...")
        
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        
        processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
        model = AutoModelForImageTextToText.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            device_map="auto",
            quantization_config=bnb_config,
        )
        
        print("‚úÖ Mod√®le charg√©!")
        
        # 3. Test de g√©n√©ration
        print("\nüî§ Test de g√©n√©ration...")
        
        prompt = "Explique-moi l'agriculture durable en 2 phrases."
        print(f"Prompt: {prompt}")
        
        inputs = processor(text=prompt, return_tensors="pt")
        
        start_time = time.time()
        
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=True,
                temperature=0.7,
            )
        
        gen_time = time.time() - start_time
        
        response = processor.decode(outputs[0], skip_special_tokens=True)
        
        if prompt in response:
            response = response.replace(prompt, "").strip()
        
        print(f"\n‚è±Ô∏è Temps: {gen_time:.2f}s")
        print(f"üìù R√©ponse: {response}")
        
        print("\nüéâ Test r√©ussi!")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        print(f"Type d'erreur: {type(e).__name__}")
        return False

if __name__ == "__main__":
    simple_test() 