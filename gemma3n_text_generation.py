# =================================================================================
# GÃ‰NÃ‰RATION DE TEXTE AVEC GEMMA 3N - VERSION OPTIMISÃ‰E
# Auteur : Sidoine KolaolÃ© YEBADOKPO
# Utilise le modÃ¨le dÃ©jÃ  chargÃ©: tokenizer et model
# =================================================================================

import torch
import time
from typing import Optional

def monitor_memory(stage: str = ""):
    """Surveille l'utilisation de la mÃ©moire GPU"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / (1024 ** 3)
        reserved = torch.cuda.memory_reserved(0) / (1024 ** 3)
        print(f"ğŸ’¾ MÃ©moire ({stage}): {allocated:.2f} GB / {reserved:.2f} GB")

def generate_text_with_gemma3n(
    prompt: str,
    max_new_tokens: int = 256,
    temperature: float = 0.7,
    top_p: float = 0.9,
    repetition_penalty: float = 1.1
) -> Optional[str]:
    """
    GÃ©nÃ¨re du texte avec le modÃ¨le Gemma 3n dÃ©jÃ  chargÃ©
    
    Args:
        prompt: Le texte d'entrÃ©e
        max_new_tokens: Nombre maximum de tokens Ã  gÃ©nÃ©rer
        temperature: ContrÃ´le la crÃ©ativitÃ© (0.0 = dÃ©terministe, 1.0 = trÃ¨s crÃ©atif)
        top_p: ContrÃ´le la diversitÃ© du vocabulaire
        repetition_penalty: PÃ©nalise la rÃ©pÃ©tition
    
    Returns:
        Le texte gÃ©nÃ©rÃ© ou None en cas d'erreur
    """
    
    try:
        print(f"\nğŸ”¤ GÃ©nÃ©ration de texte avec Gemma 3n...")
        print(f"ğŸ“ Prompt: {prompt[:100]}{'...' if len(prompt) > 100 else ''}")
        
        # VÃ©rification que le modÃ¨le est chargÃ©
        if 'tokenizer' not in globals() or 'model' not in globals():
            print("âŒ Erreur: ModÃ¨le non chargÃ©. ExÃ©cutez d'abord le setup.")
            return None
        
        # Configuration du tokenizer si nÃ©cessaire
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # Tokenisation avec gestion des erreurs
        try:
            inputs = tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=512,  # Limite pour Ã©viter les dÃ©passements
                padding=True
            )
        except Exception as e:
            print(f"âŒ Erreur de tokenisation: {e}")
            return None
        
        # DÃ©placer vers le bon device
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Surveillance mÃ©moire avant gÃ©nÃ©ration
        monitor_memory("Avant gÃ©nÃ©ration")
        
        # GÃ©nÃ©ration avec timing
        start_time = time.time()
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=temperature,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                # ParamÃ¨tres de sÃ©curitÃ©
                use_cache=True,
                return_dict_in_generate=False
            )
        
        generation_time = time.time() - start_time
        
        # Surveillance mÃ©moire aprÃ¨s gÃ©nÃ©ration
        monitor_memory("AprÃ¨s gÃ©nÃ©ration")
        
        # DÃ©codage du rÃ©sultat
        try:
            response = tokenizer.decode(
                outputs[0],
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )
        except Exception as e:
            print(f"âŒ Erreur de dÃ©codage: {e}")
            return None
        
        # Nettoyage de la rÃ©ponse
        if prompt in response:
            response = response.replace(prompt, "").strip()
        
        # Statistiques
        tokens_generated = len(outputs[0]) - len(inputs['input_ids'][0])
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        print(f"â±ï¸ Temps de gÃ©nÃ©ration: {generation_time:.2f}s")
        print(f"ğŸ”¢ Tokens gÃ©nÃ©rÃ©s: {tokens_generated}")
        print(f"âš¡ Vitesse: {tokens_per_second:.1f} tokens/s")
        print(f"ğŸ“ RÃ©ponse ({len(response)} caractÃ¨res)")
        
        return response
        
    except Exception as e:
        print(f"âŒ Erreur lors de la gÃ©nÃ©ration: {e}")
        return None

def test_agricultural_queries():
    """Test avec des requÃªtes agricoles spÃ©cifiques"""
    
    print("\n" + "="*60)
    print("ğŸ§ª TESTS DE GÃ‰NÃ‰RATION AGRICOLE")
    print("="*60)
    
    # Liste des prompts de test
    test_prompts = [
        {
            "title": "SymptÃ´mes du mildiou",
            "prompt": "DÃ©cris en dÃ©tail les symptÃ´mes de la maladie du mildiou chez les plants de tomate. Inclus l'apparence des feuilles, des tiges et des fruits, ainsi que la progression de la maladie.",
            "max_tokens": 300
        },
        {
            "title": "MÃ©thodes de prÃ©vention",
            "prompt": "Explique les mÃ©thodes de prÃ©vention contre les maladies fongiques dans un jardin potager. Donne des conseils pratiques pour les agriculteurs.",
            "max_tokens": 250
        },
        {
            "title": "Diagnostic rapide",
            "prompt": "Comment identifier rapidement si une plante est malade ? Donne les signes visuels les plus courants Ã  surveiller.",
            "max_tokens": 200
        },
        {
            "title": "Traitements biologiques",
            "prompt": "Quels sont les traitements biologiques efficaces contre les maladies des plantes ? Inclus des recettes naturelles et des mÃ©thodes alternatives.",
            "max_tokens": 350
        }
    ]
    
    # ExÃ©cution des tests
    for i, test in enumerate(test_prompts, 1):
        print(f"\nğŸ“‹ Test {i}: {test['title']}")
        print("-" * 40)
        
        response = generate_text_with_gemma3n(
            prompt=test['prompt'],
            max_new_tokens=test['max_tokens'],
            temperature=0.7
        )
        
        if response:
            print("ğŸ“ RÃ©ponse:")
            print(response)
        else:
            print("âŒ Ã‰chec de la gÃ©nÃ©ration")
        
        print("-" * 40)
        
        # Pause entre les tests pour Ã©viter la surcharge
        if i < len(test_prompts):
            print("â³ Pause de 2 secondes...")
            time.sleep(2)

def interactive_generation():
    """Mode interactif pour tester des prompts personnalisÃ©s"""
    
    print("\n" + "="*60)
    print("ğŸ¯ MODE INTERACTIF - GÃ‰NÃ‰RATION PERSONNALISÃ‰E")
    print("="*60)
    print("Tapez 'quit' pour arrÃªter")
    
    while True:
        try:
            # Saisie utilisateur
            user_prompt = input("\nğŸŒ± Votre prompt: ").strip()
            
            if user_prompt.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ Au revoir!")
                break
            
            if not user_prompt:
                print("âš ï¸ Prompt vide, essayez Ã  nouveau")
                continue
            
            # ParamÃ¨tres de gÃ©nÃ©ration
            try:
                max_tokens = int(input("ğŸ”¢ Nombre max de tokens (dÃ©faut 200): ") or "200")
                temperature = float(input("ğŸŒ¡ï¸ TempÃ©rature (0.1-1.0, dÃ©faut 0.7): ") or "0.7")
            except ValueError:
                max_tokens = 200
                temperature = 0.7
                print("âš ï¸ Valeurs invalides, utilisation des valeurs par dÃ©faut")
            
            # GÃ©nÃ©ration
            response = generate_text_with_gemma3n(
                prompt=user_prompt,
                max_new_tokens=max_tokens,
                temperature=temperature
            )
            
            if response:
                print("\nğŸ“ RÃ©ponse de Gemma 3n:")
                print("-" * 30)
                print(response)
                print("-" * 30)
            else:
                print("âŒ Ã‰chec de la gÃ©nÃ©ration")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ArrÃªt demandÃ© par l'utilisateur")
            break
        except Exception as e:
            print(f"âŒ Erreur: {e}")

def main():
    """Fonction principale"""
    
    print("ğŸš€ GÃ‰NÃ‰RATION DE TEXTE AVEC GEMMA 3N")
    print("=" * 50)
    
    # VÃ©rification que le modÃ¨le est disponible
    if 'tokenizer' not in globals() or 'model' not in globals():
        print("âŒ Erreur: ModÃ¨le non chargÃ©!")
        print("ğŸ’¡ ExÃ©cutez d'abord le setup pour charger le modÃ¨le")
        return
    
    print("âœ… ModÃ¨le Gemma 3n dÃ©tectÃ© et prÃªt")
    print(f"ğŸ“Š ModÃ¨le sur device: {next(model.parameters()).device}")
    
    # Menu de choix
    print("\nğŸ¯ Choisissez le mode:")
    print("1. Tests agricoles automatiques")
    print("2. Mode interactif personnalisÃ©")
    print("3. Test simple rapide")
    
    try:
        choice = input("\nVotre choix (1-3): ").strip()
        
        if choice == "1":
            test_agricultural_queries()
        elif choice == "2":
            interactive_generation()
        elif choice == "3":
            # Test simple
            print("\nğŸ§ª Test simple...")
            response = generate_text_with_gemma3n(
                "Explique briÃ¨vement l'importance de la rotation des cultures.",
                max_new_tokens=150
            )
            if response:
                print("ğŸ“ RÃ©ponse:")
                print(response)
        else:
            print("âŒ Choix invalide")
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ArrÃªt demandÃ©")
    except Exception as e:
        print(f"âŒ Erreur: {e}")
    
    print("\nğŸ‰ GÃ©nÃ©ration terminÃ©e!")

# ExÃ©cution directe si le script est lancÃ©
if __name__ == "__main__":
    main() 