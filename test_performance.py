#!/usr/bin/env python3
"""
Script de test des performances pour AgriLens AI
Teste les diff√©rents modes de performance et mesure les temps de r√©ponse.
"""

import time
import torch
import os
import sys
from PIL import Image
import numpy as np

def test_performance_modes():
    """Teste les diff√©rents modes de performance."""
    
    print("üöÄ Test des performances AgriLens AI")
    print("=" * 50)
    
    # V√©rifier que le mod√®le local existe
    LOCAL_MODEL_PATH = "D:/Dev/model_gemma"
    if not os.path.exists(LOCAL_MODEL_PATH):
        print(f"‚ùå Mod√®le local non trouv√© : {LOCAL_MODEL_PATH}")
        print("üí° Assurez-vous que le mod√®le est t√©l√©charg√© et configur√©")
        return False
    
    try:
        from transformers import AutoProcessor, AutoModelForCausalLM
        
        print("üì¶ Chargement du mod√®le...")
        
        # Configuration optimis√©e
        model = AutoModelForCausalLM.from_pretrained(
            LOCAL_MODEL_PATH,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            device_map="auto",
            torch_dtype=torch.float16,
            attn_implementation="flash_attention_2",
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
        
        processor = AutoProcessor.from_pretrained(LOCAL_MODEL_PATH, trust_remote_code=True)
        
        print("‚úÖ Mod√®le charg√© avec succ√®s")
        
        # Cr√©er une image de test simple
        test_image = Image.new('RGB', (224, 224), color='green')
        
                        # Configurations de test
                test_configs = {
                    "fast": {"max_new_tokens": 250, "top_k": 50},
                    "balanced": {"max_new_tokens": 300, "top_k": 100},
                    "quality": {"max_new_tokens": 350, "top_k": 200}
                }
        
        # Test avec une image
        print("\nüì∏ Test d'analyse d'image :")
        print("-" * 30)
        
        for mode, config in test_configs.items():
            print(f"\nüîç Test mode {mode.upper()} :")
            print(f"   ‚Ä¢ max_new_tokens: {config['max_new_tokens']}")
            print(f"   ‚Ä¢ top_k: {config['top_k']}")
            
            # Pr√©parer les inputs
            messages = [
                {"role": "system", "content": [{"type": "text", "text": "Tu es un expert en pathologie v√©g√©tale."}]},
                {"role": "user", "content": [
                    {"type": "image", "image": test_image},
                    {"type": "text", "text": "Analyse cette image de plante."}
                ]}
            ]
            
            inputs = processor.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt",
            )
            
            device = getattr(model, 'device', 'cpu')
            if hasattr(inputs, 'to'):
                inputs = inputs.to(device)
            
            input_len = inputs["input_ids"].shape[-1]
            
            # Mesurer le temps
            start_time = time.time()
            
            with torch.inference_mode():
                generation = model.generate(
                    **inputs,
                    max_new_tokens=config['max_new_tokens'],
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    top_k=config['top_k'],
                    repetition_penalty=1.1,
                    use_cache=True,
                    num_beams=1,
                )
                
                response = processor.decode(generation[0][input_len:], skip_special_tokens=True)
            
            end_time = time.time()
            duration = end_time - start_time
            
            print(f"   ‚è±Ô∏è  Temps de r√©ponse : {duration:.2f} secondes")
            print(f"   üìù Tokens g√©n√©r√©s : {len(response.split())} mots")
            print(f"   üöÄ Vitesse : {config['max_new_tokens']/duration:.1f} tokens/seconde")
        
        # Test avec du texte
        print("\nüìù Test d'analyse de texte :")
        print("-" * 30)
        
        test_text = "Les feuilles de ma plante de tomate ont des taches jaunes et brunes."
        
        for mode, config in test_configs.items():
            print(f"\nüîç Test mode {mode.upper()} :")
            
            prompt_template = f"Tu es un assistant agricole expert. Analyse ce probl√®me : {test_text}"
            messages = [{"role": "user", "content": [{"type": "text", "text": prompt_template}]}]
            
            inputs = processor.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt",
            )
            
            device = getattr(model, 'device', 'cpu')
            if hasattr(inputs, 'to'):
                inputs = inputs.to(device)
            
            input_len = inputs["input_ids"].shape[-1]
            
            # Mesurer le temps
            start_time = time.time()
            
            with torch.inference_mode():
                generation = model.generate(
                    **inputs,
                    max_new_tokens=config['max_new_tokens'],
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    top_k=config['top_k'],
                    repetition_penalty=1.1,
                    use_cache=True,
                    num_beams=1,
                )
                
                response = processor.decode(generation[0][input_len:], skip_special_tokens=True)
            
            end_time = time.time()
            duration = end_time - start_time
            
            print(f"   ‚è±Ô∏è  Temps de r√©ponse : {duration:.2f} secondes")
            print(f"   üìù Tokens g√©n√©r√©s : {len(response.split())} mots")
            print(f"   üöÄ Vitesse : {config['max_new_tokens']/duration:.1f} tokens/seconde")
        
        print("\n‚úÖ Tests de performance termin√©s !")
        print("\nüí° Recommandations :")
        print("   ‚Ä¢ Mode FAST : Pour les diagnostics rapides")
        print("   ‚Ä¢ Mode BALANCED : Pour un bon √©quilibre vitesse/qualit√©")
        print("   ‚Ä¢ Mode QUALITY : Pour les analyses d√©taill√©es")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors du test : {e}")
        return False

def test_memory_usage():
    """Teste l'utilisation m√©moire."""
    print("\nüíæ Test d'utilisation m√©moire :")
    print("-" * 30)
    
    try:
        import psutil
        
        # Avant chargement
        process = psutil.Process()
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        print(f"üìä M√©moire avant chargement : {memory_before:.1f} MB")
        
        # Charger le mod√®le
        from transformers import AutoProcessor, AutoModelForCausalLM
        
        LOCAL_MODEL_PATH = "D:/Dev/model_gemma"
        model = AutoModelForCausalLM.from_pretrained(
            LOCAL_MODEL_PATH,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            device_map="auto",
            torch_dtype=torch.float16,
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
        
        # Apr√®s chargement
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        memory_used = memory_after - memory_before
        
        print(f"üìä M√©moire apr√®s chargement : {memory_after:.1f} MB")
        print(f"üìä M√©moire utilis√©e par le mod√®le : {memory_used:.1f} MB")
        
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
            print(f"üìä M√©moire GPU utilis√©e : {gpu_memory:.1f} MB")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors du test m√©moire : {e}")
        return False

def main():
    """Fonction principale."""
    print("üî¨ Test complet des performances AgriLens AI")
    print("=" * 60)
    
    # Test des performances
    if test_performance_modes():
        print("\n‚úÖ Tests de performance r√©ussis")
    else:
        print("\n‚ùå Tests de performance √©chou√©s")
    
    # Test de m√©moire
    if test_memory_usage():
        print("\n‚úÖ Tests de m√©moire r√©ussis")
    else:
        print("\n‚ùå Tests de m√©moire √©chou√©s")
    
    print("\nüéØ Optimisations appliqu√©es :")
    print("   ‚Ä¢ Quantisation 4-bit avec double quantisation")
    print("   ‚Ä¢ Flash Attention 2 (si disponible)")
    print("   ‚Ä¢ Param√®tres de g√©n√©ration optimis√©s")
    print("   ‚Ä¢ Cache activ√© pour la g√©n√©ration")
    print("   ‚Ä¢ Modes de performance configurables")

if __name__ == "__main__":
    main() 