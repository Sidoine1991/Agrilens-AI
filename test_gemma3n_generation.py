# =================================================================================
# Test de GÃ©nÃ©ration de Texte avec Gemma 3n E2B IT
# 
# Auteur : Sidoine KolaolÃ© YEBADOKPO
# Objectif : Tester les capacitÃ©s de gÃ©nÃ©ration de texte du modÃ¨le Gemma 3n
# =================================================================================

import os
import torch
import time
from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig
from huggingface_hub import HfFolder
from PIL import Image
import requests
from io import BytesIO

# Configuration de l'environnement
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Chemin local du modÃ¨le fourni par Kaggle
MODEL_PATH = "/kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1"

def load_model():
    """Charge le modÃ¨le Gemma 3n avec la configuration optimisÃ©e"""
    try:
        print("ğŸ”„ Chargement du modÃ¨le Gemma 3n E2B IT...")
        
        # Configuration 4-bit pour Ã©conomiser la mÃ©moire
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_block_size=16,
        )
        
        # Chargement du processeur
        print("ğŸ“ Chargement du processeur...")
        processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)
        
        # Chargement du modÃ¨le
        print("ğŸ¤– Chargement du modÃ¨le...")
        model = AutoModelForImageTextToText.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            device_map="auto",
            quantization_config=bnb_config,
        )
        
        print("âœ… ModÃ¨le chargÃ© avec succÃ¨s!")
        return model, processor
        
    except Exception as e:
        print(f"âŒ Erreur lors du chargement: {e}")
        return None, None

def load_test_image():
    """Charge une image de test pour la dÃ©monstration"""
    try:
        # URL d'une image de plante pour test
        image_url = "https://images.unsplash.com/photo-1546094096-0df4bcaaa337?w=400"
        response = requests.get(image_url)
        image = Image.open(BytesIO(response.content))
        print(f"âœ… Image de test chargÃ©e: {image.size}")
        return image
    except Exception as e:
        print(f"âŒ Erreur lors du chargement de l'image: {e}")
        return None

def test_text_generation(model, processor, prompt, max_new_tokens=200):
    """Teste la gÃ©nÃ©ration de texte avec un prompt simple"""
    try:
        print(f"\nğŸ”¤ Test de gÃ©nÃ©ration de texte:")
        print(f"Prompt: {prompt}")
        print("-" * 50)
        
        # PrÃ©paration des inputs
        inputs = processor(text=prompt, return_tensors="pt")
        
        # Mesure du temps de gÃ©nÃ©ration
        start_time = time.time()
        
        # GÃ©nÃ©ration
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        generation_time = time.time() - start_time
        
        # DÃ©codage de la rÃ©ponse
        response = processor.decode(outputs[0], skip_special_tokens=True)
        
        # Nettoyage de la rÃ©ponse (enlever le prompt original)
        if prompt in response:
            response = response.replace(prompt, "").strip()
        
        print(f"â±ï¸ Temps de gÃ©nÃ©ration: {generation_time:.2f} secondes")
        print(f"ğŸ“ RÃ©ponse gÃ©nÃ©rÃ©e:")
        print(response)
        print("-" * 50)
        
        return response, generation_time
        
    except Exception as e:
        print(f"âŒ Erreur lors de la gÃ©nÃ©ration: {e}")
        return None, 0

def test_multimodal_generation(model, processor, image, prompt, max_new_tokens=200):
    """Teste la gÃ©nÃ©ration multimodale (image + texte)"""
    try:
        print(f"\nğŸ–¼ï¸ Test de gÃ©nÃ©ration multimodale:")
        print(f"Prompt: {prompt}")
        print(f"Image: {image.size}")
        print("-" * 50)
        
        # PrÃ©paration des inputs avec image
        inputs = processor(text=prompt, images=image, return_tensors="pt")
        
        # Mesure du temps de gÃ©nÃ©ration
        start_time = time.time()
        
        # GÃ©nÃ©ration
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        generation_time = time.time() - start_time
        
        # DÃ©codage de la rÃ©ponse
        response = processor.decode(outputs[0], skip_special_tokens=True)
        
        # Nettoyage de la rÃ©ponse
        if prompt in response:
            response = response.replace(prompt, "").strip()
        
        print(f"â±ï¸ Temps de gÃ©nÃ©ration: {generation_time:.2f} secondes")
        print(f"ğŸ“ RÃ©ponse gÃ©nÃ©rÃ©e:")
        print(response)
        print("-" * 50)
        
        return response, generation_time
        
    except Exception as e:
        print(f"âŒ Erreur lors de la gÃ©nÃ©ration multimodale: {e}")
        return None, 0

def test_plant_diagnosis(model, processor, image=None):
    """Test spÃ©cifique pour le diagnostic de plantes"""
    
    # Prompts de test pour le diagnostic
    test_prompts = [
        {
            "type": "Diagnostic gÃ©nÃ©ral",
            "prompt": "Analyse cette image de plante et fournis un diagnostic prÃ©cis des maladies ou problÃ¨mes visibles.",
            "multimodal": True
        },
        {
            "type": "SymptÃ´mes spÃ©cifiques", 
            "prompt": "DÃ©cris les symptÃ´mes visibles sur cette plante et suggÃ¨re les causes possibles.",
            "multimodal": True
        },
        {
            "type": "Traitement recommandÃ©",
            "prompt": "BasÃ© sur l'analyse de cette plante, quels traitements recommandes-tu?",
            "multimodal": True
        },
        {
            "type": "Conseils gÃ©nÃ©raux",
            "prompt": "Donne des conseils gÃ©nÃ©raux pour l'entretien des plantes et la prÃ©vention des maladies.",
            "multimodal": False
        }
    ]
    
    print("\nğŸŒ± Tests de diagnostic de plantes")
    print("=" * 60)
    
    results = []
    
    for i, test_case in enumerate(test_prompts, 1):
        print(f"\nğŸ” Test {i}: {test_case['type']}")
        
        if test_case['multimodal'] and image is not None:
            response, gen_time = test_multimodal_generation(
                model, processor, image, test_case['prompt']
            )
        else:
            response, gen_time = test_text_generation(
                model, processor, test_case['prompt']
            )
        
        if response:
            results.append({
                'test': test_case['type'],
                'response': response,
                'time': gen_time,
                'success': True
            })
        else:
            results.append({
                'test': test_case['type'],
                'response': 'Ã‰chec',
                'time': 0,
                'success': False
            })
    
    return results

def test_performance_metrics(model, processor):
    """Teste les mÃ©triques de performance du modÃ¨le"""
    print("\nğŸ“Š Tests de performance")
    print("=" * 40)
    
    # Test avec diffÃ©rents types de prompts
    performance_tests = [
        "Explique-moi la photosynthÃ¨se en termes simples.",
        "Quels sont les avantages de l'agriculture biologique?",
        "Comment identifier les carences en nutriments chez les plantes?",
        "DÃ©cris les mÃ©thodes de lutte biologique contre les ravageurs.",
        "Qu'est-ce que la rotation des cultures et pourquoi est-elle importante?"
    ]
    
    total_time = 0
    successful_tests = 0
    
    for i, prompt in enumerate(performance_tests, 1):
        print(f"\nâš¡ Test de performance {i}/{len(performance_tests)}")
        response, gen_time = test_text_generation(model, processor, prompt, max_new_tokens=100)
        
        if response:
            total_time += gen_time
            successful_tests += 1
    
    if successful_tests > 0:
        avg_time = total_time / successful_tests
        print(f"\nğŸ“ˆ MÃ©triques de performance:")
        print(f"  â€¢ Tests rÃ©ussis: {successful_tests}/{len(performance_tests)}")
        print(f"  â€¢ Temps moyen: {avg_time:.2f} secondes")
        print(f"  â€¢ Temps total: {total_time:.2f} secondes")
    
    return successful_tests, total_time

def main():
    """Fonction principale de test"""
    print("ğŸš€ DÃ©marrage des tests de gÃ©nÃ©ration Gemma 3n")
    print("=" * 60)
    
    # Chargement du modÃ¨le
    model, processor = load_model()
    
    if model is None or processor is None:
        print("âŒ Impossible de continuer sans modÃ¨le chargÃ©")
        return
    
    # Chargement d'une image de test
    test_image = load_test_image()
    
    # Test 1: GÃ©nÃ©ration de texte simple
    print("\n" + "="*60)
    print("TEST 1: GÃ‰NÃ‰RATION DE TEXTE SIMPLE")
    print("="*60)
    
    simple_prompts = [
        "Qu'est-ce que l'intelligence artificielle?",
        "Explique-moi l'agriculture durable en 3 points.",
        "Comment les plantes communiquent-elles entre elles?"
    ]
    
    for prompt in simple_prompts:
        test_text_generation(model, processor, prompt)
    
    # Test 2: GÃ©nÃ©ration multimodale (si image disponible)
    if test_image:
        print("\n" + "="*60)
        print("TEST 2: GÃ‰NÃ‰RATION MULTIMODALE")
        print("="*60)
        
        multimodal_prompts = [
            "DÃ©cris ce que tu vois dans cette image.",
            "Cette plante semble-t-elle en bonne santÃ©?",
            "Quels conseils donnerais-tu pour cette plante?"
        ]
        
        for prompt in multimodal_prompts:
            test_multimodal_generation(model, processor, test_image, prompt)
    
    # Test 3: Diagnostic de plantes
    print("\n" + "="*60)
    print("TEST 3: DIAGNOSTIC DE PLANTES")
    print("="*60)
    
    diagnosis_results = test_plant_diagnosis(model, processor, test_image)
    
    # Test 4: MÃ©triques de performance
    print("\n" + "="*60)
    print("TEST 4: MÃ‰TRIQUES DE PERFORMANCE")
    print("="*60)
    
    performance_results = test_performance_metrics(model, processor)
    
    # RÃ©sumÃ© final
    print("\n" + "="*60)
    print("ğŸ“‹ RÃ‰SUMÃ‰ DES TESTS")
    print("="*60)
    
    successful_diagnosis = sum(1 for r in diagnosis_results if r['success'])
    print(f"âœ… Tests de diagnostic rÃ©ussis: {successful_diagnosis}/{len(diagnosis_results)}")
    print(f"âœ… Tests de performance rÃ©ussis: {performance_results[0]}/5")
    
    if successful_diagnosis > 0:
        avg_diagnosis_time = sum(r['time'] for r in diagnosis_results if r['success']) / successful_diagnosis
        print(f"â±ï¸ Temps moyen de diagnostic: {avg_diagnosis_time:.2f} secondes")
    
    print("\nğŸ‰ Tests terminÃ©s avec succÃ¨s!")

if __name__ == "__main__":
    main() 